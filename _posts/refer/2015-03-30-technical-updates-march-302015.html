---
layout: post
category: refer
title: Technical Updates - March 30/2015
date: '2015-03-29T21:11:00.001-07:00'
author: Sivan Sasidharan
tags: [bigdata, hadoop, apache, hortonworks, compression]
modified_time: '2015-03-29T21:11:21.278-07:00'
blogger_id: tag:blogger.com,1999:blog-6557886645283142724.post-330681956241863780
blogger_orig_url: http://sivansasidharan.blogspot.com/2015/03/technical-updates-march-302015.html
comments: true
---

<div dir="ltr" style="text-align: left;" trbidi="on"><div class="MsoPlainText"><br /></div><div class="MsoPlainText">This tutorial describes how to build a kerberos-enabled Hadoop cluster inside of a VM (the steps are valuable outside of a VM, too). The author provides a script for setting up kerberos before running the quickstart wizard that comes with Cloudera Manager. The script, which includes thorough comments, makes kerberos much less intimidating.<o:p></o:p></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText"><a href="http://blog.cloudera.com/blog/2015/03/how-to-quickly-configure-kerberos-for-your-apache-hadoop-cluster/">http://blog.cloudera.com/blog/2015/03/how-to-quickly-configure-kerberos-for-your-apache-hadoop-cluster/</a><o:p></o:p></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText">This post provides a brief introduction to the DockerContainerExecutor that was introduced in YARN as part of Apache Hadoop 2.6. It describes one of the main motivations for running inside of docker containersâ€”managing system-level dependencies.<o:p></o:p></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText"><a href="https://www.altiscale.com/hadoop-blog/dockercontainerexecutor/">https://www.altiscale.com/hadoop-blog/dockercontainerexecutor/</a><o:p></o:p></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText">The following slides and video are from a presentation given at the recent Strata San Jose conference on optimizing Spark programs. Topics covered include understanding shuffle in Spark (and common problems), understanding which code runs on the client vs. the workers, and tips for organizing code for reusability and testability.<o:p></o:p></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText"><a href="http://www.slideshare.net/databricks/strata-sj-everyday-im-shuffling-tips-for-writing-better-spark-programs">http://www.slideshare.net/databricks/strata-sj-everyday-im-shuffling-tips-for-writing-better-spark-programs</a><o:p></o:p></div><div class="MsoPlainText"><a href="https://www.youtube.com/watch?v=Wg2boMqLjCg&amp;feature=youtu.be">https://www.youtube.com/watch?v=Wg2boMqLjCg&amp;feature=youtu.be</a><o:p></o:p></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText">As noted in the Apache Spark 1.3 release, Spark SQL is no-longer alpha. This post explains that this guarantee means binary compatibility across Spark 1.x. It also describes some plans for improving Spark SQL (better integration with Hive), the new data sources API, improvements to Parquet support (automatic partition discovery and schema migration), and support for JDBC sources.<o:p></o:p></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText"><a href="https://databricks.com/blog/2015/03/24/spark-sql-graduates-from-alpha-in-spark-1-3.html">https://databricks.com/blog/2015/03/24/spark-sql-graduates-from-alpha-in-spark-1-3.html</a><o:p></o:p></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText">The Cloudera blog has a post from a software engineer working at Edmunds.com on how they built a spark-streaming based analytics dashboard to monitor traffic related to superbowl ads. The system also uses Flume, HBase, Solr, Morphlines, and Banana (a port of kibana to Solr) as well as algebird's implementation of HyperLogLog. The post is a good end-to-end description of how the system was built and how it works (with screenshots).<o:p></o:p></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText"><a href="http://blog.cloudera.com/blog/2015/03/how-edmunds-com-used-spark-streaming-to-build-a-near-real-time-dashboard/">http://blog.cloudera.com/blog/2015/03/how-edmunds-com-used-spark-streaming-to-build-a-near-real-time-dashboard/</a><o:p></o:p></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText">For those looking to scale machine learning implementations, the Databricks blog has a post on Spark 1.3's implementation of Latent Dirichlet Allocation (LDA). The post describes LDA, common use-cases, and how it's implemented atop of GraphX (the Graph API for Spark).<o:p></o:p></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText"><a href="https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html">https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html</a><o:p></o:p></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText">This post describes how to enable support for impersonation from Hue in HBase so that users can only view/modify data which they're allowed to via HBase permissions. It also describes how to configure the HBase Thrift Server for kerberos authentication. There are screen shots of the Hue-HBase application, and several troubleshooting steps for common configuration issues.<o:p></o:p></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText"><a href="http://gethue.com/hbase-browsing-with-doas-impersonation-and-kerberos/">http://gethue.com/hbase-browsing-with-doas-impersonation-and-kerberos/</a><o:p></o:p></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText">As a developer, it can become easy to get used to peculiarities of a system you're working with. It's good to take a step back and understand these issues (or even decide if they really are issues!). In this case, the ingest.tips blog has a post that gathers feedback on "what is confusing about Kafka?" In addition to collecting the feedback, there are responses/links for several of the issues.<o:p></o:p></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText"><a href="http://ingest.tips/2015/03/26/what-is-confusing-about-kafka/">http://ingest.tips/2015/03/26/what-is-confusing-about-kafka/</a><o:p></o:p></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText">The Hortonworks blog has the third part in a series on anomaly detection in healthcare data. In this post, they use SociaLite, an open-source graph analysis framework to compute a variant of PageRank. The post gives an overview of SociaLite (which integrates with Python) and describes the implementation to find anomalies. All code is available on github.<o:p></o:p></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText"><a href="http://hortonworks.com/blog/using-pagerank-to-detect-anomalies-and-fraud-in-healthcare-part3/">http://hortonworks.com/blog/using-pagerank-to-detect-anomalies-and-fraud-in-healthcare-part3/</a><o:p></o:p></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText"><br /></div><div class="MsoPlainText">Most folks working with batch systems start out with a simple workflow system that spawns one job after another via cron. From their, they often move to a job that runs based on the availability of input data. As a post on the Cask blog explains, it's difficult to implement a data-driven workflow efficiently. Most systems poll for the availability of input, which can be slow. The Cask Data Application Platform (CDAP) uses notifications to trigger jobs. The follow post describes the architecture in greater detail.<o:p></o:p></div><div class="MsoPlainText"><br /></div><br /><div class="MsoPlainText"><a href="http://blog.cask.co/2015/03/data-driven-job-scheduling-in-hadoop/">http://blog.cask.co/2015/03/data-driven-job-scheduling-in-hadoop/</a><o:p></o:p></div></div>