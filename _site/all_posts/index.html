<!DOCTYPE html>
<!--
    Forty by HTML5 UP
    html5up.net | @ajlkn
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
  -->
<html>

  <head>
	<title>TECHNOLOGY PRÉCIS!</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="/assets/css/main.css" />
	<!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->
</head>


  <body>

    <!-- Wrapper -->
<div id="wrapper">

<!-- Header -->
<header id="header">
	<a href="http://localhost:4000//" class="logo"><strong>TECHNOLOGY PRÉCIS!</strong> <span>by Sivan Sasidharan</span></a>
	<nav>
		<a href="#menu">Menu</a>
	</nav>
</header>

<!-- Menu -->
<nav id="menu">
	<ul class="links">
        
		    
		
		    
		
		    
		
		    
		
		    
		        <li><a href="http://localhost:4000//">Home</a></li>
	    	
		
		    
		
		    
		
		    
		
		
		    
		
		    
		        <li><a href="http://localhost:4000/all_posts/">WriteUps</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/freferences/">References</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/generic/">Experience</a></li>
		    
		
		    
		
		    
		        <li><a href="http://localhost:4000/landingnews/">News Feeds</a></li>
		    
		
		    
		
		    
		        <li><a href="http://localhost:4000/zlanding/">Webinars</a></li>
		    
		
	</ul>
	<ul class="actions vertical">
		<li><a href="#" class="button special fit">Get Started</a></li>
		<li><a href="#" class="button fit">Log In</a></li>
	</ul>
</nav>
 
    
    <!-- Main -->
    <div id="main" class="alt">

      <!-- One -->
                <h4 align="center">Technical Know-Hows!</h4>

      <section id="one">
	<div class="inner">

          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/Performance-Tuning-SPSS-Analytic-Server/" title="Performance tuning">Performance tuning</a></h3>
      <p>Performance tuning - SPSS Analytic Server</p>
      <h6> Posted on : 2018-03-14 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><p><strong>Performance tuning - SPSS Analytic Server</strong></p>
<p>Analytic Server is a component in the Ambari framework that utilizes other components such as HDFS, Yarn and Spark. Common performance tuning techniques for Hadoop, HDFS and Spark apply to Analytic Server workloads.</p>
<blockquote>
<p>Every Analytic Server workload is different therefore tuning<br />
experimentation is required based on your speciﬁc deployments workload</p>
</blockquote>
<p>The following properties and tuning tips are key changes that have impacted the results of the Analytic Server benchmarking and scaling tests.</p>
<p>When the ﬁrst job runs on Analytic Server, the server will start a persistent Spark application that will be active until the Analytic Server is shut down. The persistent Spark application will allocate and hold onto all the cluster resources allocated to it for the duration of the Analytic Server running, even if an Analytic Server job is not actively running. Careful thought should be given to the amount of resources allocated to the Analytic Server Spark application. If all cluster resources are allocated to the Analytic Server Spark application, then other jobs could be delayed or not run. These jobs could be queued waiting for sufﬁcient free<br />
resources and those resources will be consumed by Analytic Server Spark application.</p>
<p><code>If multiple Analytic Server services are conﬁgured and deployed, each service instance could potentially allocate its own persistent Spark application.</code></p>
<p>For example, if two Analytic Server services are deployed to support high availability failover, then you could see two persistent Spark applications active, each allocating cluster resources.</p>
<p>An additional complexity is that in certain situations, Analytic Server may start a map reduce job that will require cluster resources. These map reduce jobs will require resources that are not allocated to the Spark application. The speciﬁc components that require map reduce jobs are PSM model builds.</p>
<hr />

<p>The following properties can be conﬁgured to allocate resources to Spark application. If they are set in the <em>spark-defaults.conf</em> of the Spark installation, then they are allocated for all Spark jobs run in the environment. If they are set in the Analytic Server conﬁguration as custom poperties under the “<em>Custom analytic.cfg</em>” section, then they are allocated for the Analytic Server Spark application only.</p>
<pre><code>#spark.executor.memory
Amount of memory to use per executor process.
#spark.executor.instances
The number of executor processes to start
#spark.executor.cores - 
The number of executor worker threads per executor process.
</code></pre>
<p>An example of setting the three key Spark properties. There are 10 data nodes in a HDFS cluster and each data node has 24 logical cores and 48 GB of memory and is only running HDFS processes. Here is one way to conﬁgure the properties for this environment, assuming you are only running Analytic Server jobs on this environment and desire maximum allocation to a single Analytic Server Spark application.</p>
<ul>
<li>
<p>Set <strong>spark.executor.instances=20</strong><br />
This would attempt to run 2 Spark executor processes per data node.</p>
</li>
<li>
<p>Set <strong>spark.executor.memory=22G</strong><br />
This would set the max heap size for each Spark executor process to 22 GB, allocating 44 GB on each data node. Other JVMs and the OS need the extra memory.</p>
</li>
<li>
<p>Set <strong>spark.executor.cores=5</strong><br />
This will provide 5 worker threads for each Spark executor, for a total of 10 worker threads per data node.</p>
</li>
</ul>
<p><strong>Monitor the Spark UI for running jobs</strong></p>
<p>If you see Spill to disk that could impact performance.<br />
Some possible solutions are:</p>
<ol>
<li>
<p>Increase memory and allocate it to Spark executors via<br />
spark.executor.memory</p>
</li>
<li>
<p>Reduce the number of spark.executor.cores. This will reduce the number of concurrent work threads allocating memory, but it will also reduce the amount of parallelism for the jobs.</p>
</li>
<li>
<p>Change the Spark memory properties. <em>spark.shuffle.memoryFraction</em> and <em>spark.storage.memoryFraction</em> allocation percentage of the Spark executor heap for Spark.</p>
</li>
</ol>
<p><strong>Ensure the name node has enough memory</strong><br />
If the number of blocks in HDFS is large and growing, ensure you name node heap increases to accommodate this growth. This is a common HDFS tuning recommendation.</p>
<p><strong>Alter the amount of memory used for caching</strong><br />
By default, spark.storage.memoryFraction has value 0.6. This can be increased up to 0.8 in case the HDFS block size of the data is 64MB. If the HDFS block size of the input data is greater than 64MB then this value could be increased only if the memory allocated per task is greater than 2GB.</p>
<p><strong>Spark map-side join</strong><br />
The Analytic Server Spark join implementation does not support map-side join functionality (Spark join is mainly a reduce side). The implementation does not take advantage of map-side joins to optimize joins when one input is small. Not taking advantage of map-side join results in an extremely resource intensive Spark job that eventually fails.</p>
<hr />

<h5 id="references">References</h5>
<p><a href="https://spark.apache.org/">Apache Spark</a><br />
<a href="https://hortonworks.com/blog/">Hortonworks Blog</a><br />
<a href="https://ibm.com/in-en/marketplace/spss-analytic-server">IBM SPSS Analytic Server</a></p>

</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/Spark-(Streaming)-Kafka-with-Kerberos-in-HDP-2.6/" title="Spark (Streaming) - Kafka on Kerberized HDP">Spark (Streaming) - Kafka on Kerberized HDP</a></h3>
      <p>Spark (Streaming) - Kafka on Kerberized HDP</p>
      <h6> Posted on : 2018-02-14 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><p>The common issues we probably encounter when doing debugging. The environment is HDP 2.6.1.0 with FreeIPA kerberized, Spark 1.6.3 and Kafka 0.10.1<br />
Refer to the test code from below link [–&gt; added lines enabling it running in kerberized.]<br />
<a href="https://github.com/eBay/Spark/blob/master/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java">JavaDirectKafkaWordCount</a></p>
<pre><code>package spark.example;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Arrays;
import java.util.regex.Pattern;
import scala.Tuple2;
import com.google.common.collect.Lists;
import kafka.serializer.StringDecoder;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.function.*;
import org.apache.spark.streaming.api.java.*;
import org.apache.spark.streaming.kafka.KafkaUtils;
import org.apache.spark.streaming.Durations;
public final class JavaDirectKafkaWordCount {
    private static final Pattern SPACE = Pattern.compile( " " );
    public static void main(String[] args) {
        // Create context with a 2 seconds batch interval
        SparkConf sparkConf = new SparkConf().setAppName("JavaDirectKafkaWordCount");
        JavaStreamingContext jssc = new JavaStreamingContext( sparkConf, Durations.seconds( 200 ) );
        HashSet&lt;String&gt; topicsSet = new HashSet&lt;String&gt;( Arrays.asList( "kerb" ) );
        HashMap&lt;String, String&gt; kafkaParams = new HashMap&lt;String,String&gt;();
        kafkaParams.put("bootstrap.servers", "local0.field.hortonworks.com:6667");
        kafkaParams.put("group.id", "test_sparkstreaming_kafka");
        kafkaParams.put("auto.offset.reset", "largest");
        kafkaParams.put("security.protocol", "PLAINTEXTSASL");
        System.setProperty("java.security.auth.login.config","/usr/hdp/current/kafka-broker/config/kafka_client_jaas.conf");
        System.setProperty("java.security.krb5.conf", "/etc/krb5.conf");
        // Create direct kafka stream with brokers and topics
        JavaPairInputDStream&lt;String, String&gt; messages = KafkaUtils.createDirectStream(
                jssc,
                String.class,
                String.class,
                StringDecoder.class,
                StringDecoder.class,
                kafkaParams,
                topicsSet
        );
        // Get the lines, split them into words, count the words and print
        JavaDStream&lt;String&gt; lines = messages.map( new Function&lt;Tuple2&lt;String, String&gt;, String&gt;() {
            @Override
            public String call(Tuple2&lt;String, String&gt; tuple2) {
                return tuple2._2();
            }
        } );
        JavaDStream&lt;String&gt; words = lines.flatMap( new FlatMapFunction&lt;String, String&gt;() {
            @Override
            public Iterable&lt;String&gt; call(String x) {
                return Lists.newArrayList( SPACE.split( x ) );
            }
        } );
        JavaPairDStream&lt;String, Integer&gt; wordCounts = words.mapToPair(
                new PairFunction&lt;String, String, Integer&gt;() {
                    @Override
                    public Tuple2&lt;String, Integer&gt; call(String s) {
                        return new Tuple2&lt;String, Integer&gt;( s, 1 );
                    }
                } ).reduceByKey(
                new Function2&lt;Integer, Integer, Integer&gt;() {
                    @Override
                    public Integer call(Integer i1, Integer i2) {
                        return i1 + i2;
                    }
                } );
        wordCounts.print();
        // Start the computation
        jssc.start();
        jssc.awaitTermination();
    }
} 
</code></pre>

<p>The jaas configuration file wraps the principal and its keytab path.<br />
Here we have directly used the kafka service account. Ideally you should build the user account for this.</p>
<pre><code>[root@local0 ~]# cat /usr/hdp/current/kafka-broker/config/kafka_client_jaas.confKafkaClient
KafkaClient {
com.sun.security.auth.module.Krb5LoginModule required
useKeyTab=true
doNotPrompt=true
principal="kafka/local0.field.hortonworks.com@test.hortonworks.com"
keyTab="kafka.service.keytab"
useTicketCache=true
renewTicket=true
serviceName="kafka";
};
Client {
com.sun.security.auth.module.Krb5LoginModule required
useKeyTab=true
doNotPrompt=true
principal="kafka/local0.field.hortonworks.com@test.hortonworks.com"
keyTab="kafka.service.keytab"
useTicketCache=true
renewTicket=true
serviceName="kafka";
}; 
</code></pre>
<p>Details on the submission command as below -</p>
<pre><code>//spark job for kerberized hdp 2.5
spark-submit --master=yarn --deploy-mode=cluster \
--files /usr/hdp/current/kafka-broker/config/kafka_client_jaas.conf,/etc/security/keytabs/kafka.service.keytab \
--conf "spark.executor.extraJavaOptions=-Djava.security.auth.login.config=kafka_client_jaas.conf" \
--conf "spark.driver.extraJavaOptions=-Djava.security.auth.login.config=kafka_client_jaas.conf" \
--conf "spark.yarn.historyServer.address=http://local0.field.hortonworks.com:18080"  \
--conf "spark.eventLog.dir=hdfs:///spark-history"  \
--conf "spark.eventLog.enabled=true" \
--jars /usr/hdp/current/spark-client/lib/spark-assembly-1.6.3.2.6.1.0-129-hadoop2.7.3.2.6.1.0-129.jar,/usr/hdp/current/kafka-broker/libs/kafka_2.10-0.10.1.2.6.1.0-129.jar,/usr/hdp/current/spark-client/lib/spark-examples-1.6.3.2.6.1.0-129-hadoop2.7.3.2.6.1.0-129.jar \
--class spark.example.JavaDirectKafkaWordCount kafka-producer-1.0-SNAPSHOT.jar
There is no need such in HDP 2.6
//spark job for kerberized hdp 2.6
spark-submit --master=yarn --deploy-mode=cluster \
--files /usr/hdp/current/kafka-broker/config/kafka_client_jaas.conf,/etc/security/keytabs/kafka.service.keytab \
--conf "spark.executor.extraJavaOptions=-Djava.security.auth.login.config=kafka_client_jaas.conf" \
--conf "spark.driver.extraJavaOptions=-Djava.security.auth.login.config=kafka_client_jaas.conf" \
--jars /usr/hdp/current/spark-client/lib/spark-assembly-1.6.3.2.6.1.0-129-hadoop2.7.3.2.6.1.0-129.jar,/usr/hdp/current/kafka-broker/libs/kafka_2.10-0.10.1.2.6.1.0-129.jar,/usr/hdp/current/spark-client/lib/spark-examples-1.6.3.2.6.1.0-129-hadoop2.7.3.2.6.1.0-129.jar \
--class spark.example.JavaDirectKafkaWordCount kafka-producer-1.0-SNAPSHOT.jar 
</code></pre>
<p>Details on the above code and command, refer  <a href="https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.3/bk_spark-guide/content/spark-streaming-kafka-kerb.html">https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.3/bk_spark-guide/content/spark-streaming-kafka-kerb.html</a></p>
<hr />

<ol>
<li><strong>Zookeeper access right check</strong></li>
</ol>
<p>Because Kafka need to read topics and offset information from zookeeper znode, check if the acl satisfies</p>
<pre><code>[root@local0 tmp]# cd /usr/hdp/current/zookeeper-client/bin
[root@local0 bin]# ./zkCli.sh -server local0:2181
[zk: local0:2181(CONNECTED) 0] getAcl /
'world,'anyone
: cdrwa
[zk: local0:2181(CONNECTED) 2] getAcl /consumers
'world,'anyone
: cdrwa
[zk: local0:2181(CONNECTED) 4] getAcl /brokers   
'world,'anyone
: cdrwa 
</code></pre>
<hr />

<ol start="2">
<li><strong>HDFS access right check</strong></li>
</ol>
<pre><code>[root@local0 ~]# hdfs dfs -ls /user
Found 10 items
drwxrwx--- - ambari-qa hdfs0 2017-09-17 13:34 /user/ambari-qa
drwxr-xr-x - hbase hdfs0 2017-08-20 00:08 /user/hbased
rwxr-xr-x - hcathdfs0 2017-08-06 00:55 /user/hcat
drwx------ - hdfshdfs0 2017-09-28 03:10 /user/hdfs
drwxr-xr-x - hivehdfs0 2017-08-06 00:55 /user/hive
drwxr-xr-x - kafka hdfs0 2017-09-28 12:23 /user/kafka
drwxrwxr-x - oozie hdfs0 2017-08-06 00:56 /user/oozie
drwxr-xr-x - hdfshdfs0 2017-08-13 11:47 /user/root
drwxrwxr-x - spark hdfs0 2017-09-08 16:54 /user/spark
drwxr-xr-x - yarnhdfs0 2017-09-16 22:17 /user/yarn 
</code></pre>
<p>Note by default there did not create path “/user/kafka”, so you need do it by hand and give right access</p>
<pre><code>[root@local0 ~]# hdfs dfs -mkdir /user/kafka
[root@local0 ~]# hdfs dfs -chown kafka:hdfs /user/kafka 
</code></pre>
<hr />

<ol start="3">
<li><strong>Jaas configuration file and Keytab file access right</strong></li>
</ol>
<p>Sparkstreaming job here is submitted in yarn cluster, which then plays as yarn user to read jaas and keytab files, should check the file acl. By default keytab file only its principal user to read, other users cannot access. You should grant other user to read those two files.</p>
<pre><code>[root@local0 ~]# ls -l /usr/hdp/current/kafka-broker/config/kafka_client_jaas.conf
-rw-r--r-- 1 kafka hadoop 560 Sep 26 13:35 /usr/hdp/current/kafka-broker/config/kafka_client_jaas.conf
[root@local0 ~]# ls -l /etc/security/keytabs/kafka.service.keytab
-r--r--r-- 1 kafka hadoop 208 Aug6 01:53 /etc/security/keytabs/kafka.service.keytab 
</code></pre>
<ol start="4">
<li><strong>Specify path of Jaas and Keytab files</strong></li>
</ol>
<p>As emphasized in most Spark articles, Sparkstreaming driver wrap Jaas and Keytab files and sent to executor to parse. The very important step is specify the path on the driver machine. There is no need to hand-copy those files to all slave nodes.</p>
<pre><code>--files /usr/hdp/current/kafka-broker/config/kafka_client_jaas.conf,/etc/security/keytabs/kafka.service.keytab \
--conf "spark.executor.extraJavaOptions=-Djava.security.auth.login.config=kafka_client_jaas.conf" \
--conf "spark.driver.extraJavaOptions=-Djava.security.auth.login.config=kafka_client_jaas.conf" \ 
</code></pre>
<p>If there's some configurations missed as detailed above , you will likely come across error(s) saying :<br />
<em>org.apache.spark.SparkException:  Couldn’t connect to leader for topic</em></p>
<p>or</p>
<p><em>javax.security.auth.login.LoginException:Unable to obtain password from user</em><br />
or</p>
<p><em>org.apache.kafka.common.KafkaException: org.apache.kafka.common.KafkaException:Jaas configuration not found</em></p>
<pre><code>[root@local0 tmp]# yarn application -list -appStates RUNNING
17/09/30 22:22:08 INFO client.RMProxy: Connecting to ResourceManager at local1.field.hortonworks.com/172.26.197.243:8050
17/09/30 22:22:09 INFO client.AHSProxy: Connecting to Application History server at local1.field.hortonworks.com/172.26.197.243:10200
Total number of applications (application-types: [] and states: [RUNNING]):3
                Application-Id    Application-Name    Application-Type      User     Queue             State       Final-State       Progress                       Tracking-URL
application_1505572583831_0056spark.example.JavaDirectKafkaWordCount               SPARK     kafka   default           RUNNING         UNDEFINED            10%        http://172.26.197.246:37394
</code></pre>
<ul>
<li><strong>Use Yarn resource manager and History server webpage to check log</strong></li>
</ul>
<p>Yarn resource manager helps you dive into containers’ logs since spark execute tasks distributed across contained executors. While History server enable you to check spark DAG and full stack printed on each executor, and the more is collecting metrics help to do problem analysis.</p>
<ul>
<li><strong>Maven: specify HDP repository and dependent jar for compiling purpose</strong></li>
</ul>
<p>Should align with HDP release package and have jar files searched from HDP official repository</p>
<pre><code>&lt;repositories&gt;
    &lt;repository&gt;
        &lt;releases&gt;
            &lt;enabled&gt;true&lt;/enabled&gt;
        &lt;/releases&gt;
        &lt;snapshots&gt;
            &lt;enabled&gt;true&lt;/enabled&gt;
        &lt;/snapshots&gt;
        &lt;id&gt;hortonworks.extrepo&lt;/id&gt;
        &lt;name&gt;Hortonworks HDP&lt;/name&gt;
        &lt;url&gt;http://repo.hortonworks.com/content/repositories/releases&lt;/url&gt;
    &lt;/repository&gt;
    &lt;repository&gt;
        &lt;releases&gt;
            &lt;enabled&gt;true&lt;/enabled&gt;
        &lt;/releases&gt;
        &lt;snapshots&gt;
            &lt;enabled&gt;true&lt;/enabled&gt;
        &lt;/snapshots&gt;
        &lt;id&gt;hortonworks.other&lt;/id&gt;
        &lt;name&gt;Hortonworks Other Dependencies&lt;/name&gt;
        &lt;url&gt;http://repo.hortonworks.com/content/groups/public&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;&lt;br&gt;
</code></pre>
<p>For this test code, we have following dependent package</p>
<pre><code>&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
        &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
        &lt;version&gt;0.11.0.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;
        &lt;version&gt;2.7.3&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
        &lt;version&gt;2.7.3&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-streaming-kafka_2.10&lt;/artifactId&gt;
        &lt;version&gt;1.6.3.2.6.1.0-129&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt;
        &lt;version&gt;1.6.3.2.6.1.0-129&lt;/version&gt;
        &lt;scope&gt;provided&lt;/scope&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;&lt;br&gt;
</code></pre>
<p>Once compiled the target jar, only pick non-dependencies to execute, and specify path of dependent jars in command.</p>
<pre><code>--jars /usr/hdp/current/spark-client/lib/spark-assembly-1.6.3.2.6.1.0-129-hadoop2.7.3.2.6.1.0-129.jar,/usr/hdp/current/kafka-broker/libs/kafka_2.10-0.10.1.2.6.1.0-129.jar,/usr/hdp/current/spark-client/lib/spark-examples-1.6.3.2.6.1.0-129-hadoop2.7.3.2.6.1.0-129.jar \
--class spark.example.JavaDirectKafkaWordCount kafka-producer-1.0-SNAPSHOT.jar
</code></pre>
<p>**Use Kafka tool to test<br />
After starting the above streaming process, meanwhile initiate an producer using</p>
<pre><code>[root@local0 bin]# ./kafka-console-producer.sh --broker-list local0:6667 --topic kerb--security-protocol PLAINTEXTSASL
Test content..
</code></pre>
<p>Check the output log generated from SparkStreaming process, either from yarn or spark history server.</p>
<p>o/p #Time: 1506791200000 ms<br />
(Kafka,1) (2.6,1) (HDP,1) (Other,1) (,1) (Connecting,1) (Kerberized,1) (Notable,1) (Issue,1) (in,1) …</p>

</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/Benchmarking-Kafka/" title="Benchmarking Kafka">Benchmarking Kafka</a></h3>
      <p>Benchmarking Kafka on an HDP Cluster</p>
      <h6> Posted on : 2018-01-29 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><p>Kafka is composed by producer, broker and consumer. HDP cluster only holds kafka broker, however producer and consumer more often imbeds in application implementation, and they take majority influence on entire Kafka performance. The following benchmark is based analysis on a few production grade clusters on HDP.</p>
<h4 id="kafka-producer">Kafka Producer</h4>
<p>Kafka producer benchmark focusing on batch.size and <a href="http://linger.ms">linger.ms</a><br />
<img src="/downloads/HDP-Kafka.png" alt="kafka producer" /></p>
<p><strong>Batch.size</strong> A small batch size will make batching less common and may reduce throughput (a batch size of zero will disable batching entirely). A very large batch size may use memory a bit more wastefully as we will always allocate a buffer of the specified batch size in anticipation of additional records.</p>
<p><strong><a href="http://Linger.ms">Linger.ms</a></strong> The producer groups together any records that arrive in between request transmissions into a single batched request. Normally this occurs only under load when records arrive faster than they can be sent out. However in some circumstances the client may want to reduce the number of requests even under moderate load. This setting accomplishes this by adding a small amount of artificial delay—that is, rather than immediately sending out a record the producer will wait for up to the given delay to allow other records to be sent so that the sends can be batched together. This can be thought of as analogous to Nagle’s algorithm in TCP. This setting gives the upper bound on the delay for batching: once we get batch.size worth of records for a partition it will be sent immediately regardless of this setting, however if we have fewer than this many bytes accumulated for this partition we will ‘linger’ for the specified time waiting for more records to show up. This setting defaults to 0 (i.e. no delay). Setting linger.ms=5, for example, would have the effect of reducing the number of requests sent but would add up to 5ms of latency to records sent in the absence of load.</p>
<p>The relationship b/w batch size and throughput can be depicted by the following curve. When batch.size is increased by 10,000, the average throughput start increasing marginally, and keeps at 5 MB/sec or so. When batch.size grows by 2500, the average latency stop increasing and stay at 6 ms around.</p>
<p><img src="/downloads/Kafka-benchmarking.jpg" alt="batch size and throughput" /></p>
<p>** Capture the average throughput and delay when gradually increasing batch size - use the <a href="http://kafka-producer-perf-test.sh">kafka-producer-perf-test.sh</a> scripts for the same.</p>
<p><code>./kafka-producer-perf-test.sh -num-records 5000000 -record-size 1000 -topic testkafka -throughput 100000 -num-threads 2 -value-bound 5000 - print-metrics - producer-props bootstrap.servers=x.x.x.x:6667 compression.type=gzip max.in.flight.request.per.connection=1 batch.size = 2000 security.protocol=SASL_PLAINTEXT</code></p>
<blockquote>
<p><strong>By tuning <a href="http://linger.ms">linger.ms</a> we can promote the glaring gap.</strong></p>
</blockquote>
<p>####Kafka Consumer</p>
<blockquote>
<p><strong>Kafka consumer focus on fetch-size and throughput.</strong></p>
</blockquote>
<p>Use the below command to get the required metrics -</p>
<p><code>java -Djava.security.auth.login.config=/temp/kafka_jaas.conf \ -Djava.security.krb5.conf=/etc/krb5.conf \ -Djavax.security.auth.useSubjectCredsOnly=true \ -cp /usr/hdp/current/kafka-broker/libs/scala-library-2.11.8.jar:/temp/consumer/jopt-simple-5.0.4.jar:/usr/hdp/current/kafka-broker/libs/*:/temp/consumer/kafka_2.11-1.1.0-SNAPSHOT.jar kafka.tools.ConsumerPerformance \ --new-consumer --broker-list 10.0.0.18:6667 --topic chen --messages 5000000 --fetch-size 1000000 --show-detailed-stats --from-latest --print-metrics \ --consumer.config /usr/hdp/2.6.1.0-129/etc/kafka/conf.default/consumer.properties</code></p>
<p>fetch-size &lt;Integer: size&gt; controls the number of bytes of messages to attempt to fetch in one request to the Kafka server.<br />
Messages &lt;Long: count&gt; Required: the number of messages to consume</p>
<p>This gives the below information -</p>
<ol>
<li>Throughput  [<strong>MB.sec</strong>]</li>
<li>For each 5 second iteration  the number of newly consumed messages [<strong>data.consumed.in.nMsg</strong>]; which is aligned record size set in producer</li>
<li>Recieved messages [<strong>data.consumed.in.nMsg</strong>]</li>
<li>Calculate --&gt; <strong>records-consumed-rate</strong> ~ (records-per-request-avg)*(fetch-rate)</li>
<li>Calculate --&gt; <strong>bytes-consumed-rate</strong> ~ (fetch-size-avg)*(fetch-rate)</li>
</ol>
<p><strong>Check if by reducing fetch-size down the throughput increases</strong></p>

</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/Comparing-Hadoop-Spark-Flink/" title="Hadoop vs Spark vs Flink">Hadoop vs Spark vs Flink</a></h3>
      <p>Comparing the Three Frameworks - Hadoop/Spark/Flink</p>
      <h6> Posted on : 2017-09-22 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><blockquote>
<p><strong>Data Processing</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hadoop was designed for batch processing, that means it takes large dataset in input, all at once, processes it and produces the result. Batch processing is very efficient in processing in high volume data. Depending on the size of the data being processed and the computational power of the system, output can be delayed significantly</td>
<td>Apache Spark is also a part of Hadoop Ecosystem, it is a batch processing System at heart too but it also supports stream processing</td>
<td>Flink provides single runtime for the streaming and as well batch processing so one common runtime is utilized for data streaming application and batch processing application</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Streaming Engine</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>Map-reduce is batch- oriented processing tool. It takes large dataset in input, all at once, processes it and produces the result</td>
<td>Spark Streaming processes data streams in micro-batches, where each batch contains a collection of events that arrived over the batch period. But it is not sufficient for use cases where we need to process large streams of live data and provide results in real time</td>
<td>Apache Flink is the true streaming engine that uses streams for workloads: streaming, SQL, micro-batch and batch. Batch is a finite set of streamed data</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Data Flow</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>MapReduce computation dataflow does not have any loops, it is a chain of stages; at each stage you progress forward using output of previous stage and producing input for the next stage.</td>
<td>Though Machine Learning algorithm is a cyclic data flow, it is represented as direct acyclic graph inside the spark.</td>
<td>Flink takes a different approach than others. It supports controlled cyclic dependency graph in run time. This helps it in representing the Machine Learning algorithms in a very efficient way</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Computation Model</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>MapReduce adopted batch-oriented model. Batch is essentially processing data at rest, taking a large amount of data at once, processing it and then writing out the output</td>
<td>Spark has adopted micro-batching. Micro-batches are an essentially “collect and then process” kind of computational model</td>
<td>Flink has adopted a continuous flow, operator-based streaming model. A continuous flow operator processes data when it arrives, without any delay in collecting the data or processing the data</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Performance</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hadoop supports batch processing only. It doesn’t process streamed data hence overall performance is slower when compared</td>
<td>Though Apache Spark has an excellent community background and now It is considered as most matured community. But Its stream processing is not much efficient than Apache Flink as it uses micro-batch processing</td>
<td>Overall performance of Apache Flink is excellent as compared to any other data processing system. Apache Flink uses native closed loop iteration operators which makes machine learning and graph processing more faster when we compare Flink and Spark and Hadoop</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Memory management</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hadoop provides configurable Memory management. You can do it dynamically or statically</td>
<td>Spark provides configurable memory management. The latest release of Spark has moved towards automating memory management</td>
<td>Flink provides automatic memory management. It has its own memory management system, separate from Java’s garbage collector</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Fault tolerance</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>MapReduce is highly fault tolerant. There is no need to restart the application from scratch in case of any failure in Hadoop</td>
<td>Spark Streaming recovers lost work and with no extra code or configuration, it delivers exactly-once semantics out of the box</td>
<td>The fault tolerance mechanism followed by Apache Flink is based on Chandy-Lamport distributed snapshots. The mechanism is lightweight, which results in maintaining high throughput rates and provide strong consistency guarantees at the same time</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Scalability</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>MapReduce has incredible scalability potential and has been used in production on tens of thousands of Nodes</td>
<td>Spark is highly scalable, we can keep adding n number of nodes in the cluster. A large known spark cluster is of 8000 nodes</td>
<td>Flink is also highly scalable, we can keep adding n number of nodes in the cluster A large known Flink cluster is of thousands of nodes</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Iterative Processing</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>Does not support iterative processing</td>
<td>Spark iterates its data in batches In Spark, each iteration has to be scheduled and executed separately</td>
<td>Flink iterates data by using its streaming architecture. Flink can be instructed to only process the parts of the data that have actually changed, thus significantly increasing the performance of the job</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Language Support</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hadoop Supports Primarily Java, other languages supported are c, c++, ruby, groovy, Perl, python</td>
<td>Spark supports java, Scala, python and R. Spark is implemented in scala, it provides API in other languages like Java, Python, and R.</td>
<td>Flink Supports java, Scala, python and R. Flink is implemented in java. It does provide Scala API too</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Optimization</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>In MapReduce, jobs have to be manually optimized. There are several ways to optimize the MapReduce Jobs: Configure your cluster correctly, use a combiner , use LZO compression, tune the number of MapReduce Task appropriately and use the most appropriate and compact writable type for your data</td>
<td>In Apache Spark, jobs have to be manually optimized. There is a new extensible optimizer, Catalyst, based on functional programming construct in scala. Catalyst’s extensible design had two purposes: First, easy to add new optimization techniques. Second, enable external developers to extend the optimizer catalyst</td>
<td>Flink comes with an optimizer that is independent with actual programming interface. The Flink optimizer works similarly to a relational Database Optimizer, but applies these optimizations to the Flink programs, rather than SQL queries</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p>Latency</p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>The MapReduce framework of Hadoop is relatively slower since it is designed to support different format, structure and huge volume of data. That’s why Hadoop has higher latency than both spark and Flink</td>
<td>Apache Spark is yet another batch processing system but it is relatively faster than Hadoop MapReduce since it caches much of the input data on memory by RDD and keeps intermediate data in memory itself, eventually writes the data to disk upon completion or whenever required.</td>
<td>With minimum efforts in configuration, Apache Flink’s data streaming runtime achieves low latency and high throughput</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Processing Speed</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>MapReduce processes slower than spark and flink. The slowness occurs only because of the nature of the MapReduce based execution, where it produces lots of intermediate data, much data exchanged between nodes, thus causes huge disk IO latency. Furthermore, it has to persist much data in disk for synchronization between phases so that it can support Job recovery from failures. Also, there are no ways in MapReduce to cache all subset of the data in memory</td>
<td>Spark processes faster than MapReduce because it caches much of the input data on memory by RDD and keeps intermediate data in memory itself, eventually writes the data to disk upon completion or whenever required. Spark is 10x times faster than mapreduce and this shows how spark is better than Hadoop MapReduce</td>
<td>Flink processes faster than Spark because of its streaming architecture. Flink can be instructed to only process the parts of the data that have actually changed, thus significantly increasing the performance of job</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Visualization</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hadoop data visualization tool is zoomdata that can connect directly to HDFS as well as to SQL-on-Hadoop technologies such as Impala, Hive, Spark SQL, Presto and more</td>
<td>Spark offers a web interface for submitting and executing jobs on which the resulting execution plan can be visualized. Flink and Spark both are integrated to Apache zeppelin It provides data analytics, ingestion, as well as discovery, visualization, and collaboration</td>
<td>Flink also offers a web interface for submitting and executing jobs. The resulting execution plan can be visualized on this interface</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Recovery</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>MapReduce is naturally resilient to system faults or failures. It is highly fault tolerant system</td>
<td>Spark RDDs allow recovery of partitions on failed nodes by re-computation of the DAG while also supporting a more similar recovery style to Hadoop by way of checkpointing, to reduce the dependencies of RDDs</td>
<td>Flink supports checkpointing mechanism that stores the program in the data sources and data sink, the state of window, as well as user-defined state that recovers streaming job after failure</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Security</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hadoop supports Kerberos authentication, which is somewhat painful to manage. However, third party vendors have enabled organizations to leverage Active Directory Kerberos and LDAP for authentication.</td>
<td>Spark’s security is a bit sparse by currently only supporting authentication via shared secret (password authentication). The security bonus that Spark can enjoy is that if you run Spark on HDFS, it can use HDFS ACLs and file-level permissions. Additionally, Spark can run on YARN to use Kerberos authentication</td>
<td>There is user-authentication support in Flink via the Hadoop / Kerberos infrastructure. If you run Flink on YARN, Flink acquires the Kerberos tokens of the user that submits programs, and authenticate itself at YARN, HDFS, and HBase with that.Flink’s upcoming connector, streaming programs can authenticate themselves as stream brokers via SSL</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Cost</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>MapReduce can typically run on less expensive hardware than some alternatives since it does not attempt to store everything in memory</td>
<td>As spark requires a lot of RAM to run in-memory, increasing it in cluster, gradually increases its cost.</td>
<td>Flink also requires a lot of RAM to run in-memory, so it will increase its cost gradually.</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Compatibility</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hadoop MapReduce and Apache Spark are compatible with each other and Spark shares all MapReduce’s compatibilities for data sources, file formats and business intelligence tools via JDBC and ODBC</td>
<td>Spark and hadoop are compatible to each other. Spark is compatible with Hadoop data. It can run in Hadoop clusters through YARN or Spark’s standalone mode, and it can process data in HDFS, HBase, Cassandra, Hive, and any Hadoop InputFormat</td>
<td>Flink is a scalable data analytics framework that is fully compatible to Hadoop. It provides a Hadoop Compatibility package to wrap functions implemented against Hadoop’s MapReduce interfaces and embed them in Flink programs</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Interactive Mode</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>MapReduce does not have interactive Mode</td>
<td>Spark has an interactive shell to learn how to make the most out of Apache Spark. This is a Spark application written in Scala to offer a command-line environment with auto-completion where you can run ad-hoc queries and get familiar with the features of Spark</td>
<td>Flink comes with an integrated interactive Scala Shell. It can be used in a local setup as well as in a cluster setup</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Real time Analysis</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>MapReduce fails when it comes to real-time data processing as it was designed to perform batch processing on voluminous amounts of data</td>
<td>It can process real time data ie data coming from the real-time event streams at the rate of millions of events per second</td>
<td>It is mainly used for real-time data Analysis Although it also provides fast batch data Processing</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Abstraction</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>In Mapreduce, we don’t have any type of abstraction</td>
<td>In Spark, for batch we have Spark RDD abstraction and DStream for streaming which is internally RDD itself</td>
<td>In flink, we have Dataset abstraction for batch and DataStreams for the streaming application</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Machine Learning</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hadoop requires machine learning tool like Apache Mahout</td>
<td>Spark has its own set of machine learning MLlib. Within memory caching and other implementation details, it’s really powerful platform to implement ML algorithms</td>
<td>Flink has FlinkML which is Machine Learning library for Flink. It supports controlled cyclic dependency graph in runtime. This makes them represent the ML algorithms in a very efficient way compared to DAG representation</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Scheduler</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>Scheduler in Hadoop becomes the pluggable component. There are two schedulers for multi user workload: fair Scheduler and capacity Scheduler. To schedule complex flows, MapReduce needs an external job scheduler like Oozie.</td>
<td>Due to in-memory computation, spark acts its own flow scheduler. Can be configured with YARN Scheduler</td>
<td>Flink can use YARN Scheduler but Flink also has its own Scheduler</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>SQL support</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>It enables users to run SQL queries using Apache Hive</td>
<td>It enables users to run SQL queries using Spark-SQL. Spark provides both Hive like query language and Dataframe like DSL for querying structured data</td>
<td>In Flink, Table API is an SQL-like expression language that supports data frame like DSL and it’s still in beta. There are plans to add the SQL interface but not sure when it will land in the framework</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Caching</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>MapReduce cannot cache the data in memory for future requirements</td>
<td>Spark can cache data in memory for further iterations which enhance its performance</td>
<td>Flink can cache data in memory for further iterations to enhance its performance</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Deployment</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>In Standalone mode, Hadoop is configured to run in a single-node, non-distributed mode. In pseudo Distributed mode, Hadoop runs in a pseudo distributed mode. The difference is that each Hadoop daemon runs in a separate java process in pseudo-distributed mode. Whereas in local mode each Hadoop daemon runs as a single java process. In a fully-distributed mode, all daemons are executed in separate nodes forming a multi-node cluster</td>
<td>In addition to running on the Mesos or YARN cluster managers, Spark also provides a simple standalone deploy mode. It can be launched either manually, by starting a master and workers by hand or use our provided launch scripts. It is also possible to run these daemons on a single machine for testing</td>
<td>In addition to running on YARN cluster Managers, Flink also provides standalone deploy mode</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Duplication elimination</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>There is no duplication elimination in Hadoop</td>
<td>Spark also process every record exactly one time hence eliminates duplication.</td>
<td>Apache Flink processes every record exactly one time hence eliminates duplication. Streaming applications can maintain custom state during their computation. Flink’s checkpointing mechanism ensures exactly once semantics for the state in the presence of failures</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Window criteria</strong></p>
</blockquote>
<p>A data stream needs to be grouped into multiple logical streams on each of which a window operator can be applied.</p>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hadoop doesn’t support streaming so there is no need of window criteria</td>
<td>Spark has time-based window criteria</td>
<td>Flink has record-based or any custom user-defined Flink Window criteria</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Back pressure Handing</strong></p>
</blockquote>
<p>BackPressure refers to the buildup of data at an I/O switch when buffers are full and not able to receive additional data. No additional data packets are transferred until the bottleneck of data has been eliminated or the buffer has been emptied.</p>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hadoop handles back pressure through Manual Configuration</td>
<td>Spark also handles back pressure through Manual Configuration</td>
<td>Flink handles back pressure Implicitly through System Architecture</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Hardware Requirements</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>MapReduce runs very well on commodity Hardware</td>
<td>Spark needs mid to high-level hardware because Spark cache data in memory for further iterations which enhance its performance</td>
<td>Flink also needs mid to High-level Hardware. Flink can also cache data in memory for further iterations which enhance its performance.</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>High Availability</strong></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>Configurable in High Availability Mode</td>
<td>Configurable in High Availability Mode</td>
<td>Configurable in High Availability Mode</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Amazon S3 connector</strong><br />
<em>Amazon Simple Storage Service (Amazon S3) is object storage with a simple web service interface to store and retrieve any amount of data from anywhere on the web</em></p>
</blockquote>

<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Spark</th>
<th>Flink</th>
</tr>
</thead>
<tbody>
<tr>
<td>Provides Supports for Amazon S3 Connector</td>
<td>Provides Supports for Amazon S3 Connector</td>
<td>Provides Supports for Amazon S3 Connector</td>
</tr>
</tbody>
</table>
<hr />

<blockquote>
<p><strong>Apache License</strong></p>
</blockquote>
<p>All the three are Apache Licensed.<br />
The Apache License, Version 2.0 (ALv2) is a permissive free software license written by the Apache Software Foundation (ASF). The Apache License requires preservation of the copyright notice and disclaimer.</p>
<hr />

<h5 id="references">References</h5>
<p><a href="http://flink.apache.org">Apache Flink</a><br />
<a href="https://spark.apache.org/">Apache Spark</a><br />
<a href="http://hadoop.apache.org/">Apache Hadoop</a><br />
<a href="http://blog.cloudera.com/blog">Cloudera Blog</a><br />
<a href="https://hortonworks.com/blog/">Hortonworks Blog</a></p>

</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/Kappa_Lambda_Architecture/" title="Kappa Architecture">Kappa Architecture</a></h3>
      <p>Kappa vs Lambda Architecture</p>
      <h6> Posted on : 2016-06-01 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><h5 id="how-to-beat-the-cap-theorem----lambda-architecture">“How to beat the CAP theorem ?” - Lambda Architecture</h5>

<p>Nathan Marz wrote a popular blog post describing an idea he called the <a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html">Lambda Architecture</a>. The Lambda Architecture is an approach to building stream processing applications on top of MapReduce and Storm or similar systems which is widely accepted as of now.</p>

<h5 id="how-does-it-look-like-">How does it look like ?</h5>

<p><img src="/downloads/lambda.png" alt="Lambda Approach" /></p>

<p>The Lambda Architecture is aimed at applications built around complex asynchronous transformations that need to run with low latency (a few seconds to a few hours). An immutable sequence of records is captured and fed into a batch system and a stream processing system in parallel and the transformation logic is implemented twice, once in the batch system and once in the stream processing system. Later we stitch together the results from both these systems at query time to produce the required answer.</p>

<p>We can swap in various similar systems for Kafka, Storm, and Hadoop, and also use two different databases to store the output tables, one optimized for real time and the other optimized for batch updates.A lot of variations are possible.A simple usecase will be a news recommendation system that needs to crawl various news sources, process and normalize all the input, and then index, rank, and store it for serving.</p>

<hr />

<h5 id="pros--cons-of-lambda-architecture">Pros &amp; Cons of Lambda Architecture</h5>

<p><strong>Pros</strong></p>

<ol>
  <li>The Lambda Architecture emphasizes retaining the input data unchanged.</li>
  <li>It highlights the problem of reprocessing data - Reprocessing (processing input data over again to re-derive output) is one of the key challenges of stream processing : for e.g. during code change.</li>
  <li>Emphasise that real-time processing is inherently approximate, less powerful, and more lossy than batch processing( with latency/availability trade-offs in stream processing)</li>
  <li>The Lambda Architecture somehow “beats the CAP theorem” by allowing a mixture of different data systems with different trade-offs</li>
</ol>

<p><strong>Cons</strong></p>

<ol>
  <li>Maintaining code that needs to produce the same result in two complex distributed systems is painful : Programming in distributed frameworks like Storm and Hadoop is complex and also the code ends up being specifically engineered towards the framework it runs on.</li>
  <li>Any new abstraction can only provide the features supported by the intersection of the two systems</li>
  <li>The uber-framework walls off the rich ecosystem of tools and languages that makes Hadoop so powerful (Hive, Pig, Crunch, Cascading, Oozie, etc)</li>
  <li>The operational burden of running and debugging two systems is going to be very high</li>
</ol>

<h5 id="questions-that-raise-are">Questions That Raise Are..</h5>

<ol>
  <li>why can’t the stream processing system just be improved to handle the full problem set in its target domain?</li>
  <li>Why do you need to glue on another system?</li>
  <li>Why can’t you do both real-time processing and also handle the reprocessing when code changes?</li>
  <li>Stream processing systems already have a notion of parallelism; why not just handle reprocessing by increasing the parallelism and replaying history very, very fast?</li>
</ol>

<h4 id="what-is-kappa-architecture">What is “Kappa architecture”?</h4>

<p><img src="/downloads/lambda1.png" alt="Lambda Architecture" />
Figure 1. Lambda architecture. Figure courtesy of Ignacio Mulas Viela and Nicolas Seyvet.</p>

<p>The Lambda architecture is composed of three layers: a batch layer, a real-time (or streaming) layer, and a serving layer. Both the batch and real-time layers receive a copy of the event, in parallel. The serving layer then aggregates and merges computation results from both layers into a complete answer.</p>

<p>The batch layer (aka, historical layer) has two major tasks: managing historical data and re-computing results such as machine learning models. Computations are based on iterating over the entire historical data set. Since the data set can be large, this produces accurate results at the cost of high latency due to high computation time.</p>

<p>The real-time layer( speed layer, streaming layer) provides low-latency results in near real-time fashion. It performs updates using incremental algorithms, thus significantly reducing computation costs, often at the expense of accuracy.</p>

<p>The Kappa architecture simplifies the Lambda architecture by removing the batch layer and replacing it with a streaming layer.
A batch is a data set with a start and an end (bounded), while a stream has no start or end and is infinite (unbounded). Because a batch is a bounded stream, one can conclude that batch processing is a subset of stream processing. Hence, the Lambda batch layer results can also be obtained by using a streaming engine. This simplification reduces the architecture to a single streaming engine capable of ingesting the needed volumes of data to handle both batch and real-time processing. Overall system complexity significantly decreases with Kappa architecture.</p>

<p><img src="/downloads/kappa1.png" alt="Kappa Architecture" />
Figure 2. Kappa architecture. Figure courtesy of Ignacio Mulas Viela and Nicolas Seyvet.</p>

<p><strong>Main Principles of Kappa</strong></p>

<ol>
  <li>Everything is a stream: Batch operations become a subset of streaming operations. Hence, everything can be treated as a stream.</li>
  <li>Immutable data sources: Raw data (data source) is persisted and views are derived, but a state can always be recomputed as the initial record is never changed.</li>
  <li>Single analytics framework: Keep it short and simple (KISS) principle. A single analytics engine is required. Code, maintenance, and upgrades are considerably reduced.</li>
  <li>Replay functionality: Computations and results can evolve by replaying the historical data from a stream.</li>
</ol>

<h5 id="building-the-analytics-pipeline">Building the Analytics Pipeline</h5>
<p>TBD.</p>

<h5 id="references">References</h5>
<p><a href="https://www.oreilly.com/ideas/questioning-the-lambda-architecture">Questioning Lambda</a>
<a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html">CAP Theorem</a>
<a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">Real-Time-Unifying</a>
<a href="http://samza.apache.org/learn/documentation/0.7.0/jobs/reprocessing.html">Samza</a>
<a href="https://www.oreilly.com/ideas/applying-the-kappa-architecture-in-the-telco-industry?imm_mid=0e446a&amp;cmp=em-data-na-na-newsltr_20160601">Applying-the-Kappa-Architecture</a></p>
</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/MemSQL_Lambda/" title="MEMSQL & LAMBDA Architecture">MEMSQL & LAMBDA Architecture</a></h3>
      <p>Simplifying the Lambda Architecture</p>
      <h6> Posted on : 2016-05-27 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><h4 id="lambda-in-a-nutshell">Lambda in a Nutshell</h4>

<p>In the book, ‘Big Data: Principles and best practices of scalable real-time data systems’, Nathan Marz coined the term ‘Lambda Architecture’ to describe a generic, scalable and fault-tolerant data processing architecture based on his experience in working on distributed systems at Backtype and Twitter.</p>

<p>The gist of the Lambda Architecture is to model everything that goes on in a complex computing system as an ordered, immutable log of events. Processing the data (totaling up the number of website visitors) is completed as a series of transformations that output to new tables or streams.It is important to keep the input unchanged. By breaking data processing into independent pieces, each with a defined input and output, you get closer to the ideal of purely functional programming. Writing and testing each piece is made simpler and parallelization can be automated. Parts of the dataflow can be replayed (when code changes or machines fail) and toyed together with other flows.
Most companies have responded to the influx of data by adapting their data management strategy. Most enterprises need instant access to both historical and real-time data and when managing both these data concurrently, the Lambda Architecture is by far the most accepted today.</p>

<h4 id="memsql-with-simplified-lambda-architecture">MEMSQL with Simplified Lambda Architecture</h4>

<p>MemSQL delivers real-time analytics on a rapidly changing data set, making it an ideal match for the characteristics of the Lambda Architecture speed service.It offers a complete solution: <strong>the ability to handle millions of transactions per second while performing complex multi-table join queries.</strong></p>

<p><strong>Scalability</strong></p>

<p>MemSQL uses a distributed shared nothing architecture that scales on commodity hardware and local storage, supporting petabytes of data. MemSQL is a memory-first, relational database that also offers a disk-based columnstore. In-memory optimization provides high-speed data ingestion while simultaneously delivering analytics on the changing data set. The disk-based columnstore provides historical data management and access to historical data trends to leverage in combination with the “hot” data to deliver real-time analytics.</p>

<p><strong>Multi-model, Multi-mode</strong></p>

<p>1) MemSQL supports the ingestion of unstructured, structured and semi-structured data. Flexibility to align a structure to data in support of analytics meets the business requirements of the operation. Real-time analytics requires a real-time data structure, which MemSQL supports through a fully relational model. Furthermore, MemSQL supports the ingestion of unstructured and semi-structured (JSON) data into key-value pairs.</p>

<p>2) Full ANSI SQL support makes MemSQL readily accessible to data analysts, business analysts and data scientists reducing application code requirements. Plugging data visualization and query tools into the analytics architecture delivers immediate value from data to the business.</p>

<p>3) MemSQL also has extended SQL including JSON support. Traversing a JSON document is similar to SQL with extensions to traverse the key-value pairs</p>

<p><strong>Open Source Connectors</strong></p>

<p>MemSQL offers several connectors for smooth integration with additional data sources. Two important ones are :</p>

<p>1) MemSQL Streamliner: an integrated Apache Spark solution. Streamliner provides easy deployment of Apache Spark — a critical component for building real-time data pipelines that delivers advanced data enrichment and transformation.</p>

<p>2) MemSQL Loader : which can import data from HDFS, as well as import and synchronize data from Amazon S3.</p>

<h4 id="applications--usecases">Applications &amp; Usecases</h4>

<p><strong>http://blog.memsql.com/pinterest-apache-spark-use-case/</strong></p>

<p><strong>http://blog.memsql.com/coinalytics-blockchain-analytics/</strong></p>

<p>Applications of today are built with infinite data sets in mind. As these real-time applications become the norm, and batch processing becomes a relic of the past, digital enterprises will implement memory-optimized, distributed data systems to simplify Lambda Architectures for real-time data processing and exploration.</p>

<h4 id="references">References</h4>

<p><a href="http://www.memsql.com/case-studies/">Memsql-CaseStudies</a></p>

<p><a href="http://blog.memsql.com/">MemSQL-Blog</a></p>
</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/ConnectedDataPlatforms-IOA/" title="Connected Data Platforms & Internet of Anything">Connected Data Platforms & Internet of Anything</a></h3>
      <p>Enabling fast analytics on fast data</p>
      <h6> Posted on : 2016-05-20 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><h4 id="connected-data-platforms--internet-of-anything">Connected Data Platforms &amp; Internet of Anything</h4>

<p>Existing data platforms need to evolve to deal with the tsunami of data-in-motion being generated by the Internet of Anything (IoAT).We need a data strategy that is more enterprise-ready and future proof. Hortonwords Connected Data Strategy provides an overview of how enterprises should utilize this (Connected Data Platforms) strategy that handles both data-in-motion and data-at-rest.</p>

<p>–&gt; According to McKinsey, if you’re a Fortune 1000 company and you digitize an asset, you can grow its value by 5x. If you sell through digital channels you can increase productivity by up to 15%. If you digitize your business strategy you can realize a 3x increase in profit margins. This would be impossible without Apache Hadoop and the data lake. processing of large data sets to a fully-fledged data platform with the necessary services for the enterprise from Security to Operational Management and more.
<a href="http://www.mckinsey.com/business-functions/business-technology/our-insights/the-internet-of-things-the-value-of-digitizing-the-physical-world">McKinsey Report</a></p>

<p>–&gt; Today we are moving even beyond the wildest predictions of the growth of data being created, stored, retrieved and analyzed only a few years ago. With 5G the smartphone can potentially deliver a staggering 1Tbps of data!
<a href="http://www.trustedreviews.com/news/5g-researchers-crack-1tbps-data-transfer-at-uk-university">TrustedReviews</a></p>

<p>–&gt; In just two years from now there will be more devices than people on this planet. 35% of Americans Now Own at Least One Smart Device other than a Phone like thermostats, refrigerators, watches all delivering signals.That’s up to 6.4B of connected things, or 21B devices, by 2020.
<a href="https://www.truste.com/about-truste/press-room/35-of-americans-now-own-at-least-one-smart-device-other-than-a-phone/">Truste</a></p>

<p>–&gt; Data types are constantly changing too. Obviously, it’s not just rows and columns any more;image data, all kinds of streaming data, geospatial coordinates, and time series. The one and a half billion monthly active Facebook users has represents more than 140 billion friend connections to be made, 265 billion photos uploaded, 62 million songs played 22B times.
<a href="http://www.ge.com/docs/chapters/Industrial_Internet.pdf">GE.com</a></p>

<p>–&gt; Gartner tells us that 32% of businesses who undertook a digital business transformation say their businesses are now digital businesses.
<a href="http://www.gartner.com/technology/research/digital-business/">Gartner.com</a></p>

<p>According to Forbes:
	- 59% of businesses consider data and analytics to be “vital” to the running of their organizations,
	with a further 29% deeming it “very important”.
	- 69% say there is business case for investing in exploring new ways to add value through
	data projects.
	- 83% say it is making existing services and products more profitable.
	- 60% of businesses claim their data is generating revenue within their organizations.
	- But 48% feel that their organizations have, in the past, failed to take advantage of opportunities to capitalize on their data.</p>

<h5 id="connected-platforms-need-to-evolve">Connected Platforms Need to Evolve</h5>

<p>–&gt; Connected Data Platforms need to handle distributed data across departments, servers and location. This data can be data-at-rest or data-in-motion. It needs to be done securely and cost effectively and take into account bandwidth and connectivity issues.</p>

<p>–&gt; Connected Data Platforms need to go beyond Apache Hadoop and need enhanced data routing, transformation, and system mediation logic that is starting to emerge from projects like Apache NiFi which supports real time ingestion, pattern detection, routing, and analysis.</p>

<p>–&gt; New capabilities and tools like Apache Zeppelin are needed to make the Connected Data Platforms more accessible</p>

<p>–&gt; Connected Data Platforms need to be enterprise ready, enterprise scale, provide predictable performance, support data encryption, security, data governance, HA, DR, operations and debugging.</p>

<h5 id="six-requirements-of-connected-data-platforms-as-per-hortonworks-research">Six Requirements of Connected Data Platforms (as per Hortonworks research)</h5>

<p>1) Secure Data Ingestion: Can you easily and securely ingest data from anywhere across the Internet of Anything and siphon off data quickly, detecting interesting patterns?</p>

<p>2) Actionable Intelligence: Can your platform provide real-time actionable intelligence based on both data-in-motion and data-at-rest?</p>

<p>3) Distributed Connectivity: Can you connect, relate, or put in context all forms of data to provide 360 degrees of context that allows you to predict and personalize data for your modern data applications across all your channels?</p>

<p>4) Mere Mortal Friendly: Does your platform provide data science tooling that make this easier for data scientists and professionals to find interesting patterns and gain actionable intelligence?</p>

<p>5) Future Proof: Is your platform built on 100% open source technology allowing you to capitalize innovation everywhere economy?</p>

<p>6) Enterprise Ready: Do you have enterprise-ready capabilities for security, high availability, disaster recovery and so on? Is the data secure? Is it access controlled? Are all compliance regulations taken care of? Is activity tracked with an audit trail? Is data controlled and managed through a lifecycle? Is your platform supported by experts?</p>

<h5 id="references">References</h5>
<p><a href="https://nifi.apache.org">Apache Nifi</a>
<a href="https://zeppelin.incubator.apache.org/">Apache Zeppelin</a>
<a href="http://hortonworks.com/solutions/connected-data-platforms/?imm_mid=0e3d2e&amp;cmp=em-data-na-na-newsltr_20160518">Hortonworks Connected Data Strategy</a>
<a href="http://info.hortonworks.com/rs/549-QAL-086/images/hortonworks-connected-data.pdf">Hortonworks Connected Data Whitepaper</a></p>

</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/Apache_Kudu_Overview/" title="Apache Kudu">Apache Kudu</a></h3>
      <p>Enabling fast analytics on fast data</p>
      <h6> Posted on : 2016-05-01 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><h4 id="apache-kudu">Apache Kudu</h4>
<p><img src="/downloads/kudu_logo.png" alt="KUDU" />
A new addition to the open source Apache Hadoop ecosystem, Apache Kudu (incubating) completes Hadoop’s storage layer which is specifically designed for use cases that require fast analytics on fast (rapidly changing) data.</p>

<p>Kudu is Open Source software, licensed under the Apache 2.0 license. Currently, a limited-functionality version of Kudu is available as a Beta.</p>

<blockquote>
  <p>Why Kudu ?</p>
</blockquote>

<p>–&gt; Kudu provides a combination of fast inserts/updates and efficient columnar scans to enable multiple real-time analytic workloads across a single storage layer.</p>

<p>–&gt; As a new complement to HDFS and Apache HBase, Kudu gives architects the flexibility to address a wider variety of use cases without exotic workarounds.</p>

<p>–&gt; Kudu lowers query latency significantly for Apache Impala (incubating) and Apache Spark (initially, with other execution engines to come).</p>

<blockquote>
  <p>Overview</p>
</blockquote>

<p>1) A Simple Data Model</p>

<p>A Kudu cluster stores tables that look just like tables you’re used to from relational (SQL) databases. A table can be as simple as an binary key and value, or as complex as a few hundred different strongly-typed attributes.Just like SQL, every table has a PRIMARY KEY made up of one or more columns.Rows can be efficiently read, updated, or deleted by their primary key.</p>

<p>Kudu’s simple data model makes it breeze to port legacy applications or build new ones: no need to worry about how to encode your data into binary blobs or make sense of a huge database full of hard-to-interpret JSON. Tables are self-describing, so you can use standard tools like SQL engines or Spark to analyze your data.The data model is fully typed, so you don’t need to worry about binary encodings or exotic serialization. You can just store primitive types, like when you use JDBC or ODBC.</p>

<p>2) Low-latency random access</p>

<p>Kudu isn’t just a file format (unlike other storage), it’s a live storage system which supports low-latency millisecond-scale access to individual rows.Kudu’s APIs (Java or C++ APIs) can be used in conjunction with batch access for machine learning or analytics.Kudu isn’t designed to be an OLTP system, but if you have some subset of data which fits in memory, it offers competitive random access performance.</p>

<p>3) Integration with the Hadoop Ecosystem</p>

<p>Kudu is a good citizen on a Hadoop ecosystem : integrating it with other data processing frameworks is simple.You can stream data in from live real-time data sources using the Java client, and then process it immediately upon arrival using Spark, Impala, or MapReduce.</p>

<p>You can even transparently join Kudu tables with data stored in other Hadoop storage such as HDFS or HBase.Kudu includes advanced in-process tracing capabilities, extensive metrics support, and even watchdog threads that helps in tracing &amp; debugging production issues must faster.</p>

<blockquote>
  <p>Architecture</p>
</blockquote>

<p><strong>Super-fast Columnar Storage</strong> - Kudu internally organizes its data by column rather than row. Columnar storage allows efficient encoding and compression. For example, a string field with only a few unique values can use only a few bits per row of storage. With techniques such as run-length encoding, differential encoding, and vectorized bit-packing, Kudu is as fast at reading the data as it is space-efficient at storing it.</p>

<p>Columnar storage also dramatically reduces the amount of data IO required to service analytic queries. Using techniques such as lazy data materialization and predicate pushdown, Kudu can perform drill-down and needle-in-a-haystack queries over billions of rows and terabytes of data in seconds.</p>

<p><strong>Distribution and Fault Tolerance</strong> - In order to scale out to large datasets and large clusters, Kudu splits tables into smaller units called tablets. This splitting can be configured on a per-table basis to be based on hashing, range partitioning, or a combination thereof. This allows the operator to easily trade off between parallelism for analytic workloads and high concurrency for more online ones.</p>

<p>In order to keep your data safe and available at all times, Kudu uses the Raft consensus algorithm to replicate all operations for a given tablet. Raft(raft.github.io) ensures that every write is persisted by at least two nodes before responding to the client request, ensuring that no data is ever lost due to a machine failure.</p>

<p>When machines do fail, replicas reconfigure themselves within a few seconds to maintain extremely high system availability.The use of majority consensus provides very low tail latencies even when some nodes may be stressed by concurrent workloads such as MapReduce jobs or heavy Impala queries.</p>

<p><strong>Designed For Next Generation Hardware</strong> - Kudu is implemented in C++, so it can scale easily to large amounts of memory per node. And because key storage data structures are designed to be highly concurrent, it can scale easily to tens of cores. Kudu’s storage is designed to take advantage of the IO characteristics of solid state drives, and it includes an experimental cache implementation with an in-memory columnar execution path.</p>

<h5 id="references">References</h5>
<p><a href="http://getkudu.io/">Apache Kudu</a>
<a href="https://blog.cloudera.com/blog/2015/09/kudu-new-apache-hadoop-storage-for-fast-analytics-on-fast-data/">Cloudera</a></p>
</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/Enterprise-Usecases-Graph-Databases/" title="Enterprise Usecases - Graph Databases">Enterprise Usecases - Graph Databases</a></h3>
      <p>Overview on few Enterprise Usecases leveraging Graphs</p>
      <h6> Posted on : 2015-12-19 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><h4 id="enterprise-usecases">Enterprise Usecases</h4>

<blockquote>
  <p>Network and Data Centre Management</p>
</blockquote>

<p>Communications networks are graph structures; graph databases are, therefore, a great fit for modelling, storing, and querying this kind of domain data.A graph representation of a network enables:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--&gt; To catalogue assets
--&gt; Visualize how they are deployed
--&gt; Identify the dependencies between Assets. 
</code></pre></div></div>

<p>The graph’s connected structure, together with a query language like Cypher, enable us to conduct sophisticated impact analyses.</p>

<p><strong>Helps answering following questions:</strong></p>

<p>–&gt; Which parts of the network — which applications, services, virtual machines, physical machines, data centers, routers, switches, and fibre — do important customers depend on? (Top-down analysis)</p>

<p>–&gt; Conversely, which applications and services, and ultimately, customers, in the network will be affected if a particular network element — a router or switch, for example — fails? (Bottom-up analysis)</p>

<p>–&gt; Is there redundancy throughout the network for the most important customers?</p>

<p>Graph databases can be used to bring together data from disparate inventory systems, providing a single view of the network and its consumers, from the smallest network element all the way to application and services and the customers who use them. It can also be used to enrich operational intelligence based on event correlations.</p>

<p>Graph databases are being successfully employed in the areas of telecommunications, network management and analysis, cloud platform management, data center and IT asset management, and network impact analysis</p>

<hr />

<blockquote>
  <p>Authorization and Access Control (Communications)</p>
</blockquote>

<p>Graph database access control and authorization solutions are particularly applicable in the areas of content manage-ment, federated authorization services, social networking preferences, and software as a service (SaaS) offerings.</p>

<p>Authorization and access control solutions (<em>traditionally using directory services or custom solutions</em>) :</p>

<p>–&gt; Store information about parties (e.g., administrators, organizational units, end-users) and resources (e.g., files, shares, network devices, products, services, agreements)</p>

<p>–&gt; Holds/defines the rules governing access to those resources</p>

<p>–&gt; Apply these rules to determine who can access or manipulate a resource</p>

<p><strong>With Graph databases</strong> :</p>

<p>–&gt; Can store complex, densely connected access control structures spanning billions of parties and resources</p>

<p>–&gt; Its structured yet schema-free data model supports both hierarchical and non-hierarchical structures</p>

<p>–&gt; Extensible property model allows for capturing rich metadata regarding every element in the system</p>

<p>–&gt; With a query engine that can traverse millions of relationships per second, access lookups over large, complex structures execute in milliseconds</p>

<hr />

<blockquote>
  <p>Master Data Management</p>
</blockquote>

<p>Master data is data that is critical to the operation of a business, but which itself is non-transactional.Master data includes data concerning users, customers, products, sup‐ pliers, departments, geographies, sites, cost centres, and business units.Data is often held in many different places, with lots of overlap and redundancy, in many different formats, and with varying degrees of quality and means of access.Master Data Management (MDM) is the practice of identifying, cleaning, storing, and, most importantly, governing this data.</p>

<p>Graph databases don’t provide a full MDM solution; they are, however, ideally applied to the modelling, storing, and querying of hierarchies, master data metadata, and master data models, and also allowing for the rapid evolution of the master data model in line with changing business needs.</p>

<blockquote>
  <p>Social and Recommendations</p>
</blockquote>

<p>Social applications allow organizations to gain competitive and operational advantage by leveraging information about the connections between people,bring in discrete information about individuals,facilitate collaboration and flow of information &amp; predict behaviour.</p>

<p>–&gt; Effective recommendations are a prime example of generating end-user value through the application of an inferential or suggestive capability.</p>

<p>–&gt; Recommendation algorithms are inductive and suggestive, identifying people, products, or services an individual or group is likely to have some interest in.</p>

<p>–&gt; Recommendation algorithms establish relationships between people and things: other people, products, services, media content (as relevant to the domain)</p>

<p>–&gt; Making an effective recommendation depends on understanding the connections between things, as well as the quality and strength of those connections — all of which are best expressed as a property graph.</p>

<p>–&gt; Social networks and recommendation engines provide key differentiating capabilities in the areas of retail, recruitment, sentiment analysis, search, and knowledge management.</p>

<p>Graphs are a good fit for such the densely connected data structures — Storing and querying this data using a graph database allows an application to surface end-user real-time results that reflect recent changes to the data, rather than pre-calculated, stale results.</p>

<hr />

<h5 id="references">References</h5>

<p><a href="http://neo4j.com/">Neo4j</a>
<a href="http://info.neotechnology.com/rs/neotechnology">Neotechnology</a>
<a href="http://graphaware.com/">Graphaware</a>
<a href="http://neo4j.com/developer/">Neo4j Developer</a>
<a href="http://www.infoq.com/research">Infoq</a>
<a href="http://www.slideshare.net">Slideshare</a>
<a href="http://www.forbes.com/">Forbes</a>
<a href="http://www.gartner.com/doc/2610218">Gartner</a></p>

</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/Neo4j-Graph-Database/" title="Neo4j - Graph Database">Neo4j - Graph Database</a></h3>
      <p>Overview on Neo4j & its capabilities & usecases</p>
      <h6> Posted on : 2015-12-16 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><h4 id="what-is-neo4j">What is Neo4j?</h4>
<p>Sponsored by Neo Technology, Neo4j is an open-source NoSQL graph database implemented in Java and Scala.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--&gt; Neo4j is a Graph Database (with Lucene indexing)
--&gt; Non-relational (“#nosql”), transactional (ACID), embedded
--&gt; Data is stored as a Graph/Network 
    --&gt; Nodes and relationships with properties 
    --&gt; “Property Graph” or “edge-labeled multidigraph”
--&gt; Schema free, bottom-up data model design
--&gt; High Availability (with Enterprise Edition)
--&gt; 32 Billion Nodes, 32 Billion Relationships,64 Billion Properties
--&gt; Neo4j is Open Source/Free (as in speech) Software AGPLv3 
    --&gt; Commercial (“dual license”) license available
</code></pre></div></div>

<p>Some particular features make Neo4j very popular among users, developers, and DBAs:</p>

<ul>
  <li>Materializing of relationships at creation time, resulting in no penalties for complex runtime queries</li>
  <li>Constant time traversals for relationships in the graph both in depth and in breadth due to efficient representation of nodes and relationships</li>
  <li>All relationships in Neo4j are equally important and fast, making it possible to materialize and use new relationships later on to “shortcut” and speed up the domain data when new needs arise</li>
  <li>Compact storage and memory caching for graphs, resulting in efficient scale-up and billions of nodes in one database on moderate hardware</li>
  <li>Written on top of the JVM</li>
</ul>

<p><a href="http://neo4j.com/developer/graph-database/#_what_is_neo4j">About Neo4j</a></p>

<hr />

<h4 id="why-neo4j">Why Neo4j?</h4>

<p>Considered to be the “World’s Best and First Graph Database”, Neo4j is used by thousands of organizations, including 50+ of the Global 2000, in mission-critical production applications.Neo4j is the only graph database on Gartner’s Operational Database Magic Quadrant providing enterprise-strength graph database that combines Native Graph Storage and Processing, Scalable architecture optimized for speed, and ACID compliance to ensure predictability of relationship-based queries.
Rock-Solid Reliability for Mission-Critical Production Applications — Neo4j is the only graph database recognized by key analysts (Forrester, Gartner and others) to have enough production applications to warrant inclusion in reports.Easy to use with Cypher, the world’s most powerful and productive graph query language adds more value and adaptability for Neo4j.</p>

<h3 id="key-features">Key Features</h3>

<ul>
  <li>Cypher Query — Powerful and Expressive Query language
    <ul>
      <li>SQL Like easy query language (Neo4j CQL)</li>
      <li>Cypher often requires 10x to 100x less code than SQL</li>
    </ul>
  </li>
  <li>
    <p>Neo4j implements the Property Graph Model efficiently down to the storage level</p>
  </li>
  <li>
    <p>Neo4j provides full database characteristics including ACID transaction compliance, cluster support, and runtime failover, making it suitable to use graph data in production scenarios</p>
  </li>
  <li>
    <p>Materializing of relationships at creation time, resulting in no penalties for complex runtime queries</p>
  </li>
  <li>
    <p>Constant time traversals for relationships in the graph both in depth and in breadth due to efficient representation of nodes and relationships</p>
  </li>
  <li>
    <p>All relationships in Neo4j are equally important and fast, making it possible to materialize and use new relationships later on to “shortcut” and speed up the domain data when new needs arise</p>
  </li>
  <li>Compact storage and memory caching for graphs, resulting in efficient scale-up and billions of nodes in one database on moderate hardware
Supports both Embedded and Server mode</li>
</ul>

<blockquote>
  <p>Neo4j provides sustainable competitive advantage when :</p>
</blockquote>

<ul>
  <li>
    <p>Building new applications by leveraging value in data relationships</p>
  </li>
  <li>
    <p>Reimagining existing applications by harnessing the ever-increasing relatedness of data</p>
  </li>
  <li>
    <p>Accelerating innovation by decreasing the time to create complex applications
Lowering total cost of ownership compared to traditional database management systems</p>
  </li>
</ul>

<h6 id="internal-applications--master-data-management--network-and-it-operations--fraud-detection"><em>Internal Applications : Master Data Management / Network and IT Operations / Fraud Detection</em></h6>

<h6 id="customer-facing-applications--real-time-recommendations--graph-based-search-identity-and-access-management"><em>Customer-facing applications : Real-Time Recommendations / Graph-Based Search /Identity and Access Management</em></h6>

<h4 id="where-does-neo4j-fit-in">Where does Neo4j fit in?</h4>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Recommendations
Business intelligence
Social computing
Geospatial
Systems management
Web of things
Genealogy
Time series data
Product catalogue
Web analytics
Scientific computing (especially bioinformatics)
Indexing slow RDBMS and much more!
</code></pre></div></div>

<p><a href="/blog/2015-12-19-Enterprise-Usecases-Graph-Databases.md">Refer - Enterprise Usecases - GraphDatabases</a></p>

<hr />

<h5 id="references">References</h5>

<p><a href="http://neo4j.com/">Neo4j</a>
<a href="http://info.neotechnology.com/rs/neotechnology">Neotechnology</a>
<a href="http://graphaware.com/">Graphaware</a>
<a href="http://neo4j.com/developer/">Neo4j Developer</a>
<a href="http://www.infoq.com/research">Infoq</a>
<a href="http://www.slideshare.net">Slideshare</a>
<a href="http://www.forbes.com/">Forbes</a>
<a href="http://www.gartner.com/doc/2610218">Gartner</a></p>

</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/Standard-File-Formats-In-Hadoop/" title="Standard File Formats In Hadoop - Some Considerations">Standard File Formats In Hadoop - Some Considerations</a></h3>
      <p>Some considerations for storing standard file formats in Hadoop</p>
      <h6> Posted on : 2015-12-15 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><p>###Standard File Formats</p>

<p>One of the most powerful features of Hadoop is the ability to store all of your data regardless of format. Having access to data in its raw, source form —“full fidelity” data— means it will always be possible to perform new processing and analytics with the data as requirements change. But it’s preferable to use one of the Hadoop-specific container formats for storing data in Hadoop. 
Some considerations for storing standard file formats in Hadoop:</p>

<blockquote>
  <p>Text data</p>
</blockquote>

<p>A very common use of Hadoop is the storage and analysis of logs such as web logs and server logs and also many other forms: CSV files, or unstructured data such as emails etc.</p>

<ol>
  <li>
    <p>A primary consideration when you are storing text data in Hadoop is the organization of the files in the filesystem 
<em>Refer - <a href="HDFS Schema Design">HDFS Schema Design</a></em></p>
  </li>
  <li>
    <p>A compression format for the files needs to be selected, since text files can very quickly consume considerable space on your Hadoop cluster.
Selection of compression format will be influenced by how the data will be used.For archival purposes you may choose the most compact compression available, but if the data will be used in processing jobs such as MapReduce, you’ll likely want to select a splittable format. Splittable formats enable Hadoop to split files into chunks for processing, which is critical to efficient parallel processing.
<em>Refer - <a href="http://sivansasidharan.me/blog/Data-Compression/">Data Compression in Hadoop</a></em></p>
  </li>
  <li>
    <p>There is an overhead of type conversion associated with storing data in text format. 
For example, storing 1234 in a text file and using it as an integer requires a string-to-integer conversion during reading, and vice versa during writing. It also takes up more space to store 1234 as text than as an integer. This overhead adds up when you do many such conversions and store large amounts of data.</p>
  </li>
</ol>

<blockquote>
  <p>Structured text data</p>
</blockquote>

<p>A more specialized form of text files is structured formats such as XML and JSON. These types of formats can present special challenges with Hadoop since splitting XML and JSON files for processing is tricky, and Hadoop does not provide a built-in InputFormat for either. JSON presents even greater challenges than XML, since there are no tokens to mark the beginning or end of a record. In the case of these formats, you have a couple of options:</p>

<ol>
  <li>
    <p>Use a container format such as Avro. Transforming the data into Avro can provide a compact and efficient way to store and process the data.</p>
  </li>
  <li>
    <p>Use a library designed for processing XML or JSON files. 
Examples of this for XML include XMLLoader in the PiggyBank library for Pig.For JSON, the Elephant Bird project provides the LzoJsonInputFormat.</p>
  </li>
</ol>

<blockquote>
  <p>Binary data</p>
</blockquote>

<p>Although text is typically the most common source data format stored in Hadoop, you can also use Hadoop to process binary files such as images. For most cases of storing and processing binary files in Hadoop, using a container format such as SequenceFile is preferred. If the splittable unit of binary data is larger than 64 MB, you may consider putting the data in its own file, without using a container format.</p>

<blockquote>
  <p>Hadoop File Types</p>
</blockquote>

<p>There are several Hadoop-specific file formats that were specifically created to work well with MapReduce. These Hadoop-specific file formats include file-based data structures such as sequence files, serialization formats like Avro, and columnar formats such as RCFile and Parquet. These file formats have differing strengths and weaknesses. 
Few characteristics these file formats have which are important for Hadoop applications include :</p>

<ol>
  <li>
    <p>Splittable compression</p>
  </li>
  <li>
    <p>Agnostic compression</p>
  </li>
</ol>

<p><em>Agnostic compression - The file can be compressed with any compression codec, without readers having to know the codec. This is possible because the codec is stored in the header metadata of the file format.</em></p>

<hr />

<p><em>Reference -</em>
<a href="http://blog.cloudera.com">Cloudera</a> | 
<a href="http://hortonworks.com">Hortonworks</a> | 
<a href="http://shop.oreilly.com/product/0636920033196.do">O’Reilly - Hadoop Application Architectures</a></p>
</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/Data-Modelling-In-Hadoop/" title="Data Modelling In Hadoop - Key Considerations">Data Modelling In Hadoop - Key Considerations</a></h3>
      <p>Key considerations while modelling data in Hadoop</p>
      <h6> Posted on : 2015-12-10 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><p>###Data Modelling In Hadoop</p>

<p>A common term one hears in the context of Hadoop is Schema-on-Read. This simply refers to the fact that raw, unprocessed data can be loaded into Hadoop, with the structure imposed at processing time based on the requirements of the processing application.This is different from Schema-on-Write, which is generally used with traditional data management systems.When the application or structure of data is not as well understood, the agility provided by the Schema-on-Read pattern can provide invaluable insights on data not previously accessible.</p>

<p>Relational databases and data warehouses are often a good fit for well-understood and frequently accessed queries and reports on high-value data. Increasingly, though, Hadoop is taking on many of these workloads, particularly for queries that need to operate on volumes of data that are not economically or technically practical to process with traditional systems.</p>

<p>Although being able to store all of your raw data is a powerful feature, there are still many factors that you should take into consideration before dumping your data into Hadoop. These considerations include:</p>

<blockquote>
  <p>Data storage formats</p>
</blockquote>

<p>There are a number of file formats and compression formats supported on Hadoop.Each has particular strengths that make it better suited to specific applications. Additionally, although Hadoop provides the Hadoop Distributed File System (HDFS) for storing data, there are several commonly used systems implemented on top of HDFS, such as HBase for additional data access functionality and Hive for additional data management functionality. Such systems need to be taken into consideration as well.</p>

<blockquote>
  <p>Multitenancy</p>
</blockquote>

<p>It’s common for clusters to host multiple users, groups, and application types.Supporting multitenant clusters involves a number of important considerations when you are planning how data will be stored and managed.</p>

<blockquote>
  <p>Schema design</p>
</blockquote>

<p>Despite the schema-less nature of Hadoop, there are still important considerations to take into account around the structure of data stored in Hadoop. This includes directory structures for data loaded into HDFS as well as the output of data processing and analysis. This also includes the schemas of objects stored in systems such as HBase and Hive.</p>

<blockquote>
  <p>Metadata management</p>
</blockquote>

<p>As with any data management system, metadata related to the stored data is often as important as the data itself. Understanding and making decisions related to metadata management are critical.</p>

<p><em>Another important factor when making storage decisions with Hadoop is security and its associated considerations. This includes decisions around authentication, fine-grained access control, and encryption—both for data on the wire and data at rest.</em></p>

<p><em>Reference -</em>
<a href="http://blog.cloudera.com">Cloudera</a> | 
<a href="http://hortonworks.com">Hortonworks</a> | 
<a href="http://shop.oreilly.com/product/0636920033196.do">O’Reilly - Hadoop Application Architectures</a></p>

</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/Graph-Databases/" title="Graph Databases - Why?">Graph Databases - Why?</a></h3>
      <p>Why Graphs</p>
      <h6> Posted on : 2015-12-01 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><p>###Graph Databases</p>

<p><em>“Graph analysis is possibly the single most effective competitive differentiator for organizations pursuing data driven operations and decisions after the design of data capture.”</em> Gartner Research</p>

<p>####Why Does it actually matter ?</p>

<blockquote>
  <p>#####Data Size</p>
</blockquote>

<p>Exabyte’s of data stored per year — the IDC Digital Universe study projects enormous growth in the amount of data being created and stored each year
<img src="/downloads/Neo1.png" alt="Data Size" /></p>

<blockquote>
  <p>#####Connectedness</p>
</blockquote>

<p>Over time data has evolved to be more and more interlinked and connected</p>

<p><img src="/downloads/Neo2.png" alt="Connectedness" /> <img src="/downloads/Neo2_1.png" alt="Connectedness" /></p>

<blockquote>
  <p>#####Semi-Structure</p>
</blockquote>

<p>Accelerated by the decentralization of content generation that is the hallmark of the age of participation (“Web 2.0”)</p>

<ul>
  <li>
    <p>Individualization of content
  <em>In the salary lists of the 1970s, all elements had exactly one job</em>
  <em>In the salary lists of the 2000s, we need 5 job columns! Or 8? Or 15?</em>
  <em>If you tried to collect all the data of every movie ever made, model will have Actors, Characters, Locations, Dates, Costs, Ratings, Showings, Ticket Sales, etc.</em></p>
  </li>
  <li>
    <p>Architectural Changes
Moving towards decoupled services with their own backend by the 2000s</p>
  </li>
</ul>

<p><img src="/downloads/Neo3.png" alt="Connectedness" /></p>

<blockquote>
  <p>#####Performance</p>
</blockquote>

<p>We are building applications today that have complexity requirements that a Relational Database cannot handle with sufficient performance
<img src="/downloads/Neo4.png" alt="Connectedness" /></p>

<hr />

<h4 id="challenges-faced-by-todays-enterprise">Challenges Faced by todays Enterprise</h4>

<ol>
  <li>Fundamental challenge — there’s far more data than what can be handled</li>
  <li>The big data tidal wave that is transforming the database management industry, employee skill sets, and business strategy</li>
  <li>Identifying solutions that are key to discovering, capturing, and making sense of complex interdependences and relationships, both for running an IT organization more effectively and for building next-generation functionality for businesses</li>
  <li>How to model and navigate networks of data, with extremely high performance
The rise of sensors and connected devices leading to variety and velocity of data</li>
</ol>

<hr />

<h3 id="why-graphs">Why Graphs</h3>

<p>Graphs are Everywhere - The real world, unlike the forms-based model behind the relational database, is rich and interrelated, uniform and rule-bound in parts, exceptional and irregular in others.Graphs are extremely useful in understanding a wide diversity of datasets in fields such as science, government, and business.Gartner identifies five graphs in the world of business — social, intent, consumption, interest, and mobile.The ability to leverage graphs provides a “sustainable competitive advantage” (Gartner).</p>

<hr />

<h3 id="graph-databases">Graph Databases</h3>
<p>NoSQL databases can be categorized according to their data model into the following four categories:</p>
<ul>
  <li>Key-Value-stores</li>
  <li>BigTable-implementations</li>
  <li>Document-stores</li>
  <li>Graph Databases</li>
</ul>

<p>A graph database management system (a graph database) is an online database management system with Create, Read, Update, and Delete (CRUD) methods that expose a graph data model.Graph databases are generally built for use with transactional (OLTP) systems.They are normally optimized for transactional performance, and are engineered with transactional integrity and operational availability.
Relationships are first-class citizens of the graph data model.No complexity like other database management systems which require us to infer connections between entities using contrived properties such as foreign keys, or out-of-band processing like map-reduce.By assembling the simple abstractions of nodes and relationships into connected structures, graph databases enable us to build arbitrarily sophisticated models that map closely to our problem domain.The resulting models are simpler and at the same time more expressive than those produced using traditional relational databases and the other NOSQL stores</p>

<blockquote>
  <h5 id="key-features">Key Features</h5>
</blockquote>

<ul>
  <li>Schema Less and Efficient storage of Semi Structured Information</li>
  <li>No O/R (Object/Relational) mismatch — very natural to map a graph to an Object Oriented language like Ruby</li>
  <li>Express Queries as Traversals - Fast deep traversal instead of slow SQL queries that span many table joins</li>
  <li>Very natural to express graph related problem with traversals (recommendation engine, find shortest path etc.)</li>
  <li>Seamless integration with various existing programming languages</li>
  <li>ACID Transaction with rollbacks support</li>
  <li>Whiteboard friendly — you use the language of node, properties and relationship to describe your domain (instead of, e.g., UML) and there is no need to have a complicated O/R mapping tool to implement it in your database</li>
</ul>

<blockquote>
  <p>Optimized For Connections vs Aggregations in RDBMS
<img src="/downloads/Neo5.png" alt="Connectedness" /></p>
</blockquote>

<blockquote>
  <p>Optimized For Traversing Connected Data vs Simple Look-ups in Key-Value Stores
<img src="/downloads/Neo6.png" alt="Connectedness" /></p>
</blockquote>

<blockquote>
  <p>Optimized For seeing the “Forest” and the trees and the branches vs “Trees” of Data
<img src="/downloads/Neo7.png" alt="Connectedness" /></p>
</blockquote>

<blockquote>
  <p>Performance</p>
</blockquote>

<p>Sheer performance increase when dealing with connected data versus relational databases and NOSQL stores.In relational databases, where join-intensive query performance deteriorates as the dataset gets bigger, with a graph database performance tends to remain relatively constant, even as the dataset grows.</p>

<blockquote>
  <p>Flexibility</p>
</blockquote>

<p>The graph data model expresses and accommodates business needs in a way that enables IT to move at the speed of business.Graphs are naturally additive - 
we can add new kinds of relationships, new nodes, new labels, and new subgraphs to an existing structure without disturbing existing queries and application functionality.Fewer migrations, thereby reducing maintenance overhead and risk.</p>

<blockquote>
  <p>Agility</p>
</blockquote>

<ul>
  <li>Easy to evolve the data model in step with the rest of our application</li>
  <li>Equip us to perform frictionless development and graceful systems maintenance thereby empowering us to evolve an application in a controlled manner</li>
  <li>Being schema free, graph databases lack the kind of schema-oriented data governance mechanisms as in relation databases but they provide a far more visible and actionable kind of governance.Governance is typically applied in a programmatic fashion, using tests to drive out the data model and queries, as well as assert the business rules that depend upon the graph.Provides a agile and test-driven software development practices, allowing graph database — backed applications to evolve in step with changing business environments.</li>
</ul>

<hr />

<h4 id="why-organizations-choose-graph-databases">Why Organizations Choose Graph Databases</h4>

<ul>
  <li>
    <p>“Minutes to milliseconds” performance:
  Query performance and responsiveness are top of many organizations’ concerns with regard to their data platforms.</p>
  </li>
  <li>Drastically accelerated development cycles:
  The graph data model reduces:
    <ul>
      <li>Impedance mismatch by reducing the development overhead of translating back and forth between an object model and a tabular relational model</li>
      <li>Mismatch between the technical and business domains:
  Subject matter experts, architects, and developers can talk about and picture the core domain using a shared model that is then incorporated into the application itself</li>
    </ul>
  </li>
  <li>
    <p>Extreme business responsiveness
The schema-free nature of a graph database coupled with the ability to simultaneously relate data elements allows a graph database solution to evolve as the business evolves, reducing risk and time-to-market.</p>
  </li>
  <li>Enterprise ready
Data is important: When employed in a business-critical application, a data technology must be robust, scalable, and more often than not, transactional
Matured graph databases available provide all the —ilities:ACID (Atomic, Consistent, Isolated, Durable), Transactionality,High-availability,Horizontal read scalability, Performance and flexibility, Storage of billions of entities — needed by large enterprises</li>
</ul>

<hr />

<h4 id="the-graph-database-space">The Graph Database Space</h4>

<p><img src="/downloads/Neo8.png" alt="Connectedness" /></p>

<p><em>“Over 25 percent of enterprises will use graph databases by 2017.”</em> Forrester Research</p>

<hr />

<h5 id="references">References</h5>

<p><a href="http://neo4j.com/">Neo4j</a>
<a href="http://info.neotechnology.com/rs/neotechnology">Neotechnology</a>
<a href="http://graphaware.com/">Graphaware</a>
<a href="http://neo4j.com/developer/">Neo4j Developer</a>
<a href="http://www.infoq.com/research">Infoq</a>
<a href="http://www.slideshare.net">Slideshare</a>
<a href="http://www.forbes.com/">Forbes</a>
<a href="http://www.gartner.com/doc/2610218">Gartner</a></p>

</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/6Vs-of-BigData/" title="Beyond Volume, Variety and Velocity">Beyond Volume, Variety and Velocity</a></h3>
      <p>An overview the 6V’s of big data</p>
      <h6> Posted on : 2015-11-29 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><p>Apart from the 3Vs of big data which are Volume, Variety and Velocity, there are additional Vs that IT, business and data scientists need to be concerned with, most notably big data Veracity, Validity &amp; Volatility.</p>

<h3 id="volume">Volume</h3>

<p>Big data implies enormous volumes of data. It used to be user created data. Now that data is generated by machines, networks and human interaction on systems like social media the volume of data to be analyzed is massive. Yet, the volume of data is not as much the problem as other V’s like veracity.</p>

<h3 id="variety">Variety</h3>

<p>Variety refers to the many sources and types of data both structured and unstructured. We used to store data from sources like spreadsheets and databases. Now data comes in the form of emails, photos, videos, monitoring devices, PDFs, audio, etc. This variety of unstructured data creates problems for storage, mining and analyzing data.</p>

<h3 id="velocity">Velocity</h3>

<p>Big Data Velocity deals with the pace at which data flows in from sources like business processes, machines, networks and human interaction with things like social media sites, mobile devices, etc. The flow of data is massive and continuous. This real-time data can help researchers and businesses make valuable decisions that provide strategic competitive advantages and ROI if you are able to handle the velocity. Sampling data can help deal with issues like volume and velocity.</p>

<h3 id="veracity">Veracity</h3>

<p>Big Data Veracity refers to the biases, noise and abnormality in data. Is the data that is being stored, and mined meaningful to the problem being analyzed. Veracity in data analysis is the biggest challenge when compares to things like volume and velocity. In scoping out your big data strategy you need to have your team and partners work to help keep your data clean and processes to keep ‘dirty data’ from accumulating in your systems.</p>

<h3 id="validity">Validity</h3>

<p>Like big data veracity is the issue of validity, meaning, is the data correct and accurate for the intended use. Clearly valid data is key to making the right decisions. IBM’s big data strategy and tools claims to offer help with data veracity and validity.</p>

<h3 id="volatility">Volatility</h3>

<p>Big data volatility refers to how long is data valid and how long should it be stored. In this world of real time data you need to determine at what point is data no longer relevant to the current analysis.</p>

<p>Big data clearly deals with issues beyond volume, variety and velocity to other concerns like veracity, validity and volatility.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Follow the Big Data Innovation Summit on twitter #BIGDBN for more info.
</code></pre></div></div>

<hr />
<p><strong>Reference www.insidebigdata.com</strong>*</p>

</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/Optimize-Hive/" title="Optimize Hive queries for Hadoop">Optimize Hive queries for Hadoop</a></h3>
      <p>Most common Hive performance optimization methods</p>
      <h6> Posted on : 2015-11-12 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><p>Hadoop clusters are not optimized for performance. Here we try to cover a few of the most common Hive performance optimization methods that you can apply to your queries.</p>

<hr />

<h4 id="scale-out-worker-nodes">Scale out worker nodes</h4>

<p>Increasing the number of worker nodes in a cluster can leverage more mappers and reducers to be run in parallel. This could be done at the provision time or at the run time.</p>

<hr />

<h4 id="enable-tez">Enable Tez</h4>

<p>Apache Tez is an alternative execution engine to the MapReduce engine:
Apache Tez provides a developer API and framework to write native YARN applications that bridge the spectrum of interactive and batch workloads. It allows those data access applications to work with petabytes of data over thousands nodes. The Apache Tez component library allows developers to create Hadoop applications that integrate natively with Apache Hadoop YARN and perform well within mixed workload clusters.</p>

<p><img src="http://localhost:4000/downloads/H1H2Tez.png" alt="HDP Reference" /></p>

<p>Tez is faster because:</p>

<ul>
  <li>
    <p>Express, model and execute processing logic: Tez models data processing as a dataflow graph, with the graph vertices representing application logic and its edges representing movement of data. A rich data flow definition API allows users to intuitively express complex query logic. The API fits well with query plans produced by higher-level declarative applications like Apache Hive and Apache Pig.
http://hortonworks.com/blog/apache-tez-a-new-chapter-in-hadoop-data-processing/</p>
  </li>
  <li>
    <p>Execute Directed Acyclic Graph (DAG) as a single job in the MapReduce engine, the DAG that is expressed requires each set of mappers to be followed by one set of reducers. This causes multiple MapReduce jobs to be spun off for each Hive query. Tez does not have such constraint and can process complex DAG as one job thus minimizing job startup overhead.</p>
  </li>
  <li>
    <p>Avoids unnecessary writes: Due to multiple jobs being spun for the same Hive query in the MapReduce engine, the output of each job is written to HDFS for intermediate data. Since Tez minimizes number of jobs for each Hive query it is able to avoid unnecessary write.</p>
  </li>
  <li>
    <p>Minimizes start-up delays: Tez is better able to minimize start-up delay by reducing the number of mappers it needs to start and also improving optimization throughout.</p>
  </li>
  <li>
    <p>Reuses containers Whenever possible: Tez is able to reuse containers to ensure that latency due to starting up containers is reduced.Tez follows the traditional Hadoop model of dividing a job into individual tasks, all of which are run as processes via YARN, on the users’ behalf. This model comes with inherent costs for process startup and initialization, handling stragglers and allocating each container via the YARN resource manager.</p>
  </li>
  <li>
    <p>Continuous optimization techniques: Traditionally optimization was done during compilation phase. However more information about the inputs is available that allow for better optimization during runtime. Tez uses continous optimization techniques that allows it to optimize the plan further into the runtime phase.</p>
  </li>
  <li>
    <p>Optimize performance and resource management: YARN manages resources in a Hadoop cluster, based on cluster capacity and load. The Tez execution engine framework efficiently acquires resources from YARN and reuses every component in the pipeline such that no operation is duplicated unnecessarily.
http://hortonworks.com/blog/introducing-tez-sessions/</p>
  </li>
</ul>

<blockquote>
  <p>Hive query Tez enabled</p>
</blockquote>

<p>You can make any Hive query Tez enabled by prefixing the query with the setting below:</p>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">set</span> <span class="nt">hive</span><span class="nc">.execution.engine</span><span class="o">=</span><span class="nt">tez</span><span class="o">;</span></code></pre></figure>

<hr />

<h4 id="hive-partitioning">Hive partitioning</h4>

<p>I/O operation is the major performance bottleneck for running Hive queries.The performance can be improved if the amount of data that needs to be read can be reduced. By default, Hive queries scan entire Hive tables. This is great for queries like table scans, however for queries that only need to scan a small amount of data (e.g. queries with filtering), this creates unnecessary overhead. Hive partitioning allows Hive queries to access only the necessary amount of data in Hive tables.</p>

<p>Hive partitioning is implemented by reorganizing the raw data into new directories with each partition having its own directory - where the partition is defined by the user. 
The following diagram illustrates partitioning a Hive table by the column Year.A new directory is created for each year.</p>

<p><img src="http://localhost:4000/downloads/Hive-partitioning_1.png" alt="Hive partitioning sample :" /></p>

<p>Some partitioning considerations:</p>

<ul>
  <li>
    <p>Do not under-partition - Partitioning on columns with only a few values can cause very few partitions. For example, partitioning on gender will only create two partitions to be created (male and female), thus only reduce the latency by a maximum of half.</p>
  </li>
  <li>
    <p>Do not over-partition - On the other extreme, creating a partition on a column with a unique value (e.g. userid) will cause multiple partitions causing a lot of stress on the cluster namenode as it will have to handle the large amount of directories.</p>
  </li>
  <li>
    <p>Avoid data skew - Choose your partitioning key wisely so that all partitions are even size. An example is partitioning on State may cause the number of records under California to be almost 30x that of Vermont due to the difference in population.</p>
  </li>
</ul>

<p>Once the partitioned table is created, you can either create:</p>
<blockquote>
  <p>Static partitioning</p>
</blockquote>

<p>Static partitioning means that you have already sharded data in the appropriate directories and you can ask Hive partitions manually based on the directory location.</p>

<blockquote>
  <p>Dynamic partitioning.</p>
</blockquote>

<p>Dynamic partitioning means that you want Hive to create partitions automatically for you. Since we have already created the partitioning table from the staging table, all we need to do is insert data to the partitioned table.</p>

<p>To create a partition table, use the Partitioned By clause:</p>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">CREATE</span> <span class="nt">TABLE</span> <span class="nt">lineitem_part</span>
    <span class="o">(</span><span class="nt">L_ORDERKEY</span> <span class="nt">INT</span><span class="o">,</span> <span class="nt">L_PARTKEY</span> <span class="nt">INT</span><span class="o">,</span> <span class="nt">L_SUPPKEY</span> <span class="nt">INT</span><span class="o">,</span><span class="nt">L_LINENUMBER</span> <span class="nt">INT</span><span class="o">,</span>
     <span class="nt">L_QUANTITY</span> <span class="nt">DOUBLE</span><span class="o">,</span> <span class="nt">L_EXTENDEDPRICE</span> <span class="nt">DOUBLE</span><span class="o">,</span> <span class="nt">L_DISCOUNT</span> <span class="nt">DOUBLE</span><span class="o">,</span>
     <span class="nt">L_TAX</span> <span class="nt">DOUBLE</span><span class="o">,</span> <span class="nt">L_RETURNFLAG</span> <span class="nt">STRING</span><span class="o">,</span> <span class="nt">L_LINESTATUS</span> <span class="nt">STRING</span><span class="o">,</span>
     <span class="nt">L_SHIPDATE_PS</span> <span class="nt">STRING</span><span class="o">,</span> <span class="nt">L_COMMITDATE</span> <span class="nt">STRING</span><span class="o">,</span> <span class="nt">L_RECEIPTDATE</span>        <span class="nt">STRING</span><span class="o">,</span> <span class="nt">L_SHIPINSTRUCT</span> <span class="nt">STRING</span><span class="o">,</span> <span class="nt">L_SHIPMODE</span> <span class="nt">STRING</span><span class="o">,</span>
     <span class="nt">L_COMMENT</span> <span class="nt">STRING</span><span class="o">)</span>
<span class="nt">PARTITIONED</span> <span class="nt">BY</span><span class="o">(</span><span class="nt">L_SHIPDATE</span> <span class="nt">STRING</span><span class="o">)</span>
<span class="nt">ROW</span> <span class="nt">FORMAT</span> <span class="nt">DELIMITED</span> <span class="nt">FIELDS</span> <span class="nt">TERMINATED</span> <span class="nt">BY</span> <span class="s2">'\t'</span>
<span class="nt">STORED</span> <span class="nt">AS</span> <span class="nt">TEXTFILE</span><span class="o">;</span></code></pre></figure>

<p>Static partitioning:</p>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">INSERT</span> <span class="nt">OVERWRITE</span> <span class="nt">TABLE</span> <span class="nt">lineitem_part</span>
<span class="nt">PARTITION</span> <span class="o">(</span><span class="nt">L_SHIPDATE</span> <span class="o">=</span> <span class="err">‘</span><span class="nt">5</span><span class="o">/</span><span class="nt">23</span><span class="o">/</span><span class="nt">1996</span> <span class="nt">12</span><span class="nd">:00:00</span> <span class="nt">AM</span><span class="err">’</span><span class="o">)</span>
<span class="nt">SELECT</span> <span class="o">*</span> <span class="nt">FROM</span> <span class="nt">lineitem</span> 
<span class="nt">WHERE</span> <span class="nt">lineitem</span><span class="nc">.L_SHIPDATE</span> <span class="o">=</span> <span class="err">‘</span><span class="nt">5</span><span class="o">/</span><span class="nt">23</span><span class="o">/</span><span class="nt">1996</span> <span class="nt">12</span><span class="nd">:00:00</span> <span class="nt">AM</span><span class="err">’</span>
<span class="nt">ALTER</span> <span class="nt">TABLE</span> <span class="nt">lineitem_part</span> <span class="nt">ADD</span> <span class="nt">PARTITION</span> <span class="o">(</span><span class="nt">L_SHIPDATE</span> <span class="o">=</span> <span class="err">‘</span><span class="nt">5</span><span class="o">/</span><span class="nt">23</span><span class="o">/</span><span class="nt">1996</span> <span class="nt">12</span><span class="nd">:00:00</span> <span class="nt">AM</span><span class="err">’</span><span class="o">))</span>
<span class="nt">LOCATION</span> <span class="err">‘</span><span class="nt">wasb</span><span class="o">://</span><span class="nt">sampledata</span><span class="k">@ignitedemo</span><span class="p">.</span><span class="n">blob</span><span class="p">.</span><span class="n">core</span><span class="p">.</span><span class="n">windows</span><span class="p">.</span><span class="n">net</span><span class="p">/</span><span class="n">partitions</span><span class="p">/</span><span class="m">5</span><span class="n">_23_1996</span><span class="p">/</span><span class="err">'</span></code></pre></figure>

<p>Dynamic partitioning:</p>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">SET</span> <span class="nt">hive</span><span class="nc">.exec.dynamic.partition</span> <span class="o">=</span> <span class="nt">true</span><span class="o">;</span>
<span class="nt">SET</span> <span class="nt">hive</span><span class="nc">.exec.dynamic.partition.mode</span> <span class="o">=</span> <span class="nt">nonstrict</span><span class="o">;</span>
<span class="nt">INSERT</span> <span class="nt">INTO</span> <span class="nt">TABLE</span> <span class="nt">lineitem_part</span>
<span class="nt">PARTITION</span> <span class="o">(</span><span class="nt">L_SHIPDATE</span><span class="o">)</span>
<span class="nt">SELECT</span> <span class="nt">L_ORDERKEY</span> <span class="nt">as</span> <span class="nt">L_ORDERKEY</span><span class="o">,</span> <span class="nt">L_PARTKEY</span> <span class="nt">as</span> <span class="nt">L_PARTKEY</span> <span class="o">,</span> 
     <span class="nt">L_SUPPKEY</span> <span class="nt">as</span> <span class="nt">L_SUPPKEY</span><span class="o">,</span> <span class="nt">L_LINENUMBER</span> <span class="nt">as</span> <span class="nt">L_LINENUMBER</span><span class="o">,</span>
     <span class="nt">L_QUANTITY</span> <span class="nt">as</span> <span class="nt">L_QUANTITY</span><span class="o">,</span> <span class="nt">L_EXTENDEDPRICE</span> <span class="nt">as</span> <span class="nt">L_EXTENDEDPRICE</span><span class="o">,</span>
     <span class="nt">L_DISCOUNT</span> <span class="nt">as</span> <span class="nt">L_DISCOUNT</span><span class="o">,</span> <span class="nt">L_TAX</span> <span class="nt">as</span> <span class="nt">L_TAX</span><span class="o">,</span> <span class="nt">L_RETURNFLAG</span> <span class="nt">as</span>       <span class="nt">L_RETURNFLAG</span><span class="o">,</span> <span class="nt">L_LINESTATUS</span> <span class="nt">as</span> <span class="nt">L_LINESTATUS</span><span class="o">,</span> <span class="nt">L_SHIPDATE</span> <span class="nt">as</span>       <span class="nt">L_SHIPDATE_PS</span><span class="o">,</span> <span class="nt">L_COMMITDATE</span> <span class="nt">as</span> <span class="nt">L_COMMITDATE</span><span class="o">,</span> <span class="nt">L_RECEIPTDATE</span> <span class="nt">as</span>   <span class="nt">L_RECEIPTDATE</span><span class="o">,</span> <span class="nt">L_SHIPINSTRUCT</span> <span class="nt">as</span> <span class="nt">L_SHIPINSTRUCT</span><span class="o">,</span> <span class="nt">L_SHIPMODE</span> <span class="nt">as</span>     <span class="nt">L_SHIPMODE</span><span class="o">,</span> <span class="nt">L_COMMENT</span> <span class="nt">as</span> <span class="nt">L_COMMENT</span><span class="o">,</span> <span class="nt">L_SHIPDATE</span> <span class="nt">as</span> <span class="nt">L_SHIPDATE</span> <span class="nt">FROM</span> <span class="nt">lineitem</span><span class="o">;</span></code></pre></figure>

<p>https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-PartitionedTables</p>

<hr />

<p>####Use the ORCFile format</p>

<p>Hive supports different file formats. For example:</p>

<ul>
  <li>Text: this is the default file format and works with most scenarios</li>
  <li>Avro: works well for interoperability scenarios</li>
  <li>ORC/Parquet: best suited for performance
ORC (Optimized Row Columnar) format is a highly efficient way to store Hive data.Compared to other formats, ORC has the following advantages:
    <ul>
      <li>support for complex types including DateTime and complex and semi-structured types</li>
      <li>up to 70% compression</li>
      <li>indexes every 10,000 rows which allow skipping rows</li>
      <li>a significant drop in run-time execution</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>To enable ORC format</p>
</blockquote>

<p>First create a table with the clause Stored as ORC:</p>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">CREATE</span> <span class="nt">TABLE</span> <span class="nt">lineitem_orc_part</span>
    <span class="o">(</span><span class="nt">L_ORDERKEY</span> <span class="nt">INT</span><span class="o">,</span> <span class="nt">L_PARTKEY</span> <span class="nt">INT</span><span class="o">,</span><span class="nt">L_SUPPKEY</span> <span class="nt">INT</span><span class="o">,</span> <span class="nt">L_LINENUMBER</span> <span class="nt">INT</span><span class="o">,</span>
     <span class="nt">L_QUANTITY</span> <span class="nt">DOUBLE</span><span class="o">,</span> <span class="nt">L_EXTENDEDPRICE</span> <span class="nt">DOUBLE</span><span class="o">,</span> <span class="nt">L_DISCOUNT</span> <span class="nt">DOUBLE</span><span class="o">,</span>
     <span class="nt">L_TAX</span> <span class="nt">DOUBLE</span><span class="o">,</span> <span class="nt">L_RETURNFLAG</span> <span class="nt">STRING</span><span class="o">,</span> <span class="nt">L_LINESTATUS</span> <span class="nt">STRING</span><span class="o">,</span>
     <span class="nt">L_SHIPDATE_PS</span> <span class="nt">STRING</span><span class="o">,</span> <span class="nt">L_COMMITDATE</span> <span class="nt">STRING</span><span class="o">,</span> <span class="nt">L_RECEIPTDATE</span> <span class="nt">STRING</span><span class="o">,</span>
     <span class="nt">L_SHIPINSTRUCT</span> <span class="nt">STRING</span><span class="o">,</span> <span class="nt">L_SHIPMODE</span> <span class="nt">STRING</span><span class="o">,</span> <span class="nt">L_COMMENT</span>     <span class="nt">STRING</span><span class="o">)</span>
<span class="nt">PARTITIONED</span> <span class="nt">BY</span><span class="o">(</span><span class="nt">L_SHIPDATE</span> <span class="nt">STRING</span><span class="o">)</span>
<span class="nt">STORED</span> <span class="nt">AS</span> <span class="nt">ORC</span><span class="o">;</span></code></pre></figure>

<p>Next, you insert data to the ORC table from the staging table. For example:</p>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">INSERT</span> <span class="nt">INTO</span> <span class="nt">TABLE</span> <span class="nt">lineitem_orc</span>
<span class="nt">SELECT</span> <span class="nt">L_ORDERKEY</span> <span class="nt">as</span> <span class="nt">L_ORDERKEY</span><span class="o">,</span> 
       <span class="nt">L_PARTKEY</span> <span class="nt">as</span> <span class="nt">L_PARTKEY</span> <span class="o">,</span> 
       <span class="nt">L_SUPPKEY</span> <span class="nt">as</span> <span class="nt">L_SUPPKEY</span><span class="o">,</span>
       <span class="nt">L_LINENUMBER</span> <span class="nt">as</span> <span class="nt">L_LINENUMBER</span><span class="o">,</span>
       <span class="nt">L_QUANTITY</span> <span class="nt">as</span> <span class="nt">L_QUANTITY</span><span class="o">,</span> 
       <span class="nt">L_EXTENDEDPRICE</span> <span class="nt">as</span> <span class="nt">L_EXTENDEDPRICE</span><span class="o">,</span>
       <span class="nt">L_DISCOUNT</span> <span class="nt">as</span> <span class="nt">L_DISCOUNT</span><span class="o">,</span>
       <span class="nt">L_TAX</span> <span class="nt">as</span> <span class="nt">L_TAX</span><span class="o">,</span>
       <span class="nt">L_RETURNFLAG</span> <span class="nt">as</span> <span class="nt">L_RETURNFLAG</span><span class="o">,</span>
       <span class="nt">L_LINESTATUS</span> <span class="nt">as</span> <span class="nt">L_LINESTATUS</span><span class="o">,</span>
       <span class="nt">L_SHIPDATE</span> <span class="nt">as</span> <span class="nt">L_SHIPDATE</span><span class="o">,</span>
       <span class="nt">L_COMMITDATE</span> <span class="nt">as</span> <span class="nt">L_COMMITDATE</span><span class="o">,</span>
       <span class="nt">L_RECEIPTDATE</span> <span class="nt">as</span> <span class="nt">L_RECEIPTDATE</span><span class="o">,</span> 
       <span class="nt">L_SHIPINSTRUCT</span> <span class="nt">as</span> <span class="nt">L_SHIPINSTRUCT</span><span class="o">,</span>
       <span class="nt">L_SHIPMODE</span> <span class="nt">as</span> <span class="nt">L_SHIPMODE</span><span class="o">,</span>
       <span class="nt">L_COMMENT</span> <span class="nt">as</span> <span class="nt">L_COMMENT</span>
<span class="nt">FROM</span> <span class="nt">lineitem</span><span class="o">;</span></code></pre></figure>

<p>More on ORC - https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC</p>

<hr />

<p>####Vectorization</p>

<p>Vectorized query execution is a Hive feature that greatly reduces the CPU usage for typical query operations like scans, filters, aggregates, and joins.
Vectorization allows Hive to process a batch of 1024 rows together instead of processing one row at a time. This means that simple operations are done faster because less internal code needs to run.</p>

<p>To enable vectorization prefix your Hive query with the following setting:</p>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">set</span> <span class="nt">hive</span><span class="nc">.vectorized.execution.enabled</span> <span class="o">=</span> <span class="nt">true</span><span class="o">;</span></code></pre></figure>

<p>More info - https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution</p>

<hr />

<h4 id="other-optimization-methods">Other optimization methods</h4>
<p>There are more optimization methods, for example:</p>

<ul>
  <li>
    <p>Hive bucketing: a technique that allows to cluster or segment large sets of data to optimize query performance.</p>
  </li>
  <li>
    <p>Join optimization: optimization of Hive’s query execution planning to improve the efficiency of joins and reduce the need for user hints. 
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization#LanguageManualJoinOptimization-JoinOptimization.</p>
  </li>
  <li>
    <p>Increase Reducers</p>
  </li>
</ul>

<hr />

<p>References - 
<strong>http://hortonworks.com/hadoop/tez/</strong>
<strong>https://azure.microsoft.com/en-in/documentation/services/hdinsight/</strong></p>
</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/HDP-Cluster-Validation-Hive/" title="Cluster Validation - Hive - HDP">Cluster Validation - Hive - HDP</a></h3>
      <p>Cluster validation methodologies & Hive checklist</p>
      <h6> Posted on : 2015-10-28 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><p>For every PS engagement that involves installation, upgrade, or migration, it is your responsibility to ensure that the installation validation checklist is completely executed.This will ensure:</p>
<ul>
  <li>Proper configuration of the cluster</li>
  <li>Increased customer satisfaction</li>
  <li>Decreased volume to the Technical Support team</li>
</ul>

<blockquote>
  <p>The Importance of the Checklist</p>
</blockquote>

<p>The installation validation checklist is a tool that you should refer to throughout the duration of your engagement.It was assembled by the Technical Support team and contains information and best practices gathered while working on multiple projects with multiple clients.The checklists provide information on key areas that you should focus on to ensure proper configuration of the components of your cluster. This will result in an operational Customer Environment.</p>

<hr />

<h4 id="locating-the-validation-scripts">Locating the Validation Scripts</h4>

<p>You can download the scripts from GitHub using the links below:
Scripts Used to Test and Validate Hive:</p>
<ul>
  <li>Download the hdp-hive-validation.tgz file using the link below. It contains all the scripts and required libraries and needs to be copied to an edge node on the cluster that is being validated.
    <blockquote>
      <p>https://github.com/dstreev/hdp-validation/tree/master/dist</p>
    </blockquote>
  </li>
</ul>

<p>Hortonworks Data Platform Data Generation Tool:</p>
<ul>
  <li>Download the Hortonworks Data Platform Data Generation Tool. It is used to generate sample datasets used by the validation scripts. It is included in the above referenced tarball, but the sources can be downloaded using the link below.</li>
</ul>

<blockquote>
  <p>https://github.com/dstreev/hdp-data-gen</p>
</blockquote>

<hr />

<h4 id="1-running-the-hdp-configuration-utility">1. Running the HDP Configuration Utility</h4>

<p>Before you start any testing and before the cluster is turned over to the customer, run the HDP Configuration Utility with the clusters specification and apply the settings. The utility is a python script so it can be run on your local laptop to get the appropriate settings.These settings can then be applied to the cluster through Ambari.</p>

<p>You will need to provide these settings to the customer as a baseline for future support. 
The utility can be downloaded from GitHub using the link below:</p>
<blockquote>
  <p>HDP Configuration Utility:
    https://github.com/hortonworks/hdp-configuration-utils</p>
</blockquote>

<hr />

<h4 id="2-hive-metastore-and-hs2-validation">2. Hive Metastore and HS2 Validation</h4>

<p>The hive heap size by default is set to 1024, but for clusters that are more than a simple POC, this is not usually enough. If the system is going to have concurrent users (against HS2), the Hive Metastore and HiveServer2 daemons should be setup with at least 4 GB of memory. This will give the HS2 server the room it needs to handle this load.</p>

<p>If the user will be extracting LARGE datasets through the HS2 interface, this value may need to be higher, as HS2 is the conduit for channeling all of that data back to the client. With concurrent user requests, HS2 can quickly become the bottleneck.</p>

<p><strong>Note</strong>
Changing this value WILL effect ALL “hive cli” clients, which will start with the same memory settings. It is suggested that you create a Managed Config Group in Ambari to set these values for the HS2 server, independently.</p>

<blockquote>
  <p>To validate this, use the Ambari REST API via the Ambari “config.sh” script:
    cd /var/lib/ambari-server/resources/scripts
    ./configs.sh -u <ambari-user> -p <ambari-password> get <AMBARI_HOST> <CLUSTER_NAME> hive-site | grep hive.heapsize</CLUSTER_NAME></AMBARI_HOST></ambari-password></ambari-user></p>
</blockquote>

<p>The value returned after running the script should be greater than 4096.</p>

<hr />

<h4 id="3-verifying-hiveserver2-is-running-in-embedded-metastore-mode">3. Verifying HiveServer2 is Running in Embedded Metastore Mode</h4>
<p>There are several best practices that you can follow to verify that HiveServer2 is running in embedded metastore mode:</p>
<ul>
  <li>It is important to use netstat to check if the process is making a call to 9083.</li>
  <li>HiveServer2 startup via Ambari uses a template file to override the values you will see for <code class="highlighter-rouge">hive.metastore.uris</code> set in the Ambari UI for <code class="highlighter-rouge">hive-site.xml</code>. This file is located in the Ambari Resource Stack if you would like to examine the contents.</li>
</ul>

<p>** Patch for HS2 Embedded Metastore **</p>
<blockquote>
  <p>For Ambari versions previous to 1.7.0, you will not be able to get HS2 to run with an Embedded Metastore</p>
</blockquote>

<p>Apply the following patch :</p>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">diff</span> <span class="nt">--git</span> <span class="nt">a</span><span class="o">/</span><span class="nt">ambari-server</span><span class="o">/</span><span class="nt">src</span><span class="o">/</span><span class="nt">main</span><span class="o">/</span><span class="nt">resources</span><span class="o">/</span><span class="nt">stacks</span><span class="o">/</span><span class="nt">HDP</span><span class="o">/</span><span class="nt">2</span><span class="nc">.0.6</span><span class="o">/</span><span class="nt">services</span><span class="o">/</span><span class="nt">HIVE</span><span class="o">/</span><span class="nt">package</span><span class="o">/</span><span class="nt">templates</span><span class="o">/</span><span class="nt">startHiveserver2</span><span class="nc">.sh.j2</span> <span class="nt">b</span><span class="o">/</span><span class="nt">ambari-server</span><span class="o">/</span><span class="nt">src</span><span class="o">/</span><span class="nt">main</span><span class="o">/</span><span class="nt">resources</span><span class="o">/</span><span class="nt">stacks</span><span class="o">/</span><span class="nt">HDP</span><span class="o">/</span><span class="nt">2</span><span class="nc">.0.6</span><span class="o">/</span><span class="nt">services</span><span class="o">/</span><span class="nt">HIVE</span><span class="o">/</span><span class="nt">package</span><span class="o">/</span><span class="nt">templates</span><span class="o">/</span><span class="nt">startHiveserver2</span><span class="nc">.sh.j2</span>
<span class="nt">index</span> <span class="nt">62ab19e</span><span class="o">.</span><span class="nc">.a8fe21c</span> <span class="nt">100644</span>
<span class="nt">---</span> <span class="nt">a</span><span class="o">/</span><span class="nt">ambari-server</span><span class="o">/</span><span class="nt">src</span><span class="o">/</span><span class="nt">main</span><span class="o">/</span><span class="nt">resources</span><span class="o">/</span><span class="nt">stacks</span><span class="o">/</span><span class="nt">HDP</span><span class="o">/</span><span class="nt">2</span><span class="nc">.0.6</span><span class="o">/</span><span class="nt">services</span><span class="o">/</span><span class="nt">HIVE</span><span class="o">/</span><span class="nt">package</span><span class="o">/</span><span class="nt">templates</span><span class="o">/</span><span class="nt">startHiveserver2</span><span class="nc">.sh.j2</span>
<span class="o">+++</span> <span class="nt">b</span><span class="o">/</span><span class="nt">ambari-server</span><span class="o">/</span><span class="nt">src</span><span class="o">/</span><span class="nt">main</span><span class="o">/</span><span class="nt">resources</span><span class="o">/</span><span class="nt">stacks</span><span class="o">/</span><span class="nt">HDP</span><span class="o">/</span><span class="nt">2</span><span class="nc">.0.6</span><span class="o">/</span><span class="nt">services</span><span class="o">/</span><span class="nt">HIVE</span><span class="o">/</span><span class="nt">package</span><span class="o">/</span><span class="nt">templates</span><span class="o">/</span><span class="nt">startHiveserver2</span><span class="nc">.sh.j2</span>
<span class="o">@@</span> <span class="nt">-19</span><span class="o">,</span><span class="nt">11</span> <span class="o">+</span><span class="nt">19</span><span class="o">,</span><span class="nt">11</span> <span class="o">@@</span>
 <span class="err">#</span>
 <span class="err">#</span>
  
<span class="nt">-HIVE_SERVER2_OPTS</span><span class="o">=</span><span class="s1">" -hiveconf hive.metastore.uris=\" \" -hiveconf hive.log.file=hiveserver2.log -hiveconf hive.log.dir=$5"</span>
<span class="o">+</span><span class="nt">HIVE_SERVER2_OPTS</span><span class="o">=</span><span class="s1">" -hiveconf hive.log.file=hiveserver2.log -hiveconf hive.log.dir=$5"</span>
 
  
<span class="nt">-HIVE_CONF_DIR</span><span class="o">=</span><span class="err">$</span><span class="nt">4</span> <span class="o">/</span><span class="nt">usr</span><span class="o">/</span><span class="nt">lib</span><span class="o">/</span><span class="nt">hive</span><span class="o">/</span><span class="nt">bin</span><span class="o">/</span><span class="nt">hiveserver2</span> <span class="err">$</span><span class="p">{</span><span class="err">HIVE_SERVER2_OPTS</span><span class="p">}</span> <span class="o">&gt;</span> <span class="err">$</span><span class="nt">1</span> <span class="nt">2</span><span class="o">&gt;</span> <span class="err">$</span><span class="nt">2</span> <span class="o">&amp;</span>
<span class="o">+</span><span class="nt">HIVE_CONF_DIR</span><span class="o">=</span><span class="err">$</span><span class="nt">4</span> <span class="o">/</span><span class="nt">usr</span><span class="o">/</span><span class="nt">lib</span><span class="o">/</span><span class="nt">hive</span><span class="o">/</span><span class="nt">bin</span><span class="o">/</span><span class="nt">hiveserver2</span> <span class="nt">-hiveconf</span> <span class="nt">hive</span><span class="nc">.metastore.uris</span><span class="o">=</span><span class="s1">" "</span> <span class="err">$</span><span class="p">{</span><span class="err">HIVE_SERVER2_OPTS</span><span class="p">}</span> <span class="o">&gt;</span> <span class="err">$</span><span class="nt">1</span> <span class="nt">2</span><span class="o">&gt;</span> <span class="err">$</span><span class="nt">2</span> <span class="o">&amp;</span>
 <span class="nt">echo</span> <span class="err">$</span><span class="o">!|</span><span class="nt">cat</span><span class="o">&gt;</span><span class="err">$</span><span class="nt">3</span></code></pre></figure>

<blockquote>
  <p>For Ambari 1.7.0 and above, a simple validation test can be performed by, “Shutting Down” the Hive Metastore instance and then trying to connect to HS2 via Beeline. You can then run a few commands to exercise metastore access. Remember to restart the Hive Metastore to support Hive CLI calls.</p>
</blockquote>

<hr />

<p>** Connectivity Check From Hive Server to Metastore Database **
When starting the Hive Metastore, if you receive any error( issues in connecting to DB ), you should check that the Metatstore database is reachable from the replacement host.</p>

<p>You can run the following from the command line on the replacement host to check connectivity to the Metastore database. Correct any errors and then attempt to start the Hive Metastore again:</p>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="o">/</span><span class="nt">usr</span><span class="o">/</span><span class="nt">lib</span><span class="o">/</span><span class="nt">hive</span><span class="o">/</span><span class="nt">bin</span><span class="o">/</span><span class="nt">schematool</span> <span class="nt">-initSchema</span> <span class="nt">-dbType</span> <span class="nt">mysql</span> <span class="nt">-dryRun</span> <span class="nt">-verbose</span> <span class="nt">-userName</span> <span class="nt">hive</span> <span class="nt">-passWord</span> <span class="nt">hivepwd</span></code></pre></figure>

<hr />

<h4 id="4-setting-up-an-additional-queue-for-hive-testing">4. Setting Up an Additional Queue for Hive Testing</h4>

<p>An additional queue should be set up so that it can be used later to ensure that Hive can run jobs against a queue other than the default queue. In Ambari, add an additional entry for an “Alt” queue. Be sure to restart YARN and validate that the queue has been registered via the Resource Manager interface.
Below are the Capacity Schedule Settings that you should use via Ambari:</p>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">yarn</span><span class="nc">.scheduler.capacity.maximum-am-resource-percent</span><span class="o">=</span><span class="nt">0</span><span class="nc">.2</span>
<span class="nt">yarn</span><span class="nc">.scheduler.capacity.maximum-applications</span><span class="o">=</span><span class="nt">10000</span>
<span class="nt">yarn</span><span class="nc">.scheduler.capacity.node-locality-delay</span><span class="o">=</span><span class="nt">40</span>
<span class="nt">yarn</span><span class="nc">.scheduler.capacity.root.acl_administer_queue</span><span class="o">=*</span>
<span class="nt">yarn</span><span class="nc">.scheduler.capacity.root.alt.acl_administer_jobs</span><span class="o">=*</span>
<span class="nt">yarn</span><span class="nc">.scheduler.capacity.root.alt.acl_submit_applications</span><span class="o">=*</span>
<span class="nt">yarn</span><span class="nc">.scheduler.capacity.root.alt.capacity</span><span class="o">=</span><span class="nt">20</span>
<span class="nt">yarn</span><span class="nc">.scheduler.capacity.root.alt.maximum-capacity</span><span class="o">=</span><span class="nt">50</span>
<span class="nt">yarn</span><span class="nc">.scheduler.capacity.root.alt.state</span><span class="o">=</span><span class="nt">RUNNING</span>
<span class="nt">yarn</span><span class="nc">.scheduler.capacity.root.alt.user-limit-factor</span><span class="o">=</span><span class="nt">1</span>
<span class="nt">yarn</span><span class="nc">.scheduler.capacity.root.capacity</span><span class="o">=</span><span class="nt">100</span>
<span class="nt">yarn</span><span class="nc">.scheduler.capacity.root.default.acl_administer_jobs</span><span class="o">=*</span>
<span class="nt">yarn</span><span class="nc">.scheduler.capacity.root.default.acl_submit_applications</span><span class="o">=*</span>
<span class="nt">yarn</span><span class="nc">.scheduler.capacity.root.default.capacity</span><span class="o">=</span><span class="nt">80</span>
<span class="nt">yarn</span><span class="nc">.scheduler.capacity.root.default.maximum-capacity</span><span class="o">=</span><span class="nt">100</span>
<span class="nt">yarn</span><span class="nc">.scheduler.capacity.root.default.state</span><span class="o">=</span><span class="nt">RUNNING</span>
<span class="nt">yarn</span><span class="nc">.scheduler.capacity.root.default.user-limit-factor</span><span class="o">=</span><span class="nt">1</span>
<span class="nt">yarn</span><span class="nc">.scheduler.capacity.root.queues</span><span class="o">=</span><span class="nt">default</span><span class="o">,</span><span class="nt">alt</span>
<span class="nt">yarn</span><span class="nc">.scheduler.capacity.root.unfunded.capacity</span><span class="o">=</span><span class="nt">50</span> </code></pre></figure>

<hr />

<h4 id="5-hive-ecosystem-validation-preparation">5. Hive Ecosystem Validation Preparation</h4>

<p>Listed below are some of the steps you can take in order to prepare the Hive Ecosystem for validation:</p>

<ul>
  <li>Select an edge node or some cluster machine to prepare the test datasets</li>
  <li>Download the <code class="highlighter-rouge">hdp-hive-validation.tgz</code>  package (https://wiki.hortonworks.com/download/attachments/7310572/hdp-hive-validation.tgz?version=2&amp;modificationDate=1418506590000&amp;api=v2)</li>
  <li>Copy the home directory of the “Hive” user on the edge node</li>
  <li>Unzip the <code class="highlighter-rouge">hdp-hive-validation.tgz</code> package in the Hive users home directory</li>
</ul>

<p>The <code class="highlighter-rouge">hdp-hive-validation.tgz</code> package contains several scripts and tools that can be used to create a large dataset and also build tables to test the various parts of Hive.</p>

<p>After you have unzipped the package, edit the <code class="highlighter-rouge">validation-env.sh</code> file and set the values for the HS2 server. Based on your cluster size, you will need to uncomment/comment out the section used to control the size of the validation dataset. Below is a table that provides details regarding generated files based on cluster size:</p>

<table>
  <thead>
    <tr>
      <th>Cluster Size</th>
      <th>NumberofRecords</th>
      <th>NumberOfMappers</th>
      <th>Approx Total Dataset Size</th>
      <th>Notes About Generated Files</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>5</td>
      <td>100000000</td>
      <td>5</td>
      <td>1G</td>
      <td>5 files, each UNDER the block size</td>
    </tr>
    <tr>
      <td>10</td>
      <td>500000000</td>
      <td>20</td>
      <td>5G</td>
      <td>20 files, each just over the block size</td>
    </tr>
    <tr>
      <td>10</td>
      <td>1000000000</td>
      <td>50</td>
      <td>10G</td>
      <td>50 files, each over the block size</td>
    </tr>
  </tbody>
</table>

<hr />

<h5 id="51-data-generation-and-global-function-testing">5.1 Data Generation and Global Function Testing</h5>
<p>After you have downloaded all of the files, the script below should be run. Be sure to run the script below as the “hdfs” user to create hdfs directories that will be used to store shared libraries:</p>

<p><code class="highlighter-rouge">./0-run-as-hdfs.sh</code></p>

<p>Next, run the script that will copy a jar file with custom UDFs for Hive to the cluster. It will then register those functions to run some tests to prove that the function is globally available permanently.
The second part of the script will generate the dataset that will be used to test Hive at some scale.
The process below should be run as the “hive” user to minimize user setup and permission issues:</p>

<blockquote>
  <p>Change to directory where the above package expanded to.</p>
</blockquote>

<p><code class="highlighter-rouge">./1-validation-setup.sh &gt; / tmp /step-1.txt</code></p>

<hr />

<h4 id="6-hive-checklist-items">6. Hive Checklist Items</h4>
<p>There are multiple items that should be checked to ensure that Hive is configured properly. The table below provides the key areas of focus, along with the rationale behind them:</p>

<table>
  <thead>
    <tr>
      <th>Verify</th>
      <th>Notes/Rationale</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>The Hive Metastore and HiveServer2 daemons are set up with at least 4 GB of memory</td>
      <td>The Hive heap size by default is set to 1024. For clusters beyond a simple POC, this is not usually enough. For additional information, please refer to section of this lesson section titled, “Hive Metatstore and HS2 Validation.”</td>
    </tr>
    <tr>
      <td>The HiveServer2 is running in embedded metastore mode</td>
      <td>Check with netstat if the process is making a call to 9083. For additional information, please refer to the section of this lesson title, “Verifying HiveServer 2 is Running in Embedded Metastore Mode.”</td>
    </tr>
    <tr>
      <td>All Hive clients/edge nodes can connect via remote metastore</td>
      <td>On each Hive client, run:hive –e ‘show databases;The result should yield at least the ‘default’ database on a new cluster.</td>
    </tr>
    <tr>
      <td>You have created a partition table, loaded data, and run a query that kicks off MR/Tez jobs from Hive CLI</td>
      <td>Use Beeline to test HS2 functionality. Review /tmp/step-2.txt to confirm after running the 2-hive-test.sh script.</td>
    </tr>
    <tr>
      <td>You have run a map join query in the Hive Client and Beeline</td>
      <td> </td>
    </tr>
    <tr>
      <td>The ODBC/JDBC connections and query are validated, especially in kerberized cluster</td>
      <td> </td>
    </tr>
    <tr>
      <td>You can perform “DDL” and “DML” queries with CLI and Beeline</td>
      <td>Review /tmp/step-2.txt to confirm after running the 2-hive-test.sh script. The scripts should create and drop a table via Hive CLI and Beeline.</td>
    </tr>
    <tr>
      <td>You can perform “DDL” and “DML” queries in both execution engines (MapReduce and Tez)</td>
      <td>Review /tmp/step-2.txt to confirm after running the 2-hive-test.sh script. The scripts should create and drop a table via “mr” and “tez.”</td>
    </tr>
    <tr>
      <td>You are able to create an ORC table and run queries</td>
      <td>Show “Create Table” for the ORC table and look for ORC formats. Print the header, index, etc. from one of the ORC files. For additional information, refer to the section of this lesson titled, “Verifying the Ability to Create ORC Files.”</td>
    </tr>
    <tr>
      <td>That statistics are automatically gathered</td>
      <td>The last two queries run in “build-ptn-tbl.sql” while running with Tez should return in under 1-2 seconds.</td>
    </tr>
    <tr>
      <td>Compression</td>
      <td> </td>
    </tr>
    <tr>
      <td>That unsupported configuration, CDH, and MapR jars are not present in the cluster</td>
      <td> </td>
    </tr>
    <tr>
      <td>Queries can run concurrently in different queues</td>
      <td> </td>
    </tr>
    <tr>
      <td>That partitioned data is at least 128 MB</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<hr />

<h4 id="7-testing-performance-of-the-hive-server">7. Testing Performance of the Hive Server</h4>

<p>To test performance of the Hive Server, run the queries below:</p>

<p>To get a list of values that you can run targeted queries with, from either Beeline or Hive CLI, run the following and select 20 items from the list as testing parameters:</p>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">--</span> <span class="nt">Use</span> <span class="nt">this</span> <span class="nt">query</span> <span class="nt">to</span> <span class="nt">locate</span> <span class="nt">a</span> <span class="nt">distinct</span> <span class="nt">search</span> <span class="nt">value</span> <span class="nt">for</span> <span class="nt">testing</span><span class="o">.</span>
<span class="nt">--</span> <span class="nt">The</span> <span class="nt">generated</span> <span class="nt">data</span> <span class="nt">will</span> <span class="nt">start</span> <span class="nt">with</span> <span class="nt">any</span> <span class="nt">of</span> <span class="nt">these</span> <span class="nt">characters</span>
<span class="nt">--</span>    <span class="nt">ABDEF12345</span>
<span class="nt">--</span> <span class="nt">The</span> <span class="nt">eligible</span> <span class="nt">partitions</span> <span class="nt">will</span> <span class="nt">be</span> <span class="nt">by</span> <span class="nt">day</span> <span class="nt">in</span> <span class="nt">Feb</span> <span class="nt">of</span> <span class="nt">2013</span><span class="o">.</span>
<span class="nt">select</span> 
    <span class="nt">distinct</span> <span class="nt">nm</span> 
<span class="nt">from</span> 
    <span class="nt">validation_partition_table</span> 
<span class="nt">where</span> 
    <span class="nt">my_part</span> <span class="o">&gt;=</span> <span class="s2">'2013-02-10'</span> <span class="nt">and</span> <span class="nt">my_part</span> <span class="o">&lt;=</span> <span class="s2">'2014-02-20'</span> 
    <span class="nt">and</span> <span class="s2">'D'</span> <span class="o">=</span> <span class="nt">substr</span><span class="o">(</span><span class="nt">nm</span><span class="o">,</span><span class="nt">1</span><span class="o">,</span><span class="nt">1</span><span class="o">);</span></code></pre></figure>

<hr />

<p>The query below should be run to perform an assortment of performance tests. The partition, “my_part” is valid between ‘2013-02-01’ and ‘2013-02-28’. Be sure to use the 20 items you selected from the above script as replacements for your query:</p>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">--</span> <span class="nt">tez</span> <span class="nt">or</span> <span class="nt">mr</span>
<span class="nt">set</span> <span class="nt">hive</span><span class="nc">.execution.engine</span><span class="o">=</span><span class="err">$</span><span class="p">{</span><span class="err">EXEC_ENGINE</span><span class="p">}</span><span class="o">;</span>
<span class="nt">--</span> <span class="nt">default</span> <span class="nt">or</span> <span class="nt">alt</span>
<span class="nt">set</span> <span class="nt">set</span> <span class="nt">tez</span><span class="nc">.queue.name</span><span class="o">=</span><span class="err">$</span><span class="p">{</span><span class="err">QUEUE_NAME</span><span class="p">}</span><span class="o">;</span>
 
<span class="nt">select</span> 
    <span class="nt">start_dtm</span><span class="o">,</span> <span class="nt">nm</span><span class="o">,</span> <span class="nt">some_int</span> 
<span class="nt">from</span> 
    <span class="nt">validation_partition_table</span> 
<span class="nt">where</span> 
    <span class="nt">my_part</span> <span class="o">&gt;=</span><span class="s2">'${from}'</span> <span class="nt">and</span> <span class="nt">my_part</span> <span class="o">&lt;=</span> <span class="s2">'${to}'</span>
    <span class="nt">and</span> <span class="nt">nm</span> <span class="o">=</span> <span class="s2">'${nm}'</span><span class="o">;</span></code></pre></figure>

<p>The query should be run with the following combinations:</p>
<ul>
  <li>Default queue with execution engine of ‘mr’</li>
  <li>Default queue with execution engine of ‘tez’</li>
  <li>Alt queue with execution engine of ‘mr’</li>
  <li>Alt queue with execution engine of ‘tez’</li>
</ul>

<hr />

<p>Finally, be sure to look for the following items while testing the performance of the Hive server:</p>

<ul>
  <li>
    <p>Look for warmed Application Masters. With “hive.execution.engine=tez”, queries after the first should be significantly faster (10 seconds faster) because the Application Master is being reused.</p>
  </li>
  <li>
    <p>Review the Application Masters in the Resource Manager and check that the queries are attaching to the ‘alt’ queue when specified.</p>
  </li>
</ul>

<hr />

<h4 id="8-verifying-the-ability-to-create-orc-files">8. Verifying the Ability to Create ORC Files</h4>

<p>As part of your validation testing, you should verify that ability to create ORC files. To do this, run the command below:</p>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">beeline</span> <span class="nt">-u</span> <span class="nt">jdbc</span><span class="nd">:hive2</span><span class="o">://&lt;</span><span class="nt">host</span><span class="o">&gt;:&lt;</span><span class="nt">port</span><span class="o">&gt;</span> <span class="nt">-n</span> <span class="err">$</span><span class="nt">USER</span> <span class="nt">-e</span> <span class="s2">'use hdp_validation'</span> <span class="err">\</span>
    <span class="nt">-e</span> <span class="s2">'show table create validation_partition_table'</span> <span class="nt">-e</span> <span class="s2">'show partitions validation_partition_table'</span>
<span class="nt">hive</span> <span class="nt">--service</span> <span class="nt">orcfiledump</span> <span class="nt">hdfs</span><span class="o">:///</span><span class="nt">tmp</span><span class="o">/</span><span class="nt">validation_partition_table_orc</span><span class="o">/</span><span class="nt">my_part</span><span class="o">=</span><span class="nt">2013-02-04</span><span class="o">/</span><span class="nt">000000_0</span></code></pre></figure>

<hr />

<p><strong>Reference - www.hortonworks.com</strong></p>
</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/Data-Compression/" title="Data Compression in Hadoop">Data Compression in Hadoop</a></h3>
      <p>An Overview on data compression in hadoop </p>
      <h6> Posted on : 2015-10-03 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><p>Hadoop users are encouraged to keep all data in order to prepare for future use cases and as-yet-unknown data integration points. This concept is part of what makes Hadoop and HDFS so appealing, so it is important to make sure that the data is being stored in a way that prolongs that behavior.One important factor in improving manageability is data compression.In many cluster environments, compression is disabled by default, putting the burden on the user.
We need to decide and know on how to take advantage of compression techniques and the advantages and disadvantages of specific compression codec options with respect to Hadoop.</p>

<h2 id="to-compress-or-not-to-compress">To compress or not to compress</h2>

<p>Whenever data is converted to something other than its raw data format, there is some overhead involved in completing the conversion process. It is important to take this overhead into account with respect to the benefits of reducing the data footprint.</p>

<p>One obvious benefit is that compressed data will reduce the amount of disk space that is required for storage of a particular dataset. In the big data environment, this benefit is especially significant</p>

<ul>
  <li>the Hadoop cluster will be able to keep data for a larger time range</li>
  <li>storing data for the same time range will require fewer nodes</li>
  <li>the disk usage ratios will remain lower for longer</li>
  <li>the smaller file sizes will mean lower data transfer times;either internally for MapReduce jobs or when performing exports of data results</li>
</ul>

<p>The cost of these benefits, however, is that</p>

<ul>
  <li>the data must be decompressed at every point where the data needs to be read, and compressed before being inserted into HDFS.</li>
</ul>

<p>With respect to MapReduce jobs, this processing overhead at both the map phase and the reduce phase will increase the CPU processing time.</p>

<p>How to ensure that the advantages of compression outweigh the disadvantages.</p>

<h2 id="choosing-the-right-codec-for-each-phase">Choosing the right codec for each phase</h2>

<p>Hadoop provides the user with some flexibility on which compression codec is used at each step of the data transformation process.Certain codecs are optimal for some stages, and non-optimal for others.</p>

<ul>
  <li>zlib</li>
</ul>

<p>The major benefit of using this codec is that it is the easiest way to get the benefits of data compression from a cluster and on the job configuration standpoint—the zlib codec is the default compression option. From the data transformation perspective, this codec will decrease the data footprint on disk, but will not provide much of a benefit in terms of job performance.</p>

<ul>
  <li>gzip</li>
</ul>

<p>The gzip codec available in Hadoop is the same one that is used outside of the Hadoop ecosystem. It is a common practice to use this as the codec for compressing the final output from a job, simply for the benefit of being able to share the compressed result with others (possibly outside of Hadoop) using a standard file format.</p>

<ul>
  <li>bzip2</li>
</ul>

<p>There are two important benefits for the bzip2 codec.</p>
<blockquote>
  <p>First, if reducing the data footprint is a high priority, this algorithm will compress the data more than the default zlib option.</p>
</blockquote>

<blockquote>
  <p>Second, this is the only supported codec that produces “splittable” compressed data.</p>
</blockquote>

<p>A major characteristic of Hadoop is the idea of splitting the data so that they can be handled on each node independently. With the other compression codecs, there is an initial requirement to gather all parts of the compressed file in order to have all information necessary to decompress the data. With this format, the data can be decompressed in parallel. This splittable quality makes this format ideal for compressing data that will be used as input to a map function, either in a single step or as part of a series of chained jobs.</p>

<ul>
  <li>LZO, LZ4, Snappy</li>
</ul>

<p>These three codecs are ideal for compressing intermediate data, the data output from the mappers that will be immediately read in by the reducers. All three codecs heavily favor compression speed over file size ratio, but the detailed specifications for each algorithm should be examined based on the specific licensing, cluster, and job requirements.</p>

<h2 id="enabling-compression">Enabling compression</h2>

<p>Once the appropriate compression codec for any given transformation phase has been selected, there are a few configuration properties that need to be adjusted in order to have the changes take effect in the cluster.</p>

<ul>
  <li>Intermediate data to reducer</li>
</ul>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">mapreduce</span><span class="nc">.map.output.compress</span> <span class="o">=</span> <span class="nt">true</span>
<span class="o">(</span><span class="nt">Optional</span><span class="o">)</span> <span class="nt">mapreduce</span><span class="nc">.map.output.compress.codec</span> <span class="o">=</span> <span class="nt">org</span><span class="nc">.apache.hadoop.io.compress.SnappyCodec</span></code></pre></figure>

<ul>
  <li>Final output from a job</li>
</ul>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">mapreduce</span><span class="nc">.output.fileoutputformat.compress</span> <span class="o">=</span> <span class="nt">true</span>
<span class="o">(</span><span class="nt">Optional</span><span class="o">)</span> <span class="nt">mapreduce</span><span class="nc">.output.fileoutputformat.compress.codec</span> <span class="o">=</span> <span class="nt">org</span><span class="nc">.apache.hadoop.io.compress.BZip2Codec</span></code></pre></figure>

<ul>
  <li>Within Hive &amp; Pig</li>
</ul>

<p>These compression codecs are also available within some of the ecosystem tools like Hive and Pig. In most cases, the tools will default to the Hadoop-configured values for particular codecs, but the tools also provide the option to compress the data generated between steps.</p>

<ul>
  <li>Pig</li>
</ul>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">pig</span><span class="nc">.tmpfilecompression</span> <span class="o">=</span> <span class="nt">true</span>
<span class="o">(</span><span class="nt">Optional</span><span class="o">)</span> <span class="nt">pig</span><span class="nc">.tmpfilecompression.codec</span> <span class="o">=</span> <span class="nt">snappy</span></code></pre></figure>

<ul>
  <li>Hive</li>
</ul>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">hive</span><span class="nc">.exec.compress.intermediate</span> <span class="o">=</span> <span class="nt">true</span>
<span class="nt">hive</span><span class="nc">.exec.compress.output</span> <span class="o">=</span> <span class="nt">true</span></code></pre></figure>
</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/Apache_Hive_Overview/" title="Apache Hive Overview ">Apache Hive Overview </a></h3>
      <p>An Overview on Apache Hive </p>
      <h6> Posted on : 2015-10-01 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><p>Apache Hive is part of Data Access in the Hadoop ecosystem and can be installed when you install the Hortonworks Data Platform.
Topics covered in this post :</p>

<ul>
  <li>Explain why Hive was developed</li>
  <li>Assess whether Hive is suitable for online transaction processing or online analytical processing</li>
  <li>List differences between Hive on MapReduce and Hive on Tez</li>
  <li>Describe how Hive projects structure on unstructured data</li>
  <li>List methods to submit Hive queries</li>
  <li>Recognize a few basic Hive query language commands</li>
</ul>

<h2 id="the-problem">The Problem</h2>

<p>Until recently, most enterprise data has been stored in relational databases</p>

<ul>
  <li>It has been analyzed using a structured query language (SQL)</li>
  <li>The result is that many data analysts are familiar with SQL
The problem is that Hadoop data is commonly analyzed by MapReduce programs</li>
  <li>Many data analysts are not familiar with MapReduce</li>
</ul>

<h2 id="how-do-enterprises-bridge-this-knowledge-gap">How do enterprises bridge this knowledge gap?</h2>

<p>Until recently most of the data maintained by an enterprise has been stored in a relational database and has been analyzed using a structured query language. As a result, most data analysts today are familiar with a structured query language.However, data in Hadoop is commonly analyzed using MapReduce. Many data analysts are not familiar with MapReduce and would require training to use it.This limits how quickly an enterprise can derive value from a Hadoop deployment.</p>

<h2 id="the-solution">The Solution</h2>

<p>Apache Hive bridges the knowledge gap by converting SQL-like commands to MapReduce jobs*</p>

<ul>
  <li>Hive is a data warehouse infrastructure built on top of Hadoop</li>
  <li>It was designed to enable experienced database users to analyze data using familiar SQL-like statements</li>
  <li>Hive includes a SQL-like language called Hive Query Language (HQL)</li>
  <li>With HQL, enterprises can utilize existing skillsets to quickly derive value from a Hadoop deployment</li>
</ul>

<p><code class="highlighter-rouge">Conversion to Apache Tez jobs is an option - will cover in a later post</code></p>

<p>Apache Hive bridges the knowledge gap by enabling data analysts to use familiar SQL-like commands that are automatically converted to MapReduce jobs and executed across the Hadoop cluster.Hive is a data warehouse infrastructure built on top of Hadoop. It was designed to enable users with database experience to analyze data using familiar SQL-like statements. Hive includes a SQL-like language called Hive Query Language, or HQL. Hive and HQL enable an enterprise to utilize existing skillsets to quickly derive value from a Hadoop deployment.To avoid any potential confusion, you should be aware of a recent change to Hive. Hive’s default behavior is to convert HQL statements to MapReduce jobs. However starting with version 0.13.0, Hive’s default behavior can be modified so that it converts HQL statements to Apache Tez jobs. Hive on Tez significantly improves the performance of Hive queries. For example, Hive on MapReduce is suitable for batch queries whereas Hive on Tez is suitable for batch or interactive queries.</p>

<h2 id="oltp-or-olap">OLTP or OLAP?</h2>

<ul>
  <li>Hive is used for online analytical processing (OLAP) and not online transaction processing (OLTP)</li>
  <li>Hive was designed for batch rather than interactive queries</li>
  <li>Even the simplest Hive queries can take minutes to complete because of MapReduce - ( Although interactive queries are possible with Hive on Tez )</li>
  <li>Hive offers no support for row-level inserts, updates, and deletes - (Work is being done to add these features)</li>
</ul>

<p>Hive is used for online analytical processing (OLAP) and not online transaction processing (OLTP). This is because Hive was originally designed to run batch jobs rather than performing interactive queries or random table updates. Currently Hive offers no support for row-level inserts, updates, and deletes which are commonly required for OLTP. When Hive is run over MapReduce even the simplest Hive queries can take minutes to complete.If you run Hive over Tez rather than MapReduce, Hive is still not designed for OLTP. While Tez increases interactive performance, Hive still has no supportfor row-level inserts, updates, and deletes. However, work is currently being done to add these features to Hive.</p>

<h2 id="structuring-unstructured-data">Structuring Unstructured Data</h2>

<ul>
  <li>Hive is not a relational database although it might appear like one</li>
  <li>Hadoop collects and stores heterogeneous data types</li>
  <li>Hive has a mechanism to project structure onto this data</li>
  <li>The Hive metastore database stores user-defined table schemas</li>
  <li>HDFS stores the unstructured data</li>
</ul>

<p>Hive is not a relational database although, on the surface, it can appear like one.Hadoop was built to collect, store, and analyze massive amounts of data. As such, the Hadoop distributed file system, called HDFS, is a reservoir of data from multiple sources. The data is often a mix of unstructured, semi-structured, and structured data. Hive provides a mechanism to project structure onto HDFS data and then query it using HQL. However, there is a limit to what Hive can do. Sometimes it is necessary to use another tool, like Apache Pig, to pre-format the unstructured data before processing it using Hive.If you are familiar with databases, then you understand that unstructured data has no schema associated with it. If you are not familiar with database schemas, they define the columns of a database along with the type of data in each column. Data types include such things as a string, an integer, a floating point number, or a date.</p>

<p>A Hive installation includes a metastore database. Several database types are supported by Hive including an embedded Derby database used for development or testing, or an external database like MySQL used for production deployments. To project structure on HDFS data, HQL includes statements to create a table with user-defined schema information. The table schema is stored in the metastore database.The user-defined schema is associated with the data stored in one or more HDFS files when you use HQL statements to load the files into a table. The format of the data on HDFS remains unchanged but it appears as structured data whenusing HQL commands to submit queries.</p>

<h2 id="hive-includes-four-methods-to-submit-queries">Hive includes four methods to submit queries.</h2>

<ul>
  <li>The Hive CLI</li>
  <li>Beeline CLI</li>
  <li>Web UI</li>
  <li>ODBC and JDBC drivers</li>
</ul>

<p>Hive includes many methods to submit queries. Queries submitted to either the HiveServer or newer HiveServer2 result in a MapReduce or Tez job being submitted to YARN. YARN, the Hadoop resource scheduler, works in concert with HDFS to run the job in parallel across the machines in the cluster.The Hive CLI is used to interactively or non-interactively submit HQL commands to the HiveServer.The illustration shows the Hive CLI being used interactively. Users enter HQL commands at the hive&gt; prompt. HQL commands can also be placed into a file and run using hive –f file_name.</p>

<p>The remaining three methods all submit HQL queries to the newer HiveServer2.
The Beeline CLI is a new JDBC client that connects to a local or remote HiveServer2. When connecting locally, beeline works just like the Hive CLI. Beeline can connect to a remote HiveServer2 usinga variety of methods that include TCP and HTTP. For example, HTTP access is useful for submitting queries to a firewall protected cluster, assuming the firewall will allow HTTP traffic.The Web UI, called the Hive Web Interface or HWI, enables you to submit Hive queries remotely using HTTP. Again, this is useful for submitting queries to a firewall protected cluster. The difference between using the Web UI or beeline is that no Hive client software has to be installed to use the Web UI.ODBC and JDBC drivers enable you to connect to popular business intelligence tools to query, analyze,and visualize Hive data.</p>

<p>While not illustrated above, you may also use the Hadoop User Experience, called HUE, to enter Hive queries. HUE is a graphical interface that enables you to interactively enter Hive queries, similar to what you would do from the Hive shell. Like most graphical tools, it provides many features to make Hive analysis easier. For example, you can write and test a series of Hive queries and then easily save them to a script for future use. You can use a single interface to execute queries and view the results and execution logs.</p>

<h4 id="example-hql-commands">Example HQL Commands</h4>

<blockquote>
  <p>Create a table:
CREATE TABLE stockinfo (symbol STRING, price FLOAT, change FLOAT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’;</p>
</blockquote>

<blockquote>
  <p>Load data from file in HDFS:
LOAD DATA INPATH ‘/user/me/stockdata.csv’ OVERWRITE INTO TABLE stockinfo;</p>
</blockquote>

<blockquote>
  <p>View everything in the table:
SELECT * from stockinfo;</p>
</blockquote>

<p>A few common HQL commands are illustrated here.</p>

<p>The first command creates a table named stockinfo. The table is created with three columns named symbol, price, and change. Any data in the first column will be treated as a string of characters. Any data in the second and third columns will be treated as a floating point number. The ROW FORMAT clause tells Hive to expect each row of data to be delimited by commas.</p>

<p>The second command loads the data in the HDFS file named /user/me/stockdata.csv into a table named stockinfo. If any data has been previously loaded into the table it will overwritten. Loading datainto a table only associates the data with the table. The data is not altered in any way.The final command displays the entire contents of the stockinfo table. Such a command could take a long timeto complete if there is a large amount of data.(HQL syntax rules require every command end with a semi-colon)</p>

<h2 id="hive-demonstration">Hive Demonstration</h2>
<p>The demonstration is based on one of the Hortonworks Sandbox tutorials.
The Sandbox is a downloadable virtual machine preinstalled and preconfigured with a Hadoop cluster.
It runs all master, slave, and client components on the same machine. 
The Sandbox machine and tutorials are found at <a href="http://hortonworks.com/products/hortonworks-sandbox/">Hortonworks</a> website.</p>
</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/HDP-Cluster-Validation-HDFS/" title="Cluster Validation - HDFS - HDP">Cluster Validation - HDFS - HDP</a></h3>
      <p>Cluster validation methodologies & HDFS checklist</p>
      <h6> Posted on : 2015-06-22 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><p>For every PS engagement that involves installation, upgrade, or migration, it is your responsibility to ensure that the installation validation checklist is completely executed.This will ensure:</p>
<ul>
  <li>Proper configuration of the cluster</li>
  <li>Increased customer satisfaction</li>
  <li>Decreased volume to the Technical Support team</li>
</ul>

<blockquote>
  <p>The Importance of the Checklist</p>
</blockquote>

<p>The installation validation checklist is a tool that you should refer to throughout the duration of your engagement.It was assembled by the Technical Support team and contains information and best practices gathered while working on multiple projects with multiple clients.The checklists provide information on key areas that you should focus on to ensure proper configuration of the components of your cluster. This will result in an operational Customer Environment.</p>

<h4 id="hdfs-checklist-items">HDFS Checklist Items</h4>

<p>There are multiple items that should be checked to ensure that HDFS is configured properly. The table below provides the key areas of focus along with the rationale behind them:</p>

<table>
  <thead>
    <tr>
      <th>Checklist Item</th>
      <th>Notes/Rationale</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Verify that TestDFSIO has targeted 10% of live nodes</td>
      <td>Refer to the section below , “Running TestDFSIO.”</td>
    </tr>
    <tr>
      <td>Put 1 GB of data into HDFS</td>
      <td>This should take approximately 10-17 seconds depending on the infrastructure. Perform this several times to see if the resulting times vary. You can run the following commands:mkfile 1g testfile</td>
    </tr>
    <tr>
      <td>hadoop fs –put testfile /tmp</td>
      <td> </td>
    </tr>
    <tr>
      <td>Verify the NameNode, DataNode, and JournalNode memory settings</td>
      <td>Young generation (Namenode new generation size) should be set up with 1/8 of total heap size. Refer to the Memory Sizing Document  located in the Hortonworks Wiki.</td>
    </tr>
    <tr>
      <td>Verify that young generation should be set up with 1/8 of total heap size</td>
      <td>In general, this is a good starting point.</td>
    </tr>
    <tr>
      <td>Validate that log, data, and metadata mounts have enough room to grow</td>
      <td>Log directories should NOT (if possible) be on the same mount as the root OS. Create a separate partition for logs and check the partition to ensure at least a few gigs of space are available. Consider providing the hdp-log-archive.sh  script to the operations team for log maintenance.</td>
    </tr>
    <tr>
      <td>Verify ulimits for all service users</td>
      <td>Be sure to make sure that ulimits for root are set up correctly for secure clusters. Limits should be set to the following value for users, hdfs, MR, YARN, HBASE and root (in a secure cluster only):</td>
    </tr>
    <tr>
      <td>- nofile 32768</td>
      <td> </td>
    </tr>
    <tr>
      <td>- nproc 65536</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>You can modify the ulimit -l value that the DataNode runs with. This value is usually configured in /etc/security/limits.conf</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>Configure the open fd ulimit</td>
      <td> </td>
    </tr>
    <tr>
      <td>- the default of 1024 is too low</td>
      <td> </td>
    </tr>
    <tr>
      <td>- use 16K for datanodes</td>
      <td> </td>
    </tr>
    <tr>
      <td>- use 64K for masternodes</td>
      <td> </td>
    </tr>
    <tr>
      <td>Verify that user groups, IDs, and home directories are set up uniformly on all nodes</td>
      <td> </td>
    </tr>
    <tr>
      <td>If the NFS Gateway is used, verify that it was mounted with soft mount</td>
      <td>The scratch directory needs to have enough room and at least 3 GB of memory.</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>You should have multiple redundant directories for namenode metadata:</td>
      <td> </td>
    </tr>
    <tr>
      <td>- one of the the dfs.name.dir should be NFS</td>
      <td> </td>
    </tr>
    <tr>
      <td>- NFS softmount - tcp,soft,intr, timeo=20, retrans=5</td>
      <td> </td>
    </tr>
    <tr>
      <td>Verify that all relevant mounts are used for dfs.data.dir property</td>
      <td>For example, this should not be set to /tmp or /var/log. Also, check if you have more than 12 mounts for DataNode directories, and that you have configured DataNode volume failure toleration to be at least 2.</td>
    </tr>
    <tr>
      <td>Verify that configurations and environment variables are consistent on all nodes, especially if Ambari is not used</td>
      <td>If Ambari is not setup, it is best to have all configurations managed by Puppet (Chef etc.) or have one “Master” copy distributed across all nodes.</td>
    </tr>
    <tr>
      <td>Validate that the Hadoop Classpath is set up consistently on all nodes</td>
      <td>Check this if Ambari is not setup. Below is an example for HDP 2.3</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>/usr/hdp/2.3.0.0-2557/hadoop/conf:/usr/hdp/2.3.0.0-2557/hadoop/lib/<em>:/usr/hdp/2.3.0.0-2557/hadoop/.//</em>:/usr/hdp/2.3.0.0-2557/hadoop-hdfs/./:/usr/hdp/2.3.0.0-2557/hadoop-hdfs/lib/<em>:/usr/hdp/2.3.0.0-2557/hadoop-hdfs/.//</em>:/usr/hdp/2.3.0.0-2557/hadoop-yarn/lib/<em>:/usr/hdp/2.3.0.0-2557/hadoop-yarn/.//</em>:/usr/hdp/2.3.0.0-2557/hadoop-mapreduce/lib/<em>:/usr/hdp/2.3.0.0-2557/hadoop-mapreduce/.//</em>:::/usr/share/java/mysql-connector-java.jar</td>
      <td> </td>
    </tr>
    <tr>
      <td>Verify that all unsupported CDH and HDP 1.x configurations are removed</td>
      <td>Verify along with all remaining CDH and HDP1.x in the log and all CDH HDP1.x repositories.</td>
    </tr>
    <tr>
      <td>Verify that transceivers and handler counts the NameNodes and DataNodes</td>
      <td>Be sure to verify:dfs.datanode.max.transfer.threads is at least 4096The NameNode Handler count is equal to 128 for more than 100 DataNodesThe DataNode handler count is between 10 - 20</td>
    </tr>
  </tbody>
</table>

<h4 id="running-testdfsio">Running TestDFSIO</h4>

<p>Below are the steps you should take to run TestDFSIO to target 10% of live nodes:</p>

<ol>
  <li>Run TestDFSIO in write mode and create data with the command:
    <blockquote>
      <p>yarn jar $YARN_EXAMPLES/hadoop-mapreduce-client-jobclient-2.1.0-beta-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 1000</p>
    </blockquote>
  </li>
  <li>Run TestDSFIO in read mode with the command:
    <blockquote>
      <p>yarn jar $YARN_EXAMPLES/hadoop-mapreduce-client-jobclient-2.1.0-beta-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 1000</p>
    </blockquote>
  </li>
  <li>Clean up the TestDFSIO data with the command:
    <blockquote>
      <p>yarn jar $YARN_EXAMPLES/hadoop-mapreduce-client-jobclient-2.1.0-beta-tests.jar TestDFSIO -clean</p>
    </blockquote>
  </li>
</ol>

<h4 id="points-to-rememnber">Points to rememnber</h4>

<ol>
  <li>The HDFS checklist should be completed to ensure optimal configuration of HDFS in the cluster</li>
  <li>You should run TestDFSIO to target at least 10% of live nodes</li>
</ol>

<hr />

</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/HDP-Cluster-Validation/" title="Cluster Validation General/OS - HDP">Cluster Validation General/OS - HDP</a></h3>
      <p>Cluster validation methodologies & checklist</p>
      <h6> Posted on : 2015-06-21 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><p>For every PS engagement that involves installation, upgrade, or migration, it is your responsibility to ensure that the installation validation checklist is completely executed.This will ensure:</p>
<ul>
  <li>Proper configuration of the cluster</li>
  <li>Increased customer satisfaction</li>
  <li>Decreased volume to the Technical Support team</li>
</ul>

<blockquote>
  <p>The Importance of the Checklist
The installation validation checklist is a tool that you should refer to throughout the duration of your engagement.It was assembled by the Technical Support team and contains information and best practices gathered while working on multiple projects with multiple clients.The checklists provide information on key areas that you should focus on to ensure proper configuration of the components of your cluster. This will result in an operational Customer Environment.</p>
</blockquote>

<h4 id="generalos-checklist-items">General/OS Checklist Items</h4>
<p>There are multiple items that should be checked to ensure that the General/OS functionality of the cluster is configured properly. The table below provides the key areas that you should focus on, along with the rationale behind them:</p>

<table>
  <thead>
    <tr>
      <th><strong>Checklist Item</strong></th>
      <th><strong>Notes/Rationale</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Validate that the customer has some type of log/retention/archival/deletion process in place</strong></td>
      <td>Chron and the <a href="https://raw.githubusercontent.com/dstreev/hdp-utils/master/operations/hdp-log-archive.sh"><em>Log Archival Process Script</em></a> can be used to archive the HDP logs.</td>
    </tr>
    <tr>
      <td><strong>Validate the cluster is set up with FQDNs</strong></td>
      <td>This is important because Ambari uses FQDN internally. Use the command below to verify:hostname -f</td>
    </tr>
    <tr>
      <td><strong>Validate that forward and Name resolution is set up consistently and is working as expected</strong></td>
      <td>This is critical for Ambari and can be potentially critical for HDFS.</td>
    </tr>
    <tr>
      <td><strong>Validate that all installation pre-requisites are met, including OS umask 022 (iptables, selinux, name resolution, and ntpd)</strong></td>
      <td>Ambari cannot handle iptables. As a result, you must disable them before installation. The iptables can be re-enabled after the installation has been completed with all needed ports opened.</td>
    </tr>
    <tr>
      <td><strong>Validate that * proxy hosts and users are setup properly in core-site.xml</strong></td>
      <td> </td>
    </tr>
    <tr>
      <td><strong>Validate that the mount is setup properly and uses the proper file system</strong></td>
      <td>The recommended file system is XFS or EXT4. Refer to the <a href="https://wiki.hortonworks.com/display/PS/DRAFT+-+HDP+Tuning"><em>HDP Tuning Document</em></a> for mount options.</td>
    </tr>
    <tr>
      <td><strong>Validate that Hive, Ambari, and Oozie Databases and local directories have enough space to grow</strong></td>
      <td> </td>
    </tr>
    <tr>
      <td><strong>Verify that user limits are set up with high enough numbers</strong></td>
      <td>Refer to the <a href="https://wiki.hortonworks.com/display/PS/DRAFT+-+HDP+Tuning"><em>HDP Tuning Document</em></a>.</td>
    </tr>
    <tr>
      <td><strong>Verify that all users are created consistently on all nodes</strong></td>
      <td>This is mandatory for HDFS and YARN functionality.</td>
    </tr>
    <tr>
      <td><strong>Verify that IPv6 and THP are disabled</strong></td>
      <td>See next section in this topic.</td>
    </tr>
    <tr>
      <td><strong>Verify that no custom jars are present in the system</strong></td>
      <td>Nodes should be cleanly re-imaged and not reused after manual cleaning.</td>
    </tr>
    <tr>
      <td><strong>Verify that HDP init.d and other unsupported scripts are disabled and removed</strong></td>
      <td>Init.d scripts are supposed to be working and supported in HDP 2.2.</td>
    </tr>
    <tr>
      <td><strong>Verify that all unsupported CDH and HDP 1.x configurations</strong></td>
      <td>Nodes should be cleanly re-imaged, not reused after manual cleaning.</td>
    </tr>
    <tr>
      <td><strong>Verify that there are no dropped packets, network errors or overruns on interfaces</strong></td>
      <td> </td>
    </tr>
    <tr>
      <td><strong>Verify that Rack Topology is set up</strong></td>
      <td>Rack topology is important and should be set up as soon as possible after the cluster is initialized. If it is set up later, you will not be protected from Rack failures.</td>
    </tr>
  </tbody>
</table>

<h4 id="verifying-that-ipv6-and-thp-are-disabled">Verifying that IPv6 and THP are disabled</h4>

<p>Below are the steps required to disable THP. Please note that in the following instructions, defrag_file_pathname depends on the operating system:</p>

<ul>
  <li>Red Hat/CentOS: /sys/kernel/mm/redhat_transparent_hugepage/defrag</li>
  <li>Ubuntu/Debian/OEL/SLES: /sys/kernel/mm/transparent_hugepage/defrag</li>
</ul>

<p>To determine if transparent hugepage compaction is enabled, run the following commands and check the output:</p>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="err">$</span> <span class="nt">cat</span> <span class="nt">defrag_file_pathname</span></code></pre></figure>

<ul>
  <li>[always] never means that transparent hugepage compaction is enabled</li>
  <li>always [never] means that transparent hugepage compaction is disabled</li>
</ul>

<p>You can also disable transparent hugepage compaction interactively (the settings do not persist over a reboot).</p>

<p>To disable transparent hugepage compaction, add the following command to /ect/rc.local:</p>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">echo</span> <span class="nt">never</span> <span class="o">&gt;</span> <span class="nt">defrag_file_pathname</span> </code></pre></figure>

<p>Use the following command to disable transparent hugepage compaction temporarily as root:</p>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">echo</span> <span class="s2">'never'</span> <span class="o">&gt;</span> <span class="nt">defrag_file_pathname</span> </code></pre></figure>

<p>Use the following command to disable transparent hugepage compaction temporarily using sudo:</p>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="err">$</span> <span class="nt">sudo</span> <span class="nt">sh</span> <span class="nt">-c</span> <span class="s1">"echo 'never' &gt; defrag_file_pathname"</span></code></pre></figure>

<hr />
</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/HDP-Cluster-Validation-Ambari/" title="Cluster Validation - Ambari - HDP">Cluster Validation - Ambari - HDP</a></h3>
      <p>Cluster validation methodologies & ambari checklist</p>
      <h6> Posted on : 2015-05-22 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><p>For every PS engagement that involves installation, upgrade, or migration, it is your responsibility to ensure that the installation validation checklist is completely executed.This will ensure:</p>
<ul>
  <li>Proper configuration of the cluster</li>
  <li>Increased customer satisfaction</li>
  <li>Decreased volume to the Technical Support team</li>
</ul>

<blockquote>
  <p>The Importance of the Checklist</p>
</blockquote>

<p>The installation validation checklist is a tool that you should refer to throughout the duration of your engagement.It was assembled by the Technical Support team and contains information and best practices gathered while working on multiple projects with multiple clients.The checklists provide information on key areas that you should focus on to ensure proper configuration of the components of your cluster. This will result in an operational Customer Environment.</p>

<h4 id="ambari-checklist-items">Ambari Checklist Items</h4>

<p>There are multiple items that should be checked to ensure that Ambari is configured properly. The table below provides the key areas of focus, along with the rationale behind them:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"><strong>Checklist Item</strong></th>
      <th style="text-align: left"><strong>Notes/Rationale</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Verify that the Ambari UI is up</strong></td>
      <td style="text-align: left">It is important to check if the Ambari UI is accessible and responsive. For additional details, refer to the section of this lesson titled, “Verifying that the Ambari UI is Up.”</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Verify that all services Start/Stop and that the smoke test passes</strong></td>
      <td style="text-align: left">It is important to verify that all services are in “Green” status. Perform the following actions:In the Ambari UI, verify that all services are in the “Green” statusFor each service, run the service check, all should return a “Green” statusIf any of the services do not return a “Green” status, examine the logs for potential issues. If the cause of the issue is still unclear, open a support case.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Verify that the live number DN and NM are matched to the NN and RM UI</strong></td>
      <td style="text-align: left">Information presented by Ambari and by NN/RM UIs should match the real status for running DN/NM processes.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Verify that there are no nodes with lost heartbeats or other errors</strong></td>
      <td style="text-align: left">A lost heartbeat may be the reason for false alarms. They can be caused by several factors, including:A slow networkNodes overloading (specifically an Ambari server node overload)An actual node being downAn Ambari agent process being down</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Verify that unneeded nodes are removed</strong></td>
      <td style="text-align: left">If a node is dead and cannot be revived or if the node is decommissioned, it should be removed from Ambari to avoid any confusion and/or false alerts.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Verify that Ambari is upgraded to the latest stable release</strong></td>
      <td style="text-align: left">Ambari should be on the latest release certified for running the HDP stack. For additional details, refer to the section of this lesson titled, “Verifying that Ambari is Upgraded to the Lastest Stable Release.”</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Verify that all graphs are working as expected in the Ambari UI</strong></td>
      <td style="text-align: left">If the graphs are not present, this may be due to:An incorrect name resolutionWrong network interface bindings</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Verify that Nagios or Ganglia alerts are resolved</strong></td>
      <td style="text-align: left">These alerts should be resolved because they are an indication that there are issues in the cluster.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Verify that any customizations during the install process are documented</strong></td>
      <td style="text-align: left">Be sure to document any customizations so to that the support team is aware of changes to LDAP, proxies, HTTPS, etc.</td>
    </tr>
  </tbody>
</table>

<hr />

<h4 id="verifying-that-the-ambari-ui-is-up">Verifying that the Ambari UI is up</h4>
<p>In addition to verifying the other items on the checklist, it is important to check if the Ambari UI is accessible and responsive. Below are the actions you should take to ensure that the UI is functioning properly:
First, go to http://<ambari-server-host>:8080; you should see a login dialog. If you do not see the login dialog, use the table below to troubleshoot the issue:</ambari-server-host></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"><strong>If…</strong></th>
      <th style="text-align: left"><strong>Then…</strong></th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">There is no response when you try to access the UI</td>
      <td style="text-align: left">SSH into the Ambari server node and verify that the server is running using: </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: left">Netstat –apnl</td>
      <td style="text-align: left">grep 8080, ps –ef</td>
      <td>grep Ambari</td>
    </tr>
    <tr>
      <td style="text-align: left">The process is running but listening on the wrong host name</td>
      <td style="text-align: left">Adjust the name resolution</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: left">The process is not running</td>
      <td style="text-align: left">Start the process</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: left">The process is running (and on the right port) but not responding</td>
      <td style="text-align: left">Restart the process</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: left">There is still no response after the restart</td>
      <td style="text-align: left">Examine the log files</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: left">The log files do not reveal any information related to potential issues</td>
      <td style="text-align: left">Open a support case and engage the Ambari development team</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<hr />

<h3 id="verifying-that-ambari-is-upgraded-to-the-latest-stable-release">Verifying that Ambari is Upgraded to the Latest Stable Release</h3>
<p>Ambari should be on the latest release certified by the HDP stack. Please refer to the table below:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"><strong>Ambari</strong></th>
      <th style="text-align: left"><strong>HDP 2.2</strong></th>
      <th style="text-align: left"><strong>HDP 2.1</strong></th>
      <th style="text-align: left"><strong>HDP 2.0</strong></th>
      <th style="text-align: left"><strong>HDP 1.3</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">1.7.0</td>
      <td style="text-align: left">X</td>
      <td style="text-align: left">X</td>
      <td style="text-align: left">X</td>
      <td style="text-align: left">X</td>
    </tr>
    <tr>
      <td style="text-align: left">1.6.1</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">X</td>
      <td style="text-align: left">X</td>
      <td style="text-align: left">X</td>
    </tr>
    <tr>
      <td style="text-align: left">1.6.0</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">X</td>
      <td style="text-align: left">X</td>
      <td style="text-align: left">X</td>
    </tr>
    <tr>
      <td style="text-align: left">1.5.1</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">X</td>
      <td style="text-align: left">X</td>
      <td style="text-align: left">X</td>
    </tr>
    <tr>
      <td style="text-align: left">1.5.0</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">X</td>
      <td style="text-align: left">X</td>
    </tr>
    <tr>
      <td style="text-align: left">1.4.4.23</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">X</td>
      <td style="text-align: left">X</td>
    </tr>
    <tr>
      <td style="text-align: left">1.4.3.38</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">X</td>
      <td style="text-align: left">X</td>
    </tr>
    <tr>
      <td style="text-align: left">1.4.2.104</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">X</td>
      <td style="text-align: left">X</td>
    </tr>
    <tr>
      <td style="text-align: left">1.4.1.61</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">X</td>
      <td style="text-align: left">X</td>
    </tr>
    <tr>
      <td style="text-align: left">1.4.1.25</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">X</td>
      <td style="text-align: left">X</td>
    </tr>
    <tr>
      <td style="text-align: left">1.2.3.17</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">X</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Ambari 1.7x does not install Accumulo, Hue, Ranger or Solr services for the HDP 2.2 Stack</li>
  <li>Ambari 1.7x does not install Accumulo, Hue, Knox or Solr services for the HDP 2.1 Stack</li>
  <li>Ambari 1.7.x does not install Hue for the HDP 2.0 Stack</li>
</ul>

<hr />

</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/HDP_Argus-Enterprise-Readiness/" title="Enterprise Readiness with Argus & HDP Champlain">Enterprise Readiness with Argus & HDP Champlain</a></h3>
      <p>Argus & HDP Champlain to provide enterprise readiness</p>
      <h6> Posted on : 2014-10-27 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><h3 id="enterprise-readiness-rest-apis">Enterprise Readiness (REST APIs)</h3>

<ul>
  <li>Currently, Argus policies can only be managed through GUI</li>
  <li>Not a scalable model if there are large number of policies</li>
  <li>Champlain work to expose REST APIs for the policy manager</li>
  <li>Users can create/update/delete policies through these APIs</li>
</ul>

<hr />

<blockquote>
  <p>REST APIS AVAILABLE - Repository Management</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th>REST API</th>
      <th>Request type</th>
      <th>Request URL*</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Get Repository</td>
      <td>GET</td>
      <td>service/public/api/repository/{id}</td>
    </tr>
    <tr>
      <td>Create Repository</td>
      <td>POST</td>
      <td>service/public/api/repository</td>
    </tr>
    <tr>
      <td>Update Repository</td>
      <td>PUT</td>
      <td>service/public/api/repository/{id}</td>
    </tr>
    <tr>
      <td>Delete Repository</td>
      <td>DELETE</td>
      <td>service/public/api/repository/{id}</td>
    </tr>
    <tr>
      <td>Search Repositories</td>
      <td>GET</td>
      <td>service/public/api/repository</td>
    </tr>
  </tbody>
</table>

<hr />

<blockquote>
  <p>REST API’s exposed in Champlain - Policy Management</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th>REST API</th>
      <th>Request type</th>
      <th>Request URL*</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Get Policy</td>
      <td>GET</td>
      <td>service/public/api/policy/{id}</td>
    </tr>
    <tr>
      <td>Create Policy</td>
      <td>POST</td>
      <td>service/public/api/policy</td>
    </tr>
    <tr>
      <td>Update Policy</td>
      <td>PUT</td>
      <td>service/public/api/policy/{id}</td>
    </tr>
    <tr>
      <td>Delete Policy</td>
      <td>DELETE</td>
      <td>service/public/api/policy/{id}</td>
    </tr>
    <tr>
      <td>Search Policies</td>
      <td>GET</td>
      <td>service/public/api/policy</td>
    </tr>
  </tbody>
</table>

<h3 id="enterprise-readiness-audit-log-storage-in-hdfs">Enterprise Readiness (Audit Log Storage in HDFS)</h3>

<ul>
  <li>Pre Champlain
    <ul>
      <li>Argus audit data only in RDBMS (mysql)
        <ul>
          <li>Issue with scalability</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Champlain Release
    <ul>
      <li>Option to write to RDBMS (mySQL or Oracle), HDFS
      - Argus Audit Logs To HDFS
      - Log event is written to Local log file
      - Local log file will be copied to HDFS destination (when HDFS is available)
      - Local log file and HDFS file rotated at a regular interval
      - Design being enhanced</li>
      <li>Addition of Log4j file appender
        <ul>
          <li>HDFS destination can be specified in the appender</li>
          <li>Customer/Partners can add customer log4j appenders</li>
        </ul>
      </li>
      <li>Extensible HDFS LOG format
        <ul>
          <li>Available as JSON format</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<p><strong>Reference www.hortonworks.com</strong></p>
</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/HDP-Argus-Deeper-Integration/" title="Argus Deeper Integration ">Argus Deeper Integration </a></h3>
      <p>Argus deeper integration to HBASE & HIVE</p>
      <h6> Posted on : 2014-10-26 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><h4 id="deeper-integration-hbase">Deeper Integration (HBase)</h4>

<ul>
  <li>Pre Champlain
    <ul>
      <li>Hbase Agents supports table, CF, Column level permissions</li>
      <li>Local Permissions not integrated</li>
    </ul>
  </li>
  <li>Post Champlain
    <ul>
      <li>Integrate local grant/revoke permissions</li>
      <li>New Argus/XA co-processor, no changes in HBase</li>
      <li>Hbase-site.xml</li>
    </ul>
  </li>
</ul>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="o">&lt;</span><span class="nt">property</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="nt">name</span><span class="o">&gt;</span><span class="nt">hbase</span><span class="nc">.coprocessor.master.classes</span><span class="o">&lt;/</span><span class="nt">name</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="nt">value</span><span class="o">&gt;</span><span class="nt">com</span><span class="nc">.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor</span><span class="o">&lt;/</span><span class="nt">value</span><span class="o">&gt;</span>  <span class="o">&lt;</span><span class="nt">property</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="nt">name</span><span class="o">&gt;</span><span class="nt">hbase</span><span class="nc">.coprocessor.region.classes</span><span class="o">&lt;/</span><span class="nt">name</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="nt">value</span><span class="o">&gt;</span><span class="nt">com</span><span class="nc">.xasecure.authorization.hbase.XaSecureAuthorizationCoprocessor</span><span class="o">&lt;/</span><span class="nt">value</span><span class="o">&gt;</span></code></pre></figure>

<hr />

<h5 id="hbase-grant-revoke">HBase Grant Revoke</h5>

<ul>
  <li>Command Line Operations
    <ul>
      <li>Permission supported
        <ul>
          <li>Admin (A) / Create © / Write (W) / Read (R)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Can be performed at table, CF, column level</li>
</ul>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">grant</span> <span class="o">&lt;</span><span class="nt">user</span><span class="o">&gt;</span> <span class="o">&lt;</span><span class="nt">permissions</span><span class="o">&gt;[</span> <span class="o">&lt;</span><span class="nt">table</span><span class="o">&gt;[</span> <span class="o">&lt;</span><span class="nt">column</span> <span class="nt">family</span><span class="o">&gt;[</span> <span class="o">&lt;</span><span class="nt">column</span> <span class="nt">qualifier</span><span class="o">&gt;</span> <span class="o">]</span> <span class="o">]</span> <span class="o">]</span>    <span class="err">#</span> <span class="nt">grants</span> <span class="nt">permissions</span>
<span class="nt">revoke</span> <span class="o">&lt;</span><span class="nt">user</span><span class="o">&gt;</span> <span class="o">&lt;</span><span class="nt">permissions</span><span class="o">&gt;</span> <span class="o">[</span> <span class="o">&lt;</span><span class="nt">table</span><span class="o">&gt;</span> <span class="o">[</span> <span class="o">&lt;</span><span class="nt">column</span> <span class="nt">family</span><span class="o">&gt;</span> <span class="o">[</span> <span class="o">&lt;</span><span class="nt">column</span> <span class="nt">qualifier</span><span class="o">&gt;</span> <span class="o">]</span> <span class="o">]</span> <span class="o">]</span>   <span class="err">#</span> <span class="nt">revokes</span> <span class="nt">permissions</span></code></pre></figure>

<h4 id="deeper-integration-hive">Deeper Integration (Hive)</h4>

<ul>
  <li>Pre Champlain
    <ul>
      <li>XA Secure/Argus uses multiple hooks in Hive</li>
      <li>Not all information necessary to make authorization decision are available in Hive authorizer hooks</li>
      <li>Local Grant/Revoke permission not integrated with Argus</li>
      <li>Storage based authorization only looks at POSIX permissions</li>
    </ul>
  </li>
</ul>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">hive</span><span class="nc">.security.authorization.manager</span><span class="o">=</span><span class="nt">com</span><span class="nc">.xasecure.authorization.hive.authorizer.XaSecureAuthorizer</span>
<span class="nt">hive</span><span class="nc">.semantic.analyzer.hook</span><span class="o">=</span><span class="nt">com</span><span class="nc">.xasecure.authorization.hive.hooks.XaSecureSemanticAnalyzerHook</span>
<span class="nt">hive</span><span class="nc">.exec.post.hooks</span><span class="o">=</span><span class="nt">com</span><span class="nc">.xasecure.authorization.hive.hooks.XaSecureHivePostExecuteRunHook</span></code></pre></figure>

<ul>
  <li>Champlain Integration
    <ul>
      <li>New plug-in model in Hive to support external authorizers
        <ul>
          <li>All information necessary to make authorization decision are provided to authorizer plug-in</li>
        </ul>
      </li>
      <li>XASecure/Argus Hive agent registers a single hook with Hive for authorization</li>
    </ul>
  </li>
</ul>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">hive</span><span class="nc">.security.authorization.manager</span><span class="o">=</span><span class="nt">com</span><span class="nc">.xasecure.authorization.hive.authorizer.XaSecureHiveAuthorizerFactory</span></code></pre></figure>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+ Integrate Grant/Revoke permissions
    * New Hive Plugin enables Argus to handle Grant/Revoke permission
    * Argus will store Grant/Revoke policy and enforce it, with auditing
    * Option to disable Grant/Revoke
    * Group/Roles mapped to Groups in Argus Admin
+ Storage Based Authorization
    * In SBA, Hive used HDFS permissions for allowing operations
    * HDFS Permission Check
        - Hive uses RPC to communicate with HDFS and validate permission on HDFS folders
        - If Argus is enabled, Hive will use permissions based on Argus policies in HDFS
        - Argus can be used for Storage based and regular Hive authorization
</code></pre></div></div>

<h4 id="deeper-integration-hdfs">Deeper Integration (HDFS)</h4>

<ul>
  <li>Pluggable HDFS authorization is being added (HDFS-6826)</li>
  <li>Argus will replace the JavaAgent based code injection with a custom authorization plugin</li>
</ul>

<blockquote>
  <p>Still in Discussions</p>
</blockquote>

<hr />
<p><strong>Reference www.hortonworks.com</strong></p>

</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/HDP-Argus-Features-For-Champlain/" title="Argus Features For HDP Champlain ">Argus Features For HDP Champlain </a></h3>
      <p>Argus Features</p>
      <h6> Posted on : 2014-10-25 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><h3 id="argus-features-for-champlain">Argus features for Champlain</h3>

<ul>
  <li>New Components Support
    <ul>
      <li>Storm Authorization &amp; Auditing ✔</li>
      <li>Knox Authorization &amp; Auditing ✔</li>
    </ul>
  </li>
  <li>Deeper Integration with HDP Stack
    <ul>
      <li>Windows Support</li>
      <li>Integration with HDFS Auth API ✔</li>
      <li>Integration with Hive Auth API, support grant/revoke commands ✔</li>
      <li>Support grant/revoke commands in Hbase ✔</li>
    </ul>
  </li>
  <li>Enterprise Readiness
    <ul>
      <li>Rest APIs for policy manager ✔</li>
      <li>Store Audit logs locally in HDFS ✔</li>
      <li>Support Oracle DB</li>
    </ul>
  </li>
</ul>

<h4 id="new-component-support-storm">New Component Support (Storm)</h4>

<ul>
  <li>
    <p>Storm now support ACLs for authorization</p>
  </li>
  <li>
    <p>Argus provides administration for these ACLs, also enables access auditing</p>
  </li>
  <li>
    <p>Following permission support are enabled</p>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th>Submit Topology</th>
      <th>Get Nimbus Conf</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Kill topology</td>
      <td>Get Cluster Info</td>
    </tr>
    <tr>
      <td>File Upload</td>
      <td>File Download</td>
    </tr>
    <tr>
      <td>Activate</td>
      <td>Deactivate</td>
    </tr>
    <tr>
      <td>Get Topology Conf</td>
      <td>Get Topology</td>
    </tr>
    <tr>
      <td>Get User Topology</td>
      <td>Get Topology Info</td>
    </tr>
    <tr>
      <td>Upload New Credential</td>
      <td>Rebalance</td>
    </tr>
  </tbody>
</table>

<h4 id="new-component-support-knox">New Component Support (Knox)</h4>
<ul>
  <li>Knox currently performs service level authorization (perimiter security)
    <ul>
      <li>Allow group or user access to specific REST API (WebHDFS, WebHcat, JDBC over http etc)</li>
      <li>Can also restrict based on ip address</li>
      <li>Permissions maintained in a file</li>
    </ul>
  </li>
  <li>Manage these permissions through Argus Portal
    <ul>
      <li>User experience similar to other components</li>
    </ul>
  </li>
  <li>Get access to auditing records in Argus portal</li>
</ul>

<hr />

<p><strong>Reference www.hortonworks.com</strong></p>

</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/HDP_Argus/" title="HDP Champlain and Argus ">HDP Champlain and Argus </a></h3>
      <p>HDP Champlain & Argus a Headsup</p>
      <h6> Posted on : 2014-10-24 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><h2 id="hdp-champlain-and-argus">HDP Champlain and Argus</h2>

<h3 id="about-argus-">About Argus ?</h3>
<p>Argus is a framework to enable, monitor and manage comprehensive data security across the Hadoop platform.The vision with Argus is to provide comprehensive security across the Apache Hadoop ecosystem.</p>

<p>With the advent of  Apache YARN, the Hadoop platform can now support a true data lake architecture. Enterprises can potentially run multiple workloads, in a multi tenant environment. Data security within Hadoop needs to evolve to support multiple use cases for data access, while also providing a framework for central administration of security policies and monitoring of user access.</p>

<p>Argus will deliver this comprehensive approach to central security policy administration across the core enterprise security requirements of authentication, authorization, accounting and data protection. It already extends baseline features for coordinated enforcement across Hadoop workloads from batch, interactive SQL and real–time IN Hadoop.</p>

<p>The extensible architecture of this security platform is leveraged to apply policies consistently against additional Hadoop ecosystem components (beyond HDFS, Hive, and HBase) including Storm, Solr, Spark, and more. It truly represents a major step forward for the Hadoop ecosystem by providing a comprehensive approach – all completely as open source.</p>

<blockquote>
  <p>CENTRAL SECURITY ADMINISTRATION</p>
</blockquote>

<ul>
  <li>Delivers a ‘single pane of glass’ for the security administrator</li>
  <li>Centralizes administration of security policy</li>
  <li>Ensures consistent coverage across the entire Hadoop stack</li>
</ul>

<p>Authorization Policies can be setup through the Ambari console Policy Manger UI. Argus stores the policies centrally and enforecement happen at each component level. Along with enforcement, audit data is also collected for each of the request that comes in providing a detailed audit data at one place from which reports can be generated.</p>

<h4 id="under-the-hood---how-it-works-">Under the Hood - How it works ?</h4>
<p>An Administration Portal is provided which sits on top of an audit server and policy server. The policy server has a policy DB where the policies are stored and this is then transported to plugins. Ideally, the plugins pull these policies at regular intervals, which sit within each component and provides the enforcement. Currenlty HBASE / HIVE &amp; HDFS plugins are available, and also for KNOX &amp; STORM as part of the Champlain release.</p>

<p><img src="http://localhost:4000/downloads/ARGUS.png" alt="Argus Integration" /></p>

<p>Reference - www.Hortonworks.com</p>

<hr />

</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/Why-Apache-Spark-Data-Scientists/" title="Why Apache Spark">Why Apache Spark</a></h3>
      <p> Apache Spark for Data Scientists ?</p>
      <h6> Posted on : 2014-06-12 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><p>Spark for data science has mostly noted for its ability to keep data resident in memory, which can speed up iterative machine learning workloads compared to MapReduce.Spark has a number of features that make it a compelling crossover platform for investigative as well as operational analytics:</p>

<ul>
  <li>Spark comes with a machine-learning library, MLlib.
    <ul>
      <li>Being Scala-based, Spark embeds in any JVM-based operational system
        <ul>
          <li>but can also be used interactively in a REPL, familiar to R and Python users.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>For Java programmers, Scala still presents a learning curve.
    <ul>
      <li>but at least, any Java library can be used from within Scala.</li>
    </ul>
  </li>
  <li>Spark’s RDD (Resilient Distributed Dataset) abstraction resembles Crunch’s PCollection
    <ul>
      <li>which has proved a useful abstraction in Hadoop(familiar to Crunch developers)</li>
      <li>Crunch can even be used on top of Spark</li>
    </ul>
  </li>
  <li>Spark imitates Scala’s collections API and functional style
    <ul>
      <li>which is a boon to Java and Scala developers</li>
      <li>also somewhat familiar to developers coming from Python</li>
      <li>Scala is also a compelling choice for statistical computing.</li>
    </ul>
  </li>
  <li>Spark itself, and Scala underneath it, are not specific to machine learning.
    <ul>
      <li>They provide APIs supporting related tasks, like data access, ETL, and integration.</li>
      <li>As with Python, the entire data science pipeline can be implemented within this paradigm, not just the model fitting and analysis.</li>
    </ul>
  </li>
  <li>Code that is implemented in the REPL environment can be used mostly as-is in an operational context.</li>
  <li>Data operations are transparently distributed across the cluster, even as you type.</li>
</ul>

<p>Spark is a compelling multi-purpose platform for use cases that span investigative, as well as operational, analytics.Spark, and MLlib in particular, still has a lot of growing to do. The project needs optimizations, fixes, and deeper integration with YARN. It doesn’t yet provide nearly the depth of library functions that conventional data analysis tools do.</p>

<p>** Reference - http://blog.cloudera.com **</p>

</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/Hadoop_YARN/" title="Hadoop & YARN">Hadoop & YARN</a></h3>
      <p>Hadoop & Yarn Whats new</p>
      <h6> Posted on : 2014-03-03 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><h3 id="hdfs-federation">HDFS Federation</h3>

<p>Hadoop 2.x introduces HDFS Federation, a scaling mechanism for the NameNode. The new Hadoop infrastructure provides multiple NameNodes, as opposed to the single NameNode used in Hadoop 1.x, and enables the NameNodes to work independently of each other. This offers two key benefits:</p>

<ul>
  <li>
    <p>Scalability: NameNodes can now scale horizontally, allowing for the improved performance of NameNode tasks by distributing reads and writes across a cluster of NameNodes.</p>
  </li>
  <li>
    <p>Namespaces: Multiple Namespaces can be defined, allowing for better organizing and separating of big data.</p>
  </li>
</ul>

<h3 id="multiple-federated-namenodes">Multiple Federated NameNodes</h3>

<p>The NameNodes are federated, which means they are independent and don’t require coordination with each other. DataNodes are used as common block storage by all NameNodes. Each DataNode registers with all NameNodes in the cluster and sends periodic heartbeats and block reports. Any NameNode can send commands to any DataNode.</p>

<p><img src="http://localhost:4000/downloads/Hadoop-yarn-1.png" alt="Federated NameNodes :" /></p>

<h3 id="multiple-namespaces">Multiple Namespaces</h3>

<p>All files and directories belong to a Namespace. In older versions of Hadoop, an instance had only a single Namespace. Hadoop 2.0 now allows for multiple Namespaces, with each NameNode managing a single Namespace. Multiple Namespaces offer:</p>

<ul>
  <li>
    <p>Scalability - Having multiple independent Namespaces makes scaling possible in Hadoop 2.0.</p>
  </li>
  <li>
    <p>File Management - It is now possible to associate Big Data with a Namespace, which makes it easier to manage and maintain files.</p>
  </li>
</ul>

<p><img src="http://localhost:4000/downloads/Hadoop-yarn-2.png" alt="NameNodes each manage a single Namespace :" /></p>

<h3 id="hdfs-high-availability">HDFS High Availability</h3>

<p>Prior to Hadoop 2.0, the NameNode was a single point of failure in an HDFS cluster. Each cluster had a single NameNode, and if that machine or process became unavailable, the cluster as a whole would be unavailable until the NameNode was either restarted or brought up on a separate machine.</p>

<p>The HDFS High Availability (HA) feature addresses this issue by providing the option of running two redundant NameNodes in the same cluster in an Active/Passive configuration with a hot standby. The Quorum Journal Manager (QJM) allows a fast failover to a new NameNode during hardware failure or administrator-initiated failover for planned maintenance.</p>

<ul>
  <li>Quorum Journal Manager</li>
</ul>

<p>Two separate machines are configured as NameNodes, one in an Active state, the other in a Standby state. The Active NameNode handles all cluster client operations, while the Standby acts as a slave, maintaining state to provide a fast failover if necessary.</p>

<p>Both nodes communicate with a group of separate daemons called JournalNodes. All Namespace modifications are logged durably to a majority (a quorum) of the JournalNode daemons. When the Standby Node checks the edits in the JournalNodes, it applies them to its own namespace.</p>

<p><img src="http://localhost:4000/downloads/Hadoop-yarn-3.png" alt="Configuring automatic failover with ZooKeeper:" /></p>

<p>** Note: In the event of a failover, before switching to Active state, the Standby must read all of the edits from the JournalNodes. This ensures that the Namespace state is fully synchronized before a failover occurs. **</p>

<ul>
  <li>Configuring Automatic Failover</li>
</ul>

<p>Quorum Journal Manager only provides a manual failover. To enable HA NameNodes to failover automatically, ZooKeeper and the ZKFailoverController (ZKFC) must be configured within the cluster. ZooKeeper runs an odd number of daemons that monitor if a NameNode fails. ZKFC is a ZooKeeper client that monitors and manages the state of a NameNode.</p>

<p>Each machine with a NameNode runs a ZKFC instance. The ZKFC periodically pings the NameNode with a health-check command. If the NameNode responds with a healthy status, ZKFC continues normally. If the NameNode has crashed or become unresponsive, ZKFC will mark it as unhealthy.</p>

<hr />

<h3 id="yarn">YARN</h3>

<p>Yet Another Resource Negotiator (YARN) extends Hadoop beyond just MapReduce for data processing. Although it is still able to execute MapReduce jobs across the cluster, YARN further provides a generic framework that allows for any type of application to execute on the big data.</p>

<h4 id="opensource-yarn-use-cases">Opensource YARN Use Cases</h4>

<p>Since YARN enables Hadoop to run applications beyond MapReduce, there are countless possibilities for the types of processing that can be done on data stored in HDFS. Below is just a brief list of open-source projects currently being ported onto YARN for use in Hadoop 2.0. More frameworks will be developed as YARN becomes more prevalent.</p>

<table>
  <thead>
    <tr>
      <th>Tez</th>
      <th>Improves execution of MapReduce jobs.</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>HOYA</td>
      <td>HBase on YARN.</td>
    </tr>
    <tr>
      <td>Storm and Apache S4</td>
      <td>For real-time computing.</td>
    </tr>
    <tr>
      <td>Spark</td>
      <td>A MapReduce-like cluster computing framework designed for low-latency iterative jobs and interactive use from an interpreter.</td>
    </tr>
    <tr>
      <td>Open MPI</td>
      <td>A high-performance Message Passing Library that implements MPI-2.</td>
    </tr>
    <tr>
      <td>Apache Giraph</td>
      <td>A graph-processing platform.</td>
    </tr>
  </tbody>
</table>

<p>YARN Components</p>

<p>YARN consists of the ResourceManager, the NodeManager, and the ApplicationMaster.</p>

<ul>
  <li>
    <p>ResourceManager is the central controlling authority for resource management and schedules and allocates cluster resources. It has a pluggable scheduler that allows for different algorithms (such as capacity and fair scheduling) to be used as necessary. It is a pure scheduler that does not monitor or track application status or restart failed tasks due to application or hardware failures. ResourceManager also tries to optimize the cluster (i.e. use all resources all the time) based on the constraints of the scheduler. In medium to large clusters it typically runs on a dedicated machine.</p>
  </li>
  <li>
    <p>The NodeManager is the per-machine slave. It runs on the same machines as the HDFS DataNodes. It is responsible for launching the applications’ resource containers (CPU, memory, disk, network), monitoring their resource usage and reporting the same to the ResourceManager.</p>
  </li>
  <li>
    <p>Each per-application ApplicationMaster has the responsibility of negotiating appropriate resource containers from the ResourceManager, tracking their status, and working with the NodeManager(s) to execute and monitor the component tasks. The ApplicationMaster has primary responsibility for application fault tolerance. Because each application has its own ApplicationMaster, it is not a common bottleneck for the cluster. Each ApplicationMaster runs in a container on a NodeManager machine.</p>
  </li>
</ul>

<p><img src="http://localhost:4000/downloads/Hadoop-yarn-4.png" alt="Reference:" /></p>

<p>** Note : There is no JobTracker in Hadoop 2.x. YARN’s ResourceManager and ApplicationManager replaced the Hadoop 1.x JobTracker functionality. There is no TaskTracker in Hadoop 2.x. YARN’s NodeManager replaced the Hadoop 1.x TaskTracker functionality. **</p>

<h4 id="lifecycle-of-a-yarn-application">Lifecycle of a YARN Application</h4>

<p>The YARN lifecycle</p>

<p>1) A client application submits a new Application Request to the ResourceManager (RM).</p>

<p>2) The ApplicationsManager (AsM) finds an available DataNode on the cluster.</p>

<p>3) That node’s NodeManager (NM) creates an instance of the ApplicationMaster (AM).</p>

<p>4) The AM then sends a request to the RM, asking for specific resources like memory and CPU requirements. The RM replies with a list of Containers, which includes the specific DataNodes to start the Containers on.</p>

<p>The AM starts a Container on each DataNode as instructed by the RM. The Container performs a task, as directed by the AM. As tasks are being performed by the Containers, the client application can request status updates directly from the ApplicationMaster.</p>

<p><img src="http://localhost:4000/downloads/Hadoop-yarn-5.png" alt="Reference" /></p>

<hr />
<p><strong>Reference www.hortonworks.com</strong>*</p>

</p> -->

	  
          
	  


	  <header class="major">      
	    <h3><a href="http://localhost:4000/blog/Apache_Hive_vs_Pig/" title="Apache Hive vs Apache Pig">Apache Hive vs Apache Pig</a></h3>
      <p>An Overview on Apache Hive </p>
      <h6> Posted on : 2013-10-01 00:00:00 +0530</h6>
	  </header>
	  
	  

	  <!-- <p><p>Apache Hive &amp; Pig try to ease the complexity of writing MapReduce jobs in a programming language like Java by giving the user a set of tools that they may be more familiar with.</p>

<h2 id="what-are-their-similiarities-">What are their Similiarities ?</h2>

<ul>
  <li>
    <p>The raw data is stored in Hadoop’s HDFS and can be any format although natively it usually is a TAB separated text file, while internally they also may make use of Hadoop’s SequenceFile file format.</p>
  </li>
  <li>
    <p>Both translate the irrespective high level languages to MapReducejobs</p>
  </li>
  <li>
    <p>Both offer significant reductions in program size over Java</p>
  </li>
  <li>
    <p>Both provide points of extension to cover gaps in functionality</p>
  </li>
  <li>
    <p>Both provide interoperability withotherlanguages</p>
  </li>
  <li>
    <p>None support random reads/writes or low-latency queries</p>
  </li>
</ul>

<h2 id="let-look-into-the-differences">Let look into the Differences</h2>

<h3 id="pig">PIG</h3>

<p>Apache Pig is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs. The salient property of Pig programs is that their structure is amenable to substantial parallelization, which in turns enables them to handle very large data sets.
Pig was Developed by Yahoo! as a scripting language that can consume any type of data, where the schmea of the data can be anything &amp; is defined at the load time. Different data types supported include:</p>

<ul>
  <li>Primitive datatypes</li>
</ul>

<p>++ scalars 
 ++ int 
 ++ float 
 ++ double 
 ++ long 
 ++ Arrays 
 ++ chararray 
 ++ bytearray</p>

<ul>
  <li>Complex datatype</li>
</ul>

<p>++ tuple - An ordered set of fields.
 ++ bag - An collection of tuples.
 ++ map - A set of key value pairs.</p>

<h4 id="pig-components">PIG Components</h4>

<h5 id="pig-latin">Pig Latin</h5>

<p>Pig’s language layer currently consists of a textual language called Pig Latin, which has the following key properties:</p>

<ul>
  <li>Ease of programming. It is trivial to achieve parallel execution of simple, “embarrassingly parallel” data analysis tasks. Complex tasks comprised of multiple interrelated data transformations are explicitly encoded as data flow sequences, making them easy to write, understand, and maintain.</li>
  <li>Optimization opportunities. The way in which tasks are encoded permits the system to optimize their execution automatically, allowing the user to focus on semantics rather than efficiency.</li>
  <li>Extensibility. Users can create their own functions to do special-purpose processing.</li>
</ul>

<h5 id="compiler--stages-of-operation">Compiler &amp; Stages of operation</h5>

<p>On the FrontEnd</p>

<ul>
  <li>
    <p>The parser transforms a Pig Latin script into a logical plan</p>
  </li>
  <li>
    <p>Semantic checks and optimizations are done in this logical plan</p>
  </li>
  <li>
    <p>Logical plan is then transformed into physical plan .This Physical Plan contains the operators that will be applied to the data</p>
  </li>
  <li>
    <p>This physical plan is divided into a set of MapReduce jobs by the MRCompiler into an MROperPlan</p>
  </li>
  <li>
    <p>This MROperPlan is then optimized</p>
  </li>
  <li>
    <p>Finally a set of MapReduce jobs are generated by the JobControlCompiler. These are submitted to Hadoop and monitored by the MapReduceLauncher</p>
  </li>
</ul>

<p>On the BackEnd</p>

<ul>
  <li>Each PigGenericMapReduce.Map, PigCombiner.Combine, and PigGenericMapReduce.Reduce use the pipeline of physical operators constructed in the front end to load, process, and store data</li>
</ul>

<p><strong>Pig has no metadata database. Datatypes and schemas are defined within each script</strong></p>

<h5 id="join-optimization">Join Optimization</h5>

<ul>
  <li>Replicated Joins</li>
</ul>

<p>Works well if one or more relations are small enough to fit into main memory. In this type of join the large relation is followed by one or more small relations. The small relations must be small enough to fit into main memory; if they don’t, the process fails and an error is generated.</p>

<p>Conditions: 
Fragment replicate joins are experimental; we don’t have a strong sense of how small the small relation must be to fit into memory. On a simple test with a query that involves just a JOIN, a relation of up to 100 M can be used if the process overall gets 1 GB of memory.</p>

<ul>
  <li>Merge Joins</li>
</ul>

<p>If both the inputs are already sorted on the join key ,the data can be joined in the map phase of the mapreduce jobs. This provides a significant performance improvement compared to passing all of the data through unneeded sort and shuffle phases.</p>

<p>Conditions:</p>

<ul>
  <li>The merge join only has two inputs</li>
  <li>Only inner join will be supported</li>
  <li>Between the load of the sorted input and the merge join statement there can only be filter statements and foreach statement where the foreach statement should meet the following conditions:</li>
</ul>

<p>++ There should be no UDFs in the foreach statement 
 ++ The foreach statement should not change the position of the join keys 
 ++ There should not transformation on the join keys which will change the sort order</p>

<ul>
  <li>Skewed Joins</li>
</ul>

<p>Skewed join computes a histogram of the key space and uses this data to allocate reducers for a given key. Skewed join does not place a restriction on the size of the input keys.
It accomplishes this by splitting the left input on the join predicate and streaming the right input. The left input is sampled to create the histogram.</p>

<p>Conditions:</p>

<ul>
  <li>Skewed join works with two-table inner join. Do not support more than two tables for skewed join</li>
  <li>Specifying three-way (or more) joins will fail validation. For such joins, you have to break them up into two-way joins</li>
</ul>

<h5 id="no-jdbcodbc-driver">No JDBC/ODBC driver</h5>
<p>Pigserver is used for connecting to Pig using Java program</p>

<h5 id="no-partitions">No partitions</h5>
<p>filters can achieve the partitions</p>

<h5 id="server">Server</h5>
<p>No such server 
No Web UI</p>

<h5 id="user-defined-functions">User Defined Functions</h5>
<p>UDF functions in pig can be implemented by extending any of the abstract classes like EvalFunc,StoreFunc,LoadFunc and FilterFunc</p>
</p> -->

	  
          
	</div>
      </section>

    </div>

    <!-- Contact -->
<section id="contact">
	<div class="inner">
		<section>
			<form action="https://formspree.io/sivan.consult@gmail.com" method="POST">
				<div class="field half first">
					<label for="name">Name</label>
					<input type="text" name="name" id="name" />
				</div>
				<div class="field half">
					<label for="email">Email</label>
					<input type="text" name="_replyto" id="email" />
				</div>
				<div class="field">
					<label for="message">Message</label>
					<textarea name="message" id="message" rows="6"></textarea>
				</div>
				<ul class="actions">
					<li><input type="submit" value="Send Message" class="special" /></li>
					<li><input type="reset" value="Clear" /></li>
				</ul>
			</form>
		</section>
		<section class="split">
			<section>
				<div class="contact-method">
					<span class="icon alt fa-envelope"></span>
					<h3>Email</h3>
					<a href="#">sivan.consult@gmail.com</a>
				</div>
			</section>
			<section>
				<div class="contact-method">
					<span class="icon alt fa-phone"></span>
					<h3>Phone</h3>
					<span>+91-999-545-5746</span>
				</div>
			</section>
			<section>
				<div class="contact-method">
					<span class="icon alt fa-home"></span>
					<h3>Address</h3>
					<span>
					
					    Karyavattam<br />
					
					
					    Trivandrum,
					
					
					    Kerala 
					
					
					    695581<br />
					
					
					    India
					
					</span>
				</div>
			</section>
		</section>
	</div>
</section>

<!-- Footer -->
	<footer id="footer">
		<div class="inner">
			<ul class="icons">
				
				<li><a href="https://twitter.com/sivansasidharan" class="icon alt fa-twitter" target="_blank"><span class="label">Twitter</span></a></li>
				
				
				
				<li><a href="https:facebook.com/sivansasidharan" class="icon alt fa-facebook" target="_blank"><span class="label">Facebook</span></a></li>
				
				
				
				
				
				
				<li><a href="https://github.com/sivansasidharan/" class="icon alt fa-github" target="_blank"><span class="label">GitHub</span></a></li>
				
				
				
				<li><a href="https://www.linkedin.com/in/sivansasidharan/" class="icon alt fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
				
			</ul>
			<ul class="copyright">
				<li>&copy; TECHNOLOGY PRÉCIS!   by Sivan Sasidharan</li>
				<li>Design: HTML5 UP</li>
				<li>Powered by: Jekyll & GitHub</li>
			</ul>
		</div>
	</footer>

</div>

<!-- Scripts -->
	<script src="http://localhost:4000/assets/js/jquery.min.js"></script>
	<script src="http://localhost:4000/assets/js/jquery.scrolly.min.js"></script>
	<script src="http://localhost:4000/assets/js/jquery.scrollex.min.js"></script>
	<script src="http://localhost:4000/assets/js/skel.min.js"></script>
	<script src="http://localhost:4000/assets/js/util.js"></script>
	<!--[if lte IE 8]><script src="http://localhost:4000/assets/js/ie/respond.min.js"></script><![endif]-->
	<script src="http://localhost:4000/assets/js/main.js"></script>


  </body>

</html>
